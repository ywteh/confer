PaperID,Title,Abstract13,Social Spammer Detection in Microblogging,"The explosion of microblogging services, unfortunately, makes an effective platform for spammers to unfairly outpower normal users with unwanted content via social networks, known as social spamming. The rise of social spamming inevitably hinders the use of microblogging systems for effective information dissemination and sharing. Distinct features of microblogging systems present new challenges to prevent social spamming. First, unlike traditional social networks, microblogging allows communication between two parties without approval. These relaxed requirements make it easier for spammers to imitate normal users by quickly accumulating a large number of friends. Second, the texts posted in microblogging are extremely short, noisy and unstructured. Traditional social spammer detection methods are not directly adaptable to microblogging. In this paper, we investigate to collectively use both network and content information to perform effective social spammer detection in microblogging. In particular, we present a mathematical optimization formulation that models the social network information and content information in a unified framework. Experiments on a Twitter dataset demonstrate that our proposed method can effectively utilize both kinds of information to outperform the state-of-the-art methods."18,Search Strategies for Optimal Multi-Way Number Partitioning,"The number partitioning problem seeks to divide a set of n numbers across k distinct subsets so as to minimize the sum of the largest partition. In recent years, several efficient algorithms have been developed that offer substantially improved performance, especially in the general case where k &gt; 2. In this work, we develop a new optimal algorithm for multi-way partitioning. A critical observation motivating our methodology is that a globally optimal k-way partition may be recursively constructed by obtaining suboptimal solutions to subproblems of size k - 1. We also demonstrate how to further prune unpromising partial assignments by detecting and eliminating dominated solutions. Our approach outperforms the previous state-of-the-art by up to three orders of magnitude, reducing average runtime on the largest benchmarks from several hours to less than a second."275,Robust Unsupervised Feature Selection,"In this paper, a new unsupervised feature selection method, i.e., Robust Unsupervised Feature Selection (RUFS), is proposed. Unlike traditional unsupervised feature selection methods, pseudo cluster labels are learned via local learning regularized robust nonnegative matrix factorization. During the cluster label learning process, feature selection is performed simultaneously by robust joint $l_{2, 1}$ norms minimization. To learn more accurate cluster labels, we add an orthogonal constraint on the indicator matrix, resulting in more discriminative and ideal cluster labels. Since RUFS utilizes $l_{2, 1}$ norm minimization on both label learning and feature learning, outliers and noise could be effectively handled and redundant or noisy features could be effectively reduced. Our method adopts the advantages of both robust nonnegative matrix factorization, local learning, and robust feature learning. In order to make RUFS be more applicable practically, we design a (projected) limited-memory BFGS based iterative algorithm to efficiently solve the optimization problem of RUFS in terms of both memory consumption and computation complexity. Experimental results on different benchmark real world datasets show the promising performance of RUFS over the state-of-the-arts."23,Towards Practical Planning to Predict and Exploit Intentions for Interacting with Self-Interested Ag,"A key challenge in non-cooperative multi-agent systems is that of developing efficient planning algorithms for intelligent agents to interact and perform effectively among boundedly rational, self- interested agents (e.g., humans). The practicality of existing works addressing this challenge is being undermined due to either the restrictive assumptions of the other agentsâ€™ behavior, the failure in accounting for their rationality, or the prohibitively expensive cost of modeling and predicting their intentions. To boost the practicality of research in this field, we investigate how intention prediction can be efficiently exploited and made practical in planning, thereby leading to efficient intention-aware planning frameworks capable of predicting the intentions of other agents and acting optimally with respect to their predicted intentions. We show that the performance losses incurred by the resulting planning policies are linearly bounded by the error of intention prediction. Empirical evaluations through a series of stochastic games demonstrate that our policies can achieve better and more robust performance than the state-of-the-art algorithms."35,Nonconvex Relaxation Approaches to Robust Matrix Recovery,"Motivated by the recent developments of nonconvex penalties in
sparsity modeling, we propose a nonconvex optimization model for
handing the low-rank matrix recovery problem. Different from the
famous robust principal component analysis (RPCA), we suggest
recovering low-rank and sparse matrices via a nonconvex loss function
and a nonconvex penalty. The advantage of the nonconvex approach lies
in its stronger robustness. To solve the model, we devise a
majorization-minimization augmented Lagrange multiplier (MM-ALM)
algorithm which finds the local optimal solutions of the proposed
nonconvex model. We also provide an efficient strategy to speedup
MM-ALM, which makes the running time comparable with the
state-of-the-art algorithm solving RPCA. Finally, the experimental
results demonstrate the superiority of our nonconvex approach over
RPCA in terms of matrix recovery accuracy."40,Partial-Tree Linearization for Text-to-Text Applications,"We present a novel data-driven linearization system for text-to-text applications. Syntax-based ordering of an input set of words is performed using learning-guided search and the dependency grammar. POS and dependency relations can be optionally associated with any subset of input words to further specify the output, which makes our system more practical to a dependency-based NLG pipeline, such as transfer-based MT and abstractive text summarization. In addition, our system achieves the best published results when evaluated on the standard PTB word-ordering task."43,Risk Estimates for Bandit Problems,"  We consider stochastic multiarmed bandit problems where each arm
  generates i.i.d. rewards according to an unknown distribution.
  Whereas classical bandit solutions only maximize the expected
  reward, we consider the problem of minimizing risk using notions
  such as the value-at-risk, the average value-at-risk, and the
  mean-variance risk.  We present algorithms to minimize the risk over
  a single and multiple time periods, along with PAC accuracy
  guarantees given a finite number of reward samples.  In the
  single-period case, we show that finding the arm with least risk
  requires not many more samples than the arm with highest expected
  reward.  Although minimizing the multi-period value-at-risk is known
  to be hard, we present an algorithm with comparable sample
  complexity under additional assumptions."45,Minimizing Writes in Parallel External Memory Search,"Recent research on external-memory search has suggested that disks can be effectively used
as secondary storage when performing large breadth-first searches. The current state-of-art technique
for performing a general large breadth-first search is the Two-Bit Breadth-First Search (TBBFS). This approach,
however, does not save any information about each state that can be used after the computation is
complete for further analysis, and it also uses a significant amount of external storage during the
computational process. We introduce a new algorithm which saves information about the depth
of each state in the search, and does not use any storage besides that needed to store the depth of each state.
Write-Minimizing BFS (WMBFS) does this because it was designed to minimize number of writes performed
during an external memory search. We describe our parallel implementation
of the algorithm and show that WMBFS reduces the amount of I/O over TBBFS by over an order of magnitude.
We also analyze the domain-specific tradeoffs which would favor one algorithm over another, and present the
results of a BFS on the single-agent version of Chinese Checkers, a state space with 1.88 trillion
($1.88\times10^{12}$) positions.
"48,Joint Modeling of Argument Identification and Role Determination in Chinese Event Extraction ,"Argument extraction is a challenging task in event extraction. However, most of previous studies focused on intra-sentence information and failed to extract inter-sentence arguments. This paper proposes a discourse-level  joint model of argument identification and role determination to infer those inter-sentence arguments in a discourse. Moreover, in order to better represent the relationship among relevant event mentions and the relationship between an event mention and its arguments in a discourse, this paper introduces various kinds of corpus-based and discourse-based  constraints into our joint model, either automatically learned or linguistically motivated. Evaluation on the ACE 2005 Chinese corpus justifies the effectiveness of our joint model over a strong baseline in Chinese argument extraction, in particular argument identification."53,One-Class Conditional Random Fields for Sequential Anomaly Detection,"Sequential anomaly detection is a challenging problem due to the one-class nature of the data (i.e., data is collected from only one class) and the temporal dependence in sequential data. We present one-class conditional random fields (OCCRF) for sequential anomaly detection that learn from a one-class dataset and capture the temporal dependence structure in it. We propose a hinge loss in a regularized risk minimization framework which requires that for each sequence the difference between the sequence being ``normal\'\' rather than ``abnormal\'\' be as large as possible. This allows our model to accept most of the training data, yet keeping the solution as tight as possible. Experimental results on various real-world datasets show our model outperforming several baselines, including Active Outlier, Local Outlier Factor, One-class SVM, and HMM. We also report an exploratory study on detecting abnormal organizational behavior in enterprise social networks."57,Learning Finite Beta-Liouville Mixture Models via Variational Bayes for Proportional Data Clustering,"During the past decade, finite mixture modeling has become a well-established technique in data analysis and clustering. This paper focus on developing a variational inference framework to learn finite Beta-Liouville mixture models that have been proposed recently as an efficient way for proportional data clustering. In contrast to the conventional expectation maximization (EM) algorithm, commonly used for learning finite mixture models, the proposed algorithm has the advantages that it is more efficient from a computational point of view and by preventing over- and under-fitting problems. Moreover, the complexity of the mixture model (i.e. the number of components) can be determined automatically and simultaneously with the parameters estimation in a closed form as part of the Bayesian inference procedure. The merits of the proposed approach are shown using both artificial data sets and two interesting and challenging real applications namely dynamic textures clustering and facial expression recognition."59,"Knowing That, Knowing What, and Public Communication: Public Announcement Logic with Kv Operators","In the seminal work [Plaza 89], Plaza proposed the public announcement logic (PAL), which is considered as the pilot logic in the field of dynamic epistemic logic. In the same paper, Plaza also introduced an interesting ""know-value"" operator Kv  and proposed a proof system of PAL+Kv. However, the completeness of that system has been open since then. In this paper, we show that this proof system is not complete. Moreover, we generalize the Kv operator and give a complete proof system for PAL plus this generalized operator based on a complete axiomatization of epistemic logic with the same operator in the single-agent setting. We also make a thorough comparison of the expressive powers of the various related logics."60,Learning Descriptive Visual Representation by Semantic Regularized Matrix Factorization,"This paper presents a novel semantic regularized matrix factorization method for learning descriptive visual bag-of-words (BOW) representation. Although very influential in image classification, the traditional visual BOW representation has one distinct drawback. That is, for efficiency purposes, the visual vocabulary is commonly constructed for visual BOW generation by directly clustering the low-level visual feature vectors extracted from local keypoints or regions, without considering the high-level semantics of images. In other words, the traditional visual BOW representation still suffers from the semantic gap and may lead to significant performance degradation in more challenging tasks (e.g., classification of community-contributed images with large intra-class variations). To overcome this drawback, we develop a semantic regularized matrix factorization method for learning descriptive visual BOW representation by adding Laplacian regularization defined with the tags (easy to access although noisy) of community-contributed images into matrix factorization. Experimental results on two benchmark datasets show the promising performance of the proposed method."61,Verifiable Equilibria in Boolean Games,"This work is motivated by the following concern. Suppose we have a
game exhibiting multiple Nash equilibria, with little to distinguish
them except that one of these equilibria can be verified
while the others cannot. That is, one of these equilibria carries
sufficient information that, if this is the outcome, then
the players can tell that an equilibrium has been played. This
provides an argument for this equilibrium being played, instead of
the alternatives. Verifiability can thus serve to make an
equilibrium a focal point in the game. We formalise and investigate
this concept using a model of Boolean games with incomplete
information. We define and investigate three increasingly strong
types of verifiable equilibria, characterise the 
complexity of checking these, and show how
checking for the existence of verifiable equilibria can be captured
in a variant of modal epistemic logic."83,Active Learning based on Local Representation,"In many real world scenarios, active learning methods are used to select the most representative  points for labeling to reduce the expensive human actions. Traditional active learning methods fail to get a better performance without considering the local geometrical structure properly. In this paper, we propose a novel framework named  Active Learning based on Local Representation (ALLR) by taking into account the locality information directly during the sampling. Different from most of active learning methods adopting a greedy sequential strategy for optimization, we further develop an efficient two-stage iterative procedure to solve the final optimization problem efficiently. Our empirical study shows encouraging results of the proposed algorithms in comparison to other state-of-the-art active learning algorithms on both synthetic and real visual data sets."84,A Unified Approximate Nearest Neighbor Search Scheme by Combining Data Structure and Hashing,"Nowadays, Nearest Neighbor Search becomes more and more important when facing the challenge of big data. Traditionally, to solve this problem, Reserchers are mainly focusing on building effective tree structures such as kd-tree or using hashing methods to acclerate the query time. In this paper, we propose a novel unified approximate nearest neighbor search scheme to combine the advantages of both the effective tree structure and the fast hamming distance computation in hashing methods. In this way, the searching procedure in the tree structures can be further acclerated. Computational complexity analysis and extensive experiments have demonstrated the effectiveness of our proposed scheme."86,Comprehensive Score: Towards Efficient Local Search for SAT with Long Clauses,"It is widely acknowledged that stochastic local search (SLS) algorithms can effectively find models of satisfiable formulas for the Boolean satisfiability (SAT) problem. There is much interest in studying SLS algorithms on random SAT instances. Compared to random 3-SAT instances which have special statistical properties rendering them easy to solve, random instances with long clause are more like structured ones and remain very difficult. This work is devoted to efficient SLS algorithms for random instances with long clauses. We propose a novel variable property called $subscore$. By combining $subscore$ with the commonly used property $score$, we design a scoring function named $comprehensive$ $score$, which is utilized to develop a new SLS algorithm called CScoreSAT. The experiments show that CScoreSAT outperforms state-of-the-art SLS solvers, including the winners from recent SAT competitions, by one to two orders of magnitudes on random 5-SAT and 7-SAT instances. In addition, CScoreSAT significantly outperforms its competitors on the random $k$-SAT ($k=4,5,6,7$) benchmark from SAT Challenge 2012, which indicates its robustness."99,An Empirical Investigation of Ceteris Paribus Learnability,"Eliciting the preferences of users in a reliable, efficient, and minimally intrusive manner is a major step towards developing automated assistants, recommender systems, and decision support tools. Assuming that preferences are \emph{ceteris paribus} --- that, all other things being equal, a variable's preferred value depends only on the values of a subset of the other variables --- allows for their concise representation as Conditional Preference Networks (CP-nets), which one can reason with, and learn. This work offers an empirical investigation of an algorithm for \emph{reliably} and \emph{efficiently} learning CP-nets. At the same time, it introduces a novel process for \emph{efficiently reasoning} with (the learned) preferences.
"102,Deep Feature Learning using Target Priors with Applications in ECoG Signal Decoding for BCI,"Recent years have seen great interest in using deep architecture for feature learning from data.  One drawback of the commonly used unsupervised deep feature learning methods is that for supervised or semi-supervised learning tasks, the information in the target variables are not used until  the final stage when the classifier or regressor are built on the learned features. This could lead to over-generalized features that are not competitive on the specific supervised or semi-supervised learning tasks.  In this work, we describe a new learning method that combines deep feature learning on mixed labeled and unlabeled data sets in a weakly supervised learning framework. Specifically, we describe a weakly supervised learning method of a prior supervised convolutional stacked auto-encoders (PCSA), of which information in the target variables is represented probabilistically using a Gaussian Bernoulli restricted Boltzmann machine (RBM). We apply this method to the decoding problem of a BCI system. Our experimental results show that PCSA achieves significant improvement in decoding performance on benchmark data sets compared to the unsupervised feature learning as well as to the current state-of-the-art algorithms that are based on manually crafted features."103,Group Preference based Bayesian Personalized Ranking for One-Class Collaborative Filtering,"One-class collaborative filtering or collaborative ranking with implicit feedback has been steadily receiving more attention, mostly due to the ""one-class"" characteristics of data in various services, e.g., ""like"" in Facebook and ""bought"" in Amazon. Previous works for solving this problem include pointwise regression methods based on absolute rating assumptions and pairwise ranking methods with relative score assumptions, where the latter was empirically found performing much better because it models users' ranking-related preferences more directly. However, the two fundamental assumptions made in the pairwise ranking methods, (1) individual pairwise preference over two items and (2) independence between two users, may not always hold. As a response, we propose a new and improved assumption, group Bayesian personalized ranking (GBPR), via introducing richer interactions among users. In particular, we introduce group preference, to relax the aforementioned individual and independence assumptions. We then design a novel algorithm correspondingly, which can recommend items more accurately as shown by various ranking-oriented evaluation metrics on four real-world datasets in our experiments."105,Learning Canonical Correlations of Paired Tensor Sets via Tensor-to-Vector Projection,"Canonical correlation analysis (CCA) is a useful technique for measuring relationship between two sets of vector data. For paired tensor data sets, we propose a multilinear CCA (MCCA) method. Unlike existing multilinear variations of CCA, MCCA extracts uncorrelated features under two architectures while maximizing paired correlations. Through a pair of tensor-to-vector projections, one architecture enforces zero-correlation within each set while the other enforces zero-correlation between different pairs of the two sets. We take a successive and iterative approach to solve the problem. Experiments on matching faces of different poses show that MCCA outperforms CCA and 2D-CCA, while using much fewer features. In addition, the fusion of two architectures leads to performance improvement, indicating complementary information."107,Forecast Oriented Classification of Spatio-Temporal Extreme Events,"In complex dynamic systems, accurate forecasting of extreme events, such as hurricanes, is a highly underdetermined, yet very important sustainability problem. While physics-based models deserve their own merits, they often provide unreliable predictions for variables highly related to extreme events. In this paper, we propose a new supervised machine learning problem, which we call a forecast oriented classification of spatio-temporal extreme events. We formulate three important real-world extreme event classification tasks, including seasonal forecasting of (a) tropical cyclones in Northern Hemisphere, (b) hurricanes and landfalling hurricanes in North Atlantic, and (c) North African rainfall. Corresponding predictor and predictand data sets are constructed. These data present unique characteristics and challenges that could potentially motivate future machine learning research."111,A Theoretic Framework of K-means-based Consensus Clustering,"Consensus clustering emerges as a promising solution to find cluster structures from data. As an efficient approach for consensus clustering, the Kmeans based method has garnered attention in the literature, but the existing research is still preliminary and fragmented. In this paper, we provide a systematic study on the framework of K-meansbased Consensus Clustering (KCC). We first formulate the general definition of KCC, and then reveal a necessary and sufficient condition for utility functions that work for KCC, on both complete and incomplete basic partitionings. Experimental results on various real-world data sets demonstrate that KCC is highly efficient and is comparable to the state-of-the-art methods in terms of clustering quality. In addition, KCC shows high robustness to incomplete basic partitionings with substantial missing values."117,AstonTAC: An Intelligent Agent for Energy Trading,"This paper details the development and evaluation of AstonTAC, an energy broker that successfully participates in the Power Trading Agent Competition (Power TAC) in 2012. As energy broker, AstonTAC buys electrical energy from the wholesale market and sells it in the retail market. In particular, it employs the Markov Decision Processes (MDP) to purchase energy at low prices in a real time and day-ahead power wholesale market, and to keep energy supply and demand balanced. This paper describes our MDP model for energy trading in the wholesale market. Moreover, we explain how the agent determines the MDP states using conditioned Hidden Markov Model (HMM) to forecast energy demand and price. Furthermore, we evaluate the performances of our MDP model using the Power TAC pilot competition of December 2012. According to the results of the pilot competition, our agent performed stably and successfully."118,Reasoning about Continuous Uncertainty in the Situation Calculus,"Among the many approaches for reasoning about degrees of belief in
the presence of noisy sensing and acting, the logical account
proposed by Bacchus, Halpern, and Levesque is perhaps the most expressive.
While their formalism is quite general, it is restricted to fluents
whose values are drawn from discrete countable domains, as opposed to
the continuous domains seen in many robotic applications. In this
paper, we show how this limitation in their approach can be lifted.
By dealing seamlessly with both discrete distributions and continuous
densities within a rich theory of action, we provide a very general
logical specification of how belief should change after acting and
sensing in complex noisy domains."120,Change-Point Detection with Feature Selection  in High-Dimensional Time-Series Data,"Change-point detection is the problem of finding abrupt changes in time-series, and it is attracting a lot of attention in the artificial intelligence and machine learning communities. In this paper, we present a supervised learning based change-point detection approach in which we use separability of the past and future data at time t (they are labeled as +1 and -1) as a plausibility of a change-points. Based on this framework, we propose a detection measure called \\emph{additive Hilbert-Schmidt Independence Criterion} (aHSIC), which is defined as an weighted sum of HSIC scores between each feature and its corresponding binary labels. Here, HSIC is a kernel-based independence measure. A novelty of the aHSIC score is that it can incorporate feature selection during its detection measure estimation. More specifically, we first select features that are responsible for an abrupt change by supervised manner, and then compute the aHSIC score by those selected features. Thus, compared with traditional detection measures, our approach tends to be robust to noise features, and thus the aHSIC is suited for a high-dimensional time-series data.  Through extensive experiments on synthetic and real-world human activity data set, we demonstrated that the proposed change-point detection method is promising."123,Regularized Latent Least Square Regression for Cross Pose Face Recognition ,"Pose variation is one of the challenging factors for face recognition. In this paper, we propose a novel cross pose face recognition method named as Regularized Latent Least Square Regression (RLLSR). The basic assumption is that the images captured under different poses of one person can be viewed as pose-specific transforms of a single ideal object. We treat the observed images as regressor, the ideal object as response, and then formulate this assumption in the least square regression framework, so as to learn the multiple pose-specific transforms. Specifically, we incorporate some prior knowledge as two regularization terms into the least square approach: 1) the smoothness regularization, as the transforms for nearby poses should not differ too much; 2) the local consistency constraint, as the distribution of the latent ideal objects should pre-serve the geometric structure in the observed image space. We develop an iterative algorithm to simul-taneously solve for the ideal objects of the training individuals and a set of pose-specific transforms. The experimental results on the Multi-PIE dataset demonstrate the effectiveness of the proposed method and superiority over the previous methods."124,Definability of Horn Revision from Horn Contraction,"In the AGM framework, a revision function can be defined directly
from systems of spheres, epistemic entrenchment,
and etc., or indirectly through a contraction
operation via the Levi identity. A recent trend is
to construct AGM style contraction and revision
functions that operate under Horn logic. A direct
construction of Horn revision is given in [Delgrande
and Peppas, 2011]. However, it is unknown
whether Horn revision can be defined indirectly
from Horn contraction. In this paper, we address
this problem by obtaining a model-based Horn revision
through the model-based Horn contraction
studied in [Zhuang and Pagnucco, 2012]. Our result
shows that, under proper restriction, Horn revision
is definable through Horn contraction."127,"i, Poet: Chinese Poetry Composition through a Summarization Framework under Constrained Optimization","Part of the long lasting cultural heritage of China is the classical ancient Chinese poems following strict formats and linguistic representations. Automatic Chinese poetry composition is viewed as a difficult problem in computational linguistics and requires high Artificial Intelligence assistance, and has not been well addressed. In this paper, we formulate the poetry composition task as an optimization problem based on a generative summarization framework under several constraints. Given the user specified writing intents and selected formats, the system retrieves candidate terms from large poem thesaurus, and then fits the format with possible combinations of candidate terms. The optimization process under constraints is conducted via iterative term substitutions till convergence, and outputs the subset with the highest utility as the generated poem. We perform generation on large datasets of 61,960 classic poems from Tang and Song Dynasty of China. A comprehensive evaluation, using both human judgments and ROUGE scores, has been conducted, and the results demonstrate the effectiveness of our proposed approach."131,Towards Effective Prioritizing Water Pipe Replacement and Rehabilitation,"Preventative and sustainable urban water pipe maintenance is a key activity as the pipe failure can bring significant environmental issue and economic loss, especially regarding with the large-scale urban networks. To achieve optimized replacement and rehabilitation, accurate pipe failure prediction plays an important role and has been attracting attention from both academia and industry, mostly from the civil engineering field.
 This paper presents a having-deployed industrial learning based pipe failure prediction system. As an alternative to the traditional risk matrix method depending on ad-hoc heuristics, computational models are evaluated using the massive attributes data such as pipe physical properties, environmental factors, operational conditions etc. However, challenge arises when attribute data is missing as is common in practice. The self-exciting stochastic process model is adopted to capture the temporal clustering pattern of pipe failures. To our best knowledge, we are unaware of any prior work has identified this formulation towards temporal pipe failure data modeling. And we show that it outperforms a baseline model assuming linear aging with respect to failure probability."139,Towards rational deployment of multiple heuristics in A*,"The obvious way to use several admissible heuristics in A* is to take their maximum. However, computing the additional heuristics incurs a significant overhead. We examine methods that reduces this overhead, with little or no
decrease in the effectiveness of the heuristics, thereby speeding up search. 

Lazy A* is a variant of A* where heuristics are evaluated lazily, i.e, only when they become essential to a decision to be made in the A* search process.  We then present a new rational meta-reasoning based scheme, rational lazy A*. It uses a filter that decides whether to compute the more expensive heuristics at all, based on a myopic value of information estimate. 

Both methods are examined theoretically, resulting in guidelines for predicting how well these methods are expected to perform given a search domain and set of heuristics. Empirical evaluation on several domains support the theoretical results, and show the benefit of the proposed approaches.
"477,Advanced Conflict-Driven Disjunctive Answer Set Solving,"We introduce a new approach to disjunctive ASP
solving that aims at an equitable interplay between
â€œgeneratingâ€ù and â€œtestingâ€ù solver units. To this
end, we develop a new characterization of answer
sets and unfounded sets that allows for a dynamic
bi-directional exchange between orthogonal solver
units. This results in the new multi-threaded disjunctive
ASP solver claspD-2, greatly improving
the performance over existing systems."153,Causal Inference with Rare Events in Large-Scale Time-Series Data,"Large-scale observational datasets are prevalent in many areas of research, including biomedical informatics, computational social science, and finance. However, our ability to use these data for decision-making lags behind our ability to collect and mine them. One reason for this is the lack of methods for inferring the causal impact of rare events. In cases such as the monitoring of continuous data streams from intensive care patients, social media, or finance, though, rare events may in fact be the most important ones. They may, for example, signal critical changes in a patient's status or trading volume. While prior data mining approaches can identify or predict rare events, they cannot determine their impact, and probabilistic causal inference methods fail to handle inference with infrequent events. Instead, we develop a new approach to finding the causal impact of rare events that leverages the large amount of data available to infer a model of a system's functioning and evaluates how rare events explain deviations from usual behavior. Using simulated data, we evaluate the approach and compare it against others, demonstrating that it can accurately and precisely infer the effects of rare events."158,Maximal Recursive Rule: A New Social Decision Scheme,"In social choice settings with strict preferences, random dictatorship rules were characterized by Gibbard [1977] as the only randomized social choice functions that satisfy strategyproofness and ex post efficiency. In the more general domain with indifferences, RSD (random serial dictatorship) rules are the well-known and perhaps only known generalization of random dictatorship. We present a new generalization of random dictatorship for indifferences called Maximal Recursive (MR) rule as an alternative to RSD. We show that MR is polynomial-time computable, weakly strategyproof with respect to stochastic dominance, and, in some respects, outperforms RSD on efficiency."164,A Framework to Choose Trust Models for Different E-Marketplace Environments,"Many trust models have been proposed to evaluate seller trustworthiness in multiagent e-marketplaces. Their performance varies highly depending on environments where they are applied. However, it is challenging to choose suitable models for environments where ground truth about seller trustworthiness is unknown (called unknown environments). We propose a novel framework to choose suitable trust models for unknown environments, based on the intuition that if a model performs well in one environment, it will do so in another similar environment. Specifically, for an unknown environment, we identify a similar simulated environment (with known ground truth) where the trust model performing the best will be chosen as the suitable solution. Evaluation results confirm the effectiveness of our framework in choosing suitable trust models for different environments."167,A General Framework for Interacting Bayes-Optimally with Self-Interested Agents using Any Models and,"Recent advances in Bayesian reinforcement learning (BRL) have shown that Bayes-optimality is theoretically achievable by assuming the environment's latent dynamics to be represented using Flat-Dirichlet-Model (FDM). However, the independence assumption made in FDM appears ill-suited in self-interested multi-agent systems whose transition dynamics are caused mainly by the other agent's unknown (stochastic) behaviors (Section 1). This consequently creates a gap in applying BRL into self-interested multi-agent settings. To bridge this gap, we present a generalization of BRL to integrate the general class of finitely parametric models and model priors, thus allowing the practitioners greater flexibility in encoding their prior domain knowledge regarding the other agent's behavior. Empirical evaluations of its application to the transportation domain show that our approach outperforms existing work in multi-agent contexts such as BPVI [Chalkiadakis and Boutilier, 2003]."169,Linear Temporal Logic and Linear Dynamic Logic on Finite Traces,"In this paper we look into the assumption of interpreting LTL over finite traces. In particular we show that LTLf, i.e., LTL under this assumption, is less expressive than what might appear at first sight, and that at  essentially no computational cost one can make a significant increase in expressiveness and maintain the same intuitiveness of \LTLf interpreted over finite traces. Indeed, we propose a  logic, LDLf, for Lineal Dynamic Logic over finite traces,  which borrows the syntax from Propositional Dynamic Logic (PDL), but is interpreted over finite traces. Satisfiability, validity and logical implication (as well as model checking) for LDLf are PSPACE-complete as for LTLf (and LTL). "177,Breaking Symmetries in Graph Representation,"  There are many complex combinatorial problems which involve
  searching for an undirected graph satisfying a certain
  property. These problems are often highly challenging because of the
  large number of isomorphic representations of a possible solution.
  In this paper we introduce novel, effective and compact, symmetry
  breaking constraints for undirected graph search.  While incomplete,
  these prove highly beneficial in pruning the search for a graph.  We
  illustrate the application of symmetry breaking in graph
  representation to resolve several open instances in extremal graph
  theory."194,Learning from Polyhedral Sets,"Parameterized linear systems allow %are a representation formalism
for modelling and reasoning over classes of polyhedra. Collections
of squares, rectangles, polytopes, and so on, can readily be
defined by means of linear systems with parameters. In this paper,
we investigate the problem of learning a parameterized linear
system whose class of polyhedra includes a given set of example
polyhedral sets and it is minimal."199,Abstract Dialectical Frameworks Revisited,"We present various new concepts and results related to abstract dialectical frameworks (ADFs), a powerful generalization of Dung's argumentation frameworks (AFs). In particular, we show how the existing definitions of stable and preferred semantics which are restricted to the subcase of so-called bipolar ADFs can be improved and generalized to arbitrary frameworks. Furthermore, we introduce preference handling methods for ADFs, allowing for both reasoning with and about preferences. Finally, we present an implementation based on an encoding in answer set programming."215,Detecting and Tracking Disease Outbreaks in Real-time through Social Media,"The emergence and ubiquity of online social net- works have enriched this data with evolving inter- actions and communities both at mega-scale and in real-time. This data offers an unprecedented oppor- tunity for studying the interaction between society and disease outbreak. In other words, the challenge we describe in this data paper is how to extract and leverage epidemic outbreak insights from mas- sive amounts of social media data to benefit medi- cal professionals, patients, and policymakers alike. Of the characteristics associated with this data, the size of the data, the speed at which the data is be- ing generated, and the complex heterogeneity of the structure of the data are three primary issues to the so-called â€œbig dataâ€ù problem, each of which is ade- quately addressed in this paper."208,Linear Bayesian Reinforcement Learning,"  This paper proposes a simple linear Bayesian approach to reinforcement learning. We show that with an appropriate basis, a Bayesian linear Gaussian model is sufficient for accurately estimating the system dynamics, and in particular when we allow for correlated noise. Policies are estimated by first sampling a transition model from the current posterior, and then performing approximate dynamic programming on the sampled model. This form of approximate Thompson sampling results in good exploration in unknown environments. When the approximate dynamic programming is least-squares, the approach can be seen as an online, Bayesian generalisation of LSPI, where the empirical transition matrix is replaced with a sample from the posterior.
"213,Automated Generation of Interaction Graphs for Value-Factored Decentralized POMDPs,"The Decentralized Partially Observable Markov Decision Process (Dec-POMDP) is a powerful model for multi-agent planning under uncertainty, but its applicability is hindered by its high complexity -- solving Dec-POMDPs optimally is NEXP-hard. Recently, Kumar \emph{et al.} introduced the Value Factorization (VF) framework, which exploits decomposable value functions that can be factored into subfunctions. This framework has been shown to be a generalization of several specialized models such as TI-Dec-MDPs, ND-POMDPs and TD-POMDPs, which leverage different forms of sparse agent interactions to improve the scalability of planning.  Existing algorithms for these models assume that the interaction graph of the problem is given. So far, no studies have addressed the generation of interaction graphs.  In this paper, we address this gap by introducing three algorithms to automatically generate interaction graphs for models within the VF framework and establish lower and upper bounds on the expected reward of an optimal joint policy. We illustrate experimentally the benefits of these techniques for sensor placement in a decentralized tracking application.
"216,Multi-modal Distance Learning,"Multi-modal data is dramatically increasing with the fast growth of social media. Learning a good distance measure for data with multiple modalities is of vital importance for many applications, including retrieval, clustering, classification and recommendation. In this paper, we propose an effective and scalable multi-modal distance learning framework. Based on multi-wing harmonium model, our method provides a principled way to embed data of arbitrary modalities into a single latent space and distance supervision (e.g., labeled ``similar\"" and ``dissimilar\"" pairs ) are encoded in the latent space by minimizing the distance of similar pairs while pushing dissimilar pairs apart. The parameters are learned by jointly maximizing data likelihood of the random filed model and minimizing loss induced by distance supervision, which seeks balance of explaining data well and generating an effective distance metric and can naturally avoid overfitting. Our method is highly scalable and efficient. We specialize the general framework to text/image data. Experiments on retrieval and classification tasks demonstrate the effectiveness and scalability of our method."221,Selecting Algorithms by Hierarchical Clustering,We introduce a new algorithm selector method based on cost-based hierarchical clustering. Comparisons with existing portfolio techniques on SAT and MAXSAT show improved performance.223,Adaptive management of migratory birds under sea level rise,"The best practice method for managing ecological systems under uncertainty is adaptive management (AM), an iterative process of reducing uncertainty while simultaneously optimizing a management objective. Existing solution methods used for AM problems assume that the system is stationary and that dynamics are described by one of a set of pre-defined models. In reality ecological systems are rarely stationary and evolve over time. Importantly, the effects of climate change on populations are unlikely to be captured by stationary models. Practitioners need efficient algorithms to implement AM on real world problems. AM can be formulated as a simplified mixed observability Markov Decision Process (MOMDP), which allows the state space to be factored and shows promise for the rapid resolution of large problems. We provide an ecological dataset and performance metrics for the AM of a network of shorebird species utilizing the East Asian-Australasian flyway given uncertainty about the rate of sea level rise. The non-stationary system is modelled as a stationary POMDP containing hidden alternative models with known probabilities of transition between them. We challenge the POMDP community to exploit the simplifications allowed by structuring the AM problem as a MOMDP and improve our benchmark solutions."227,Probabilistic Matrix Factorization with Sparse Covariance Prior for Collaborative Filtering,"Matrix factorization (MF) is a popular collaborative filtering approach for recommender systems due to its simplicity and effectiveness.  Existing MF methods either assume that all latent features are uncorrelated or assume that all are correlated.  To address the issue of what structure should be imposed on the features, we investigate the covariance matrix of the latent features learned from real data.  Based on the finding, we propose an MF model with a sparse covariance prior that favors a sparse yet non-diagonal covariance matrix.  Not only can this reflect the semantics more properly, but imposing sparsity can also have a side effect of preventing overfitting.  Starting from a probabilistic generative model with a sparse covariance prior, we formulate the model inference problem as a maximum a posteriori (MAP) estimation problem.  The optimization procedure makes use of stochastic gradient descent and majorization-minimization.  For empirical validation, we conduct experiments using the MovieLens and Netflix datasets to compare the proposed method with two strong baselines which use different priors.  The results show that our sparse covariance prior can lead to performance improvement."237,Monte Carlo *-Minimax Search,"This paper introduces Monte-Carlo *-Minimax Search (MCMS), a new Monte-Carlo search algorithm for finite, turned-based, stochastic, two-player, zero-sum games of perfect information. Through a combination of sparse sampling and pruning techniques in the expectimax framework, MCMS allows deep plans to be constructed in the challenging setting of densely stochastic games, i.e., games where one would rarely ever expect to see the same successor state multiple times at any particular chance node. The performance of the algorithm is evaluated on four games: Pig, EinStein W&uuml;rfelt Nicht!, Canâ€™t Stop, and Ra."472,Extending Simple Tabular Reduction with Short Supports,"Constraint propagation is one of the key techniques in constraint programming, and a large body of work has built up around it.  Special-purpose constraint propagation algorithms frequently make implicit use of short supports --- by examining a subset of the variables, they can infer support (a justification that a variable-value pair may still form part of a solution to the constraint) for all other variables and values and save substantial work. Recently short supports have been used in general purpose propagators, and (when the constraint is amenable to short supports) speed ups of more than three orders of magnitude have been demonstrated. 

In this paper we present ShortSTR2, a development of the Simple Tabular Reduction algorithm STR2. We show that ShortSTR2 is complementary to the existing algorithms ShortGAC and HaggisGAC that exploit short supports, while being much simpler. When a constraint is amenable to short supports, the short support set can be exponentially smaller than the full-length support set. Therefore ShortSTR2 can efficiently propagate many constraints that STR2 cannot even load into memory. 

We also show that ShortSTR2 can be combined with a simple algorithm to identify short supports from full-length supports, to provide a superior drop-in replacement for STR2. 
"242,Iterated Boolean Games,"Iterated games have been widely studied in the game theory literature. In this paper, we study iterated Boolean games. These are games in which players repeatedly choose values for the propositional variables over which they are assigned control. Our  model of iterated Boolean games assumes that players have goals represented as formulae of Linear Temporal Logic (LTL), a widely used formalism for expressing properties of state sequences.  To  model the strategies that players use in such games, we use a finite state machine model. After introducing and  formally defining iterated Boolean games, we investigate the  computational complexity of their associated game theoretic decision  problems as well as semantic conditions characterising the kinds of   LTL properties that are preserved by equilibrium points (pure Nash equilibria) whenever they exist. "247,Breakout local search for the vertex separator problem,"We present Breakout Local Search (BLS), the first heuristic approach for the vertex separator problem (VSP). BLS is a recent metaheuristic that follows the general framework of the popular Iterated Local Search (ILS) with a particular focus  on the perturbation strategy. Based on some relevant information on search history, it tries to introduce the most suitable degree of diversification by determining adaptively the number and type of moves for the next perturbation phase. The proposed heuristic is highly competitive with the exact state-of-art approaches from the literature on the current VSP benchmark. Moreover, we are the first ones to use for this problem a set of large publicly available graphs with up to 3000 vertices, which constitutes a new challenging benchmark for VSP approaches."251,Multi-View Sparse Reconstruction for Weakly Supervised Semantic Segmentation,"We propose a Multi-View Sparse Reconstruction (MVSR) framework for weakly supervised semantic segmentation. Compared with traditional fully supervised methods, weakly supervised methods make use of only image level labels during training. Our MVSR framework provide reliable approximate groundtruth of pixel labels for training classifiers, i.e., gives an effective criterion for the accuracy of classifiers. For each class, MVSR framework recovers the semantic subspace structure of it by sparse reconstructions which integrates multiple types of heterogenous features. We also develop an Iterative Merging Update(IMU) algorithm to efficiently calculate the optimal value under MVR framework. Experimental results on two challenging datasets shows that MVSR framework outperforms previous weakly supervised methods, and competes with state-of-the-art fully supervised methods.
"269,Manifold Alignment Preserving Global Geometry,"This paper proposes a novel algorithm for manifold alignment
preserving global geometry. This approach constructs mapping
functions that project data instances from different input domains
to a new lower-dimensional space, simultaneously matching the
instances in correspondence and preserving global distances
between instances within the original domains. In contrast to
previous approaches, which are largely based on preserving local
geometry, the proposed approach is suited to applications where
the global manifold geometry needs to be respected. We evaluate
the effectiveness of our algorithm for transfer learning in two
real-world cross-lingual information retrieval tasks."276,Extending Stable Models to the Full Propositional Language with Justifications,"Answer set programming is the most appreciated framework for non-monotonic reasoning. Stable model semantics, as the semantics behind this success, has been subject to many extensions. Equilibrium models and FLP semantics are the two main extensions to stable model semantics. Despite their very interesting foundations, they both still suffer from two problems: intended models according to such extensions (1) are not guaranteed to be minimal, and (2) more importantly, may have self-justifications (i.e., inclusion of an atom in an intended model might be justfied by its own pertinence). Both of these properties directly violate the spirit of stable model semantics.

Present paper introduces an extension to stable model semantics, called supported semantics (based on derivability in intuitionistic propositional logic), that guarantees both the minimality of a model and its being well-justified. We also discuss how supported models relate to other existing semantics for non-monotonic reasoning including equilibrium models. Last, but not the least, we discuss the complexity of reasoning about supported models and show that, despite using full intuitionistic reasoning for the wide class of propositional programs, the complexity of brave/cautious reasoning in supported semantics remains the same as such reasonings in equilibrium models, i.e., the property of being well-justified comes for no additional computational cost."278,Shifted Subspaces Tracking on Sparse Outlier,"In low-rank \&amp; sparse matrix decomposition, the sparse part is often assumed to be generated by a random model. Analysis to its structure, which is of central interest in various problems, is rarely considered. One such example is tracking multiple object flows in video. We introduce ``shifted subspace tracking (SST)'' to both separate the object flows and recover their trajectories by exploring their shifted subspaces on the sparse outliers. SST can be summarized in two steps, i.e., background modeling and flow tracking. In step 1, we propose ``semi-soft GoDec'' to separate all the moving objects as the sparse outlier $S$ from the data matrix $X$. Its soft-thresholding of $S$ significantly speeds up GoDec and facilitates the parameter setting. In step 2, we treat the sparse $S$ in step 1 as the new $X$, and develop ``SST algorithm'' decomposing $X$ as $X=\sum\nolimits_{i=1}^k L(i)\circ\tau(i)+S+G$, wherein $L(i)$ denotes the subspace of the $i^{th}$ flow after transformation $\tau(i)$. The decomposition solves $k$ sub-problems of alternating minimization in sequel, each of which recovers a $L(i)$ and its $\tau(i)$ with randomized acceleration. Sparsity of $L(i)$ and smoothness between adjacent frames are explored to save computations. We justify the promising performance of SST on four surveillance video sequences."282,Prior-Free Exploration Bonus for and beyond near Bayes-Optimal Behavior,"We study Bayesian reinforcement learning (RL) as a solution of the exploration-exploitation dilemma. As full Bayesian planning is intractable except for special cases, previous work has proposed several approximation methods. However, these were often computationally expensive or limited to Dirichlet priors. In this paper, we propose a new algorithm that is fast and of polynomial time for near Bayesian optimal policy with any prior distributions that are not greatly misspecified. Perhaps even more interestingly, the proposed algorithm can naturally avoid being misled by incorrect beliefs, while effectively utilizing useful parts of prior information. It can work well even when an utterly misspecified prior is assigned. In that case, the algorithm will follow PAC-MDP behavior instead, if an existing PAC-MDP algorithm does so. The proposed algorithm naturally outperformed other existing algorithms on a standard benchmark problem."283,Misleading Opinions Provided by Advisors: Dishonesty or Subjectivity,"It is indispensable for users to evaluate the trustworthiness of other users (referred to as advisors), to cope with possible misleading opinions provided by them. Advisors' misleading opinions may be induced by their dishonesty, subjectivity difference with users, or both. Existing approaches do not well distinguish the two different causes. In this paper, we propose a novel probabilistic graphical trust model to separately consider these two factors, involving three types of latent variables: benevolence, integrity and competence of advisors, trust propensity of users, and subjectivity difference between users and advisors. Experimental results on real datasets demonstrate that our method advances state-of-the-art approaches to a large extent."284,Bayesian Nonparametric Feature Construction for Inverse Reinforcement Learning,"In this paper, we address the feature construction in inverse reinforcement learning (IRL). Most IRL algorithms assume that the reward function is a linear function of pre-defined state and action features. However, it is often difficult to specify the set of features that can represent the reward function as a linear function. We propose a Bayesian nonparametric approach to identify the reward function structure by learning useful composite features, which are assumed to be logical conjunctions of the atomic features so that the reward function is still a linear function of composite features. We empirically show that our approach is able to learn composite features that represent important aspects in the reward function. We also show that our approach can predict the taxi driverâ€™s haviour with higher accuracies using real trace data."285,Efficient Vote Elicitation under Candidate Uncertainty,"Top-k voting is an especially natural form of partial vote elicitation in which only length k prefixes of rankings are elicited. We analyze the ability of top-k vote elicitation to correctly determine true winners, with high probability, given probabilistic models of voter preferences and can- didate availability. We provide bounds on the minimal value of k required to determine the correct winner un- der the plurality and Borda voting rules in both the worst- case preference profiles and under impartial culture and Mallows models; and we derive conditions under which the special case of zero-elicitation (i.e., k = 0) produces the correct winner. Empirical results confirm the value of top-k voting."288,The Dynamics of Reinforcement Social Learning in Cooperative Multiagent Systems,"Coordination in cooperative multiagent systems is an important problem in multiagent learning literature. In practical complex environments, the interactions between agents can be sparse, and each agent's interacting partners may change frequently and randomly. To this end, we investigate the multiagent coordination problems in cooperative environments under the social learning framework. We consider a large population of agents where each agent interacts with another agent randomly chosen from the population in each round. Each agent learns its policy through repeated interactions with the rest of agents via social learning. It is not clear a priori if all agents can learn a consistent optimal coordination policy in such a situation. We distinguish two types of learners: individual action learner and joint action learner. The learning performance of both learners are evaluated under a number of challenging cooperative games, and the influence of the information sharing degree on the learning performance is investigated as well."292,First-Order Expressibility and Boundedness of Disjunctive Logic Programs,"In this paper, the fixed point semantics developed in [Lobo et al., 1992, Foundations of Disjunctive Logic Programming] is generalized to disjunctive logic programs with default negation and over non-Herbrand structures. Based on this semantics, the boundedness of a disjunctive logic program is naturally defined. By using the tool of ultraproducts, a preservation theorem, which asserts that a disjunctive logic program without default negation is bounded if and only if it has a first-order equivalent, is then obtained. For disjunctive logic programs with default negation, a sufficient condition to assure the first-order expressibility is also proposed."296,Semi-Supervised Learning with Manifold Fitted Graphs,"In this paper we propose a locality-constrained and sparsity-encouraged manifold fitting approach, aiming at capturing the locally sparse manifold structure into neighborhood graph construction via a principled optimization model. The proposed model formulates the neighborhood graph construction problem as a sparse coding problem with the locality constraint, therefore achieving simultaneous neighbor selection and edge weight optimization. The core idea underlying our model is to perform a sparse manifold fitting task for each data point so that the close-by points lying on the same local manifold are automatically chosen to connect and meanwhile the connection weights are acquired by simple geometric reconstruction. We term the neighborhood graph generated by our proposed optimization model M-Fitted Graph since such a graph stems from sparse manifold fitting. To evaluate the robustness and effectiveness of M-fitted graphs, we leverage graph-based semi-supervised learning as the testbed. Extensive experiments carried out on six benchmark datasets validate that the proposed M-fitted graph is superior to state-of-the-art neighborhood graphs in terms of classification accuracy of popular graph-based semi-supervised learning methods."297,Reduced Heteroscedasticity Linear Regression for Nystrom Approximation ,"The Nystrom method is a well known sampling based low-rank matrix approximation approach. It is usually considered to be originated from the numerical treatment of integral equations and eigendecomposition of matrices. In this paper, we present a novel point of view for the Nystrom approximation. We show that theoretically the Nystrom method can be regraded as a set of pointwise ordinary least square linear regressions of the kernel matrix, sharing the same design matrix. With the new interpretation, we are able to analyze the approximation quality based on the fulfillment of the homoscedasticity assumption and explain the success and deficiency of various sampling methods. We also empirically demonstrate that positively skewed explanatory variable distributions can lead to heteroscedasticity. Based on this discovery, we propose to use non-symmetric explanatory functions to improve the quality of the Nystrom approximation with almost no extra computational cost. Experiments show that positively skewed datasets widely exist, and our method exhibits good improvements on these datasets."298,Robust Rank-K Matrix Completion,"Privacy is an important concern for online users.  Deciding whether another user should be trusted  can be formulated to predicting link between users. Such link scarce social graph  can be modeled as reconstructing a matrix M from noisy observations of a small and random subset of its entries. Recent work often focused on the assumption that the data matrix has low-rank and used the trace norm minimization to approximate the rank minimization. However, more recent research casts doubts about this category of learning models because such yielded solution could be indeed not low-rank and unstable for practical applications. In this paper, instead of using trace norm minimization, we propose a new robust rank-k matrix completion method, which explicitly seek a matrix with exact rank (low). Moreover, our method is robust to noise or corrupted observations. We optimize the new objective function in an alternative manner, based on a combination of ancillary variables and Augmented Lagrangian Methods (ALM). We perform the experiments on four categories of real-world data sets and all empirical results demonstrate the effectiveness of our method."300,Learning High-Order Task Relationships in Multi-Task Learning,"Multi-task learning is a way of bringing inductive transfer studied in human learning to the machine learning community.  A central issue in multi-task learning is to model the relationships between tasks appropriately and exploit them to aid the simultaneous learning of multiple tasks effectively.  While some recent methods model and learn the task relationships from data automatically, only pairwise relationships can be represented by them.  In this paper, we propose a new model, called Multi-Task High-Order relationship Learning (MTHOL), which extends in a novel way the use of pairwise task relationships to high-order task relationships.  We first propose an alternative formulation of an existing multi-task learning method.  Based on the new formulation, we propose a high-order generalization leading to a new prior for the model parameters of different tasks.  We then propose a new probabilistic model for multi-task learning and validate it empirically on some benchmark datasets."310,Modeling Lexical Cohesion for Document-Level Machine Translation,"Lexical cohesion arises from a chain of lexical items that establish links between sentences in a text. In this paper we propose three different models to capture lexical cohesion for document-level machine translation: (a) a direct reward model where translation hypotheses are rewarded whenever lexical cohesion devices occur in them, (b) a conditional probability model where the appropriateness of using lexical cohesion devices is measured, and (c) a mutual information trigger model where a lexical cohesion relation is considered as a trigger pair and the strength of the association between the trigger and the triggered item is estimated by mutual information. We integrate the three models into hierarchical phrase-based machine translation and evaluate their effectiveness on the NIST Chinese-English translation tasks with large-scale training data. Experiment results show that all three models can achieve substantial improvements over the baseline and that the {\em mutual information trigger model} performs better than the others."312,Thinking of Images as What They Are: Compound Matrix Regression for Image Classification,"In this paper, we propose a new classification framework for image matrices with increased degree of freedom. The approach is realized by learning two groups of classification vectors for each dimension of the image matrices. On top of that, we extend the two-dimensional classification method to a semi-supervised classifier which leverages both labeled and unlabeled data. A fast iterative solution is then proposed to solve the objective function. The proposed method is evaluated by several different applications. The experimental results show that our method outperforms several classification approaches. In addition, we observe that our method attains respectable classification performance even when only few labeled training samples are provided. This advantage is especially desirable for real-world problems since precisely annotated images are scarce."319,Multi-view Maximum Entropy Discrimination,"Maximum entropy discrimination (MED) is a general framework for discriminative estimation based on the well known maximum entropy principle, which embodies the Bayesian integration of prior information with large margin constraints on observations. It is a successful combination of maximum entropy learning and maximum margin learning, and can subsume support vector machines (SVMs) as a special case. In this paper, we present a multi-view maximum entropy discrimination framework that is an extension of MED to the scenario of learning with multiple feature sets. Different from existing approaches to exploiting multiple views, such as co-training style algorithms and co-regularization style algorithms, we propose a new method to make use of the distinct views where classification margins from these views are required to be identical. We give the general form of the solution to the multi-view maximum entropy discrimination, and provide an instantiation under a specific prior formulation which is analogical to a multi-view version of SVMs. Experimental results on real-world data sets show the effectiveness of the proposed multi-view maximum entropy discrimination approach."323,Tag-Weighted Topic Model for Mining  Documents  with Labels,"Semi-Structured data contains both text data and document metadata, such as papers with authors and web pages with different labels(tags).
In this paper we propose a novel method to model the semi-structured data, called the Semi-Structured LDA (SSLDA). 
The SSLDA is a framework that leverages all the metadata(labels or tags) which appears in one document to infer the topic components for each document.
This allows SSLDA to directly learn document-topic distributions, but also infers the labels(tags) topic distributions for the purpose of classifications, clusters, recommendations and so on. 
Besides, the SSLDA automatically infers the probabilistic importance of different labels(tags). 
Efficient variational inference and EM algorithm is presented for model parameter estimation. 
We demonstrate SSLDA's improved expressiveness over traditional LDA or other labeled topic models with visualizations of three corpus of semi-structured data, resulting in document modeling, text classification and label prediction."325,Map Matching with Multiple Route Metrics,"We study map matching, the problem of estimating the route that is traveled by a driver, where the observation points with the Global Positioning System (GPS) are available.  The state-of-the-art approach for this problem is a Hidden Markov Model (HMM).  We propose a particular transition probability between latent road segments by the use of the number of turns in addition to the travel distance between the latent road segments.  We show, through numerical experiments, that the error with the map matching with the state-of-the-art HMM can be reduced substantially with the proposed transition probability."328,Hartigan's $K$-means vs. Lloyd's $K$-means -- is it time for a change?,"Hartigan's method for $k$-means clustering holds several potential advantages compared to the classical and prevalent optimization heuristic known as Lloyd's algorithm. For example, it was recently shown that the set of local minima of Hartigan's algorithm is a subset of those of Lloyd's method. Here, we develop a closed-form expression that allows to establish Hartigan's method for $k$-means clustering with any Bregman divergence. We then further strengthen the case of preferring Hartigan's algorithm over Lloyd's algorithm. Specifically, we characterize a range of problems for which {\it any\/} random partition represents a local minimum for Lloyd's algorithm, while Hartigan's algorithm easily converges to the correct solution. Extensive experiments on synthetic data and real world data further support our theoretical analysis. "331,Learning Domain Differences Automatically for Dependency Parsing Adaptation,"In this paper, we address on the relation between domain differences and domain adaptation for dependency parsing. Quantitative analyses show that inconsistent behavior of same features cross-domain, rather than word or feature coverage, is the major cause of performances decrease of out-domain model. Based on the analyses, a DA method is proposed to automatically learn which features are ambiguous cross domain according to errors made by out-domain model on in-domain training data. The results of dependency parser adaptation from WSJ to Genia and Question bank showed that our method has significant improvements on small in-domain datasets where DA is mostly in need. Additionally, we achieve improvement on the published best results of CoNLL07 shared task on domain adaptation."339,Endogenous Boolean Games,"In boolean games players exercise control over propositional variables and strive to achieve a goal formula whose realization might require the opponents' cooperation. Recently, a theory of 'incentive engineering' for such games has been devised, where an external authority, i.e. the principal, steers the outcome of the game towards certain 'desirable' properties consistent with players' goals, by imposing a taxation mechanism on the players that makes the outcomes that do not comply with these properties less appealing to them. The present contribution stems from a complementary perspective and studies, instead, how boolean games can be transformed from inside, rather than from outside, by endowing players with the possibility of sacrificing a part of their payoff received at a certain outcome in order to convince other players to play a certain strategy. What we call here 'endogenous boolean games' (EBGs) boils down to enriching the framework of boolean games with the machinery of side payments coming from game theory. We analyze equilibria in EBGs, showing the preconditions needed for desirable outcomes to be achieved without external intervention. Finally, making use of taxation mechanism, we show how transform an EBG in such a way that desirable outcomes can be realized independently of side payments."340,Transition Constraints: A Study on the Computational Complexity of Qualitative Change,"Many formalisms discussed in the literature on qualitative spatial reasoning are
only designed for expressing static spatial constraints.
However, dynamic situations arise in virtually all applications of these
formalisms, which makes it necessary to study variants and extensions involving
change.

This paper presents a study on the computational complexity of qualitative
change. More precisely, we discuss the reasoning task to find a solution to a
temporal sequence of static reasoning problems where this sequence is subject to additional transition constraints.
Our focus is primarily on smoothness and continuity constraints:
we show how such transitions can be defined as relations and expressed within
qualitative constraint formalisms.
Our results demonstrate that for point-based constraint formalisms the
interesting fragments become NP-hard in the presence of continuity constraints,
even if the satisfiability problem of its static descriptions is tractable."345,Learning Optimal Auction Mechanism in Sponsored Search,"Sponsored search is an important monetization channel for search engines. In sponsored search, an auction mechanism is used to select the ads shown to users and determine the prices charged from advertisers when their ads are clicked. There have been several pieces of work in the literature that design revenue-optimal auction mechanism from a game-theoretic aspect. However, due to some ideal assumptions in game theory, the practical values of these studies are not very clear. In this paper, we propose a different approach, which optimizes the empirical revenue of the auction mechanism based on historical data using machine learning methods. This approach is non-trivial due to second-order effect, any observed bidding data will be changed if a new auction mechanism is deployed. To tackle this challenge, we first learn a Markov model to describe how advertisers change their bid during historical auctions, and then optimize the empirical revenue on the future bidding data predicted by this model with respect to an auction mechanism. We prove that the empirical revenue will converge when the prediction period approaches infinity, and a Genetic Programming algorithm can be used to effectively optimize this empirical revenue. Our experiments show that the proposed approach can produce a much more effective auction mechanism than several baselines."346,Protein Function Prediction using Networks Integration,"High-throughput experimental techniques provide a wide variety of heterogeneous proteomic data. To utilize the information spread across multiple sources in a combined fashion, several methods follow a two-phased approach: they first optimize the weights on individual graph kernels (or networks) to  produce a composite kernel, and then train a classifier on the composite kernel. As such, these methods result in an optimal composite kernel, but not necessarily in an optimal classifier. On the other hand, some methods optimize the loss of binary classifiers, and learn weights for the different kernels iteratively. A protein has multiple functions, and each function can be viewed as a label. These methods solve the problem of optimizing weights on the input kernels for each of the labels. This is computationally expensive and ignores inter-label correlations.

In this paper, we propose a method called Protein Function Prediction using Network Integration (ProNet). ProNet iteratively optimizes the phases of learning optimal weights and reducing the empirical loss of a multi-label classifier for each of the labels simultaneously, using a combined objective function. ProNet can assign larger weights to smooth graph kernels and downgrade the weights on noisy kernels. We evaluate the ability of ProNet to predict the function of proteins using several standard benchmarks. We show that our approach performs better than previously proposed protein function prediction approaches that use multiple networks integration, and multi-label multiple kernel learning methods.

"355,Harmonious Hashing,"Hashing-based fast search techniques have attracted great attention in both research and industry areas recently. Many existing hashing approaches encoded data with projection-based hash functions and measured the similarity of objects via Hamming distance, but ignored a crucial fact for the projected data -- the dimensions with largest variances hold the most energy or information of source data, while Hamming distance has equivalent weight on each bit after binarization, i.e., some bits are energy overloaded and some less informative. In this paper, we introduce a novel hashing algorithm Harmonious Hashing, adjusting projected data to fit the Hamming distance metric. Specifically, we learn a set of optimized projections holding the maximum energy of source data in the all and equivalent on each.
Despite the extreame simplicity, our method performs comparably or outperforms superiorly to many state-of-the-art hashing methods on large-scale and high-dimensional nearest neighbor search experiments."356,Elicitation and Approximately Stable Matching with Partial Preferences,"Algorithms for stable marriage and related matching problems typically assume that full preference information is available. While the Gale-Shapley algorithm can be viewed as a means of eliciting preferences incrementally, it does not prescribe a general means for matching with incomplete information, nor is it designed to minimize elicitation. We propose the use of maximum regret to measure the (inverse) degree of stability of a matching with partial preferences; minimax regret to find matchings that are maximally stable in the presence of partial preferences; and heuristic elicitation schemes that use max regret to determine relevant preference queries. We show that several of our schemes find stable matchings while eliciting considerably less preference information than Gale-Shapley and are much more appropriate in settings where approximate stability is viable."364,Variable Elimination in Binary CSP via Forbidden Patterns,"A variable elimination rule allows the polynomial-time identification of certain variables whose elimination does not affect the satisfiability of an instance. Variable elimination in the constraint satisfaction problem (CSP) can be used in preprocessing or during search to reduce search space size. We show that there are essentially just four variable elimination rules defined by forbidding
generic sub-instances, known as irreducible patterns, in arc-consistent CSP instances. One of these rules is the Broken Triangle Property, whereas the
other three are novel."360,PageRank with Priori: An Influence Propagation Perspective,"With the advance of Internet and social networks, there has been a fair amount of recent activity on measuring nodes' authority and modelling social influence. Correspondingly, many different algorithms have been proposed for each target, such as the PageRank algorithm for authority computation and the Independent Cascade~(IC) model for influence propagation. However, to the best of our knowledge, less attention has been paid to exploring and building
connections between these two applications. To that end, in this paper, we provide a focused study on understanding of PageRank as well as the relationship between PageRank and existing social influence analysis. Along this line, we first propose a linear social influence model which has been shown the traditional PageRank is a special case of this model. Furthermore, we demonstrate that more generalized priories can be included for enhancing authority computation. In addition, to deal with the challenge of high computation complexity, we design an upper bound for top authoritative nodes identification which works for all the priories. Finally, we evaluate our discoveries by experiments on a real data set that we downloaded from DBLP website."365,Listen to the Crowd: Automated analysis of Live Events via Aggregated Twitter Sentiment,"Individuals often express their opinions on social media platforms like Twitter and Facebook during events like the U.S. Presidential debate and the Oscar award ceremony. Gleaning insights from their posts is of importance for analyzing the impact of the event. In this work, we consider the problem of identifying segments and topics of an event that gained praise or proved controversial, according to aggregated Twitter responses. We propose a constrained matrix factorization approach to learn factors about segments, topics, and sentiments. To regulate the learning process, several constraints based on prior knowledge on sentiment lexicon, sentiment orientations on a few documents as well as tweets alignments to the event are enforced. We implement our approach using simple update rules to get the optimal factorization. We evaluate the proposed model both quantitatively and qualitatively on four large-scale tweet datasets associated with four events from different domains to show that it improves significantly over baseline models."373,Externalities in Cake Cutting,"The cake cutting problem models the fair division of a heterogeneous good between multiple agents. Previous work assumes that each agent derives value only from its own piece. However, agents may also care about the pieces assigned to other agents; such externalities naturally arise in fair division settings. We extend the classical model to capture externalities, and generalize the classical fairness notions of proportionality and envy-freeness. Our technical results characterize the relationship between these generalized properties, establish the existence or nonexistence of fair allocations, and explore the computational feasibility of fairness in the face of externalities. "377,Action-Model Acquisition from Noisy Plan Traces,"There is increasing awareness in the planning community that the burden of specifying complete domain models is too high, which impedes the applicability of planning technology in many real world domains. Although there have been many learning approaches that help automatically creating domain models, they all assume plan traces (training data) are \emph{correct}. In this paper, we would like to remove the assumption, allowing plan traces to be with noises. Compared to collecting large amount of correct plan traces, it is much easier to collect noisy plan traces, e.g., sensors can be used to observe noisy plan traces. We consider a novel solution for this challenge that can learn action models from noisy plan traces. We create a set of random variables to capture the possible correct plan traces behind the observed noisy ones, and build a graphical model to describe the physics of the domain. We will present the details of our approach and present an empirical evaluation of our method in comparison to a state-of-the-art learning system that depends on correct plan traces."378,Refining Incomplete Planning Domain Models Through Plan Traces,"Most existing work on learning planning models assumes that the entire model needs to be learned from scratch. A more realistic situation is that the planning agent has a partial model which it needs to refine through learning. In this paper we propose and evaluate a method for doing this. Our method takes as input a partial domain model (with missing preconditions and effects in the actions), as well as a set of plan traces that are known to be correct. It outputs a â€œrefinedâ€ù model that not only captures additional precondition/effect knowledge about the given actions, but also provides â€œmacro actions.â€ù In the first phase, we mine candidate macros mined from the plan traces, and in the second phase we learn precondition/effect models both for the primitive actions and the macro actions, and finally we use the refined model to do planning. We use a MAX- SAT framework for learning, where the constraints are derived from the executability of the given plan traces, as well as the preconditions/effects of the given partial model. Unlike traditional macro-action learners which use macros to increase the efficiency of planning (in the context of a complete model), our motivation for learning macros is to increase the accuracy of the plans generated with the refined model. We demonstrate the effectiveness of our approach through a systematic empirical evaluation."382,A Consensual Linear Opinion Pool,"An important question when eliciting opinions from experts is how to aggregate the reported opinions. In this paper, we propose a pooling method to aggregate expert opinions. Intuitively, it works as if the experts were continuously updating their opinions in order to accommodate the expertise of others. Each updated opinion takes the form of a linear opinion pool, where the weight that an expert assigns to a peer\'s opinion is inversely related to the distance between their opinions. In other words, experts are assumed to prefer opinions that are close to their own opinions. We prove that such an updating process leads to consensus, i.e., the experts all converge towards the same opinion. Further, we show that if rational experts are rewarded using the quadratic scoring rule, then the assumption that they prefer opinions that are close to their own opinions follows naturally. We empirically demonstrate the efficacy of the proposed method using real-world data."387,Improved Bin Completion for Optimal Bin Packing and Number Partitioning,"The bin-packing problem is to partition a multiset of n numbers
  into as few bins of capacity C as possible, such that the sum of
  the numbers in each bin does not exceed C.  We compare two
  existing algorithms for solving this problem: bin completion and
  branch-and-cut-and-price. We show experimentally that the problem
  difficulty and dominant algorithm are a function of n, the
  magnitude of the input elements and the number of bins in an optimal
  solution. We describe two improvements to bin completion which
  result in up to six orders of magnitude speedup as compared to the
  original algorithm. We also report up to six orders of magnitude
  speedup compared to a state of the art branch-and-cut-and-price
  algorithm written by Gleb Belov. We also show instances of bin
  packing for which branch-and-cut-and-price outperforms bin
  completion. We then explore a closely related problem, the
  number-partitioning problem, and show that an algorithm based on
  improved bin packing is up to three orders of magnitude faster than
  the state of the art solver called DIMM. Finally, we describe how to
  use number partitioning to generate difficult bin-packing instances."388,Hierarchical Bayesian Matrix Factorization with Side Information,"Bayesian Matrix factorization has proven to be useful in collaborative prediction,
since Bayesian approaches alleviate the overfitting problem by integrating out all model parameters.
However, in collaborative prediction problems Bayesian approaches still suffer from
the cold-start problem in which the users or items do not have sufficient number of given ratings.
To resolve this limitation of Bayesian approaches to matrix factorization,
we propose hierarchical Bayesian matrix factorization methods,
where the side information, such as content information or demographic user data,
is associated to each latent factor through a normal-Wishart prior distribution
whose parameters are directly modeled by this side information with a regressor.
We have developed two matrix factorization methods according to how to learn
the regressors in the variational Bayesian framework.
In addition, we provide Bayesian Cram\'{e}r-Rao Bounds for our matrix factorization models,
showing that the hierarchical Bayesian matrix factorization with side information
improves reconstruction over the conventional hierarchical Bayesian matrix factorization without side information.
Numerical experimental results demonstrate that our proposed methods outperform
existing matrix factorization methods with side information
for the prediction of missing rating entries in the cold-start condition."405,Plan Quality Optimisation via Block Decomposition,"AI planners have to compromise between the speed of the planning process
and the quality of the generated plan. Planners based on greedy heuristic
search are often able to find plans quickly, but the quality of their
solutions is often poor. Planners that guarantee solution optimality,
or bounded sub-optimality, on the other hand, do not scale up to large
problems. Anytime planners try to balance these objectives by producing
plans of better quality over time, but current anytime planners often
are not effective at making use of increasing runtime beyond the first
few minutes.
We present a new method of continuing plan improvement, by decomposing
a given plan into subplan and iteratively optimising each subplan locally.
The decomposition exploits block-structured plan deordering to identify
coherent subplans, which make sense to treat as units.
Coupled with a fast, non-optimal, planner to generate an initial, possibly
low-quality, solution, this gives a new approach to anytime planning.
We show that this approach is able to make better use of available time
than previous anytime planners, often continuing to improve plan quality
beyond the best that other planners find.
"406,Optimal Delete-Relaxed (and Semi-Relaxed) Planning with Conditional Effects,"Recently, several methods have been proposed for optimal delete-free
planning. We present an incremental compilation approach that enables
these methods to be applied to problems with conditional effects,
which none of them handle natively.
With an effective h+ solver for problems with conditional effects,
we are also able to adapt the h++ anytime lower bound function to
use more the space-efficient conditional effects-based compilation.
We show that this overcomes the limitations of the original function
caused by its reliance on an exponential-space compilation."410,PPSGen: Learning to Generate Presentation Slides for Academic Papers,"In this paper, we investigate a very challenging task of automatically generating presentation slides for academic papers.  The generated presentation slides can be used as draft slides and the presenters can prepare their formal slides based on the drafts in a quicker way. We propose a novel system called PPSGen to address this task, and it first employs regression methods to learn the importance of the sentences in an academic papers, and then exploits the integer linear programming (ILP) method to generate well-structured slides by selecting and aligning key phrases and sentences.  Evaluation results on a test set of 200 pairs of paper and slides collected on the web, and the results demonstrate that our proposed PPSGen system can generate slides with better content quality, and a user study shows that PPSGen have a few evident advantages over baseline methods.  "415,Improved Integer Programming Approaches for the Chance-Constrained Stochastic Programming,"The Chance Constrained Stochastic Programming (CCSP) is one of the models for decision making under uncertainty, and there are many studies for the CCSP in which only right-hand-side vector is random with a finite distribution.  The unit commitment problem is one of the such CCSP problems, in which the task is to create a schedule of electric generators and other devices such that the total cost is minimized.  The existing methods for exactly solving the CCSP requires an enumeration of scenarios when they model as a Mixed Integer Programming (MIP).  In this paper, we show how to reduce the number of scenarios to enumerate.  Furthermore we give a compact MIP formulation to slove the CCSP approximately.
"430,The Markov Assumption: Formalization and Impact,"We provide both a semantic interpretation and logical (inferential)  characterization of the Markov principle that underlies the main action theories in AI. This principle will be shown to  constitute a nonmonotonic assumption that justifies the actual restrictions on action descriptions in these theories, as well as constraints on allowable queries. It will be shown also that the well-known regression principle is a consequence of the Markov assumption, and it is valid also for non-deterministic domains."435,Non-negative Multiple Matrix Factorization,"Non-negative Matrix Factorization (NMF) is a traditional unsupervised machine learning technique for decomposing a matrix into a set of bases and coefficients under the non-negative constraint. NMF with sparse constraints is also known for extracting reasonable components from noisy data. However, NMF tends to give  undesired results in the case of highly sparse data, because the information included in the data is insufficient to decompose. Our key idea is that we can ease this problem if complementary data are available that we could integrate into the estimation of the bases and coefficients. In this paper, we propose a novel matrix factorization method called Non-negative Multiple Matrix Factorization (NM2F), which utilizes complementary data as additional matrices that share the row or column indices of the original matrix. The data sparseness is improved by decomposing the original and additional matrices simultaneously, since additional matrices provide information about the bases and coefficients. We formulate NM2F as a generalization of NMF, and then present a parameter estimation procedure derived from the multiplicative update rule. We examined NM2F in both synthetic and real data experiments. The effect of the additional matrices appeared in the improved NM2F performance. We also confirmed that the bases that NM2F obtained from the real data were intuitive and reasonable thanks to the non-negative constraint."437,Towards active event recognition,"While  being crucial  for human-robot interaction and other natural and dynamic contexts, directing a robot's attention to recognise activities and to anticipate events  like  goal-directed actions is complicated by intrinsic time constraints and spatially distributed sources of information.
The problem,  demanding for a  broader informational context  than the current perception and task,  clashes with the limits of the current attention control systems. In fact, it   requires an integrated solution for tracking, exploration and recognition, which  traditionally have been seen as separate problems in active-vision.
We propose a probabilistic generative framework  based on a mixture of Kalman filters and  information gain maximisation to use predictions in both recognition and attention-control. 
This framework can efficiently use the observations of one element in a dynamic environment to provide information on other elements, and consequently enables guided exploration.
Interestingly, the  sensors control policy,   directly derived from first principles,   represents the intuitive trade-off between finding the most discriminative  clues  and  maintaining overall awareness.
Experiments on a simulated humanoid robot  observing a human executing goal-oriented actions demonstrated  improvement on recognition time and precision over baseline systems."439,Probabilistic Reasoning with Undefined Properties in Ontologically-Grounded Belief Networks,"This paper concerns building probabilistic models with an underlying ontology that defines the classes and properties used in the model. In particular, it considers the problem of reasoning with properties with non-trivial domains. The properties may not always be defined, and we may be uncertain about whether an individual is in the domain of a property. One approach is to explicitly add a value ""undefined"" to the range of random variables, forming extended belief networks; however, adding an extra value to a variable's range has a large computational overhead. In this paper, we propose an alternative, ontologically-grounded belief networks, where all properties are only used when they are defined, and we show how probabilistic reasoning can be carried out without explicitly using the value ""undefined"" during inference. We prove this is equivalent to  reasoning with the corresponding extended belief network and empirically demonstrate that inference becomes more efficient.
"441,A Novel Bayesian Similarity Measure for Recommender Systems,"Collaborative filtering, a widely-used user-centric recommendation technique, predicts an item's rating for an active user by aggregating its ratings from similar users. User similarity is usually calculated by cosine similarity or Pearson correlation coefficient. However, both of them consider only the direction of rating vectors, and suffer from a range of drawbacks. To solve these issues, we propose a novel Bayesian similarity measure based on the Dirichlet distribution, taking into consideration both direction and length of rating vectors. Our principled method, further, reduces correlation due to chance and thus reveals users' true similarity. Experimental results on six real-world data sets show that our method achieves superior accuracy."442,On the Complexity of Trick Taking Card Games,"Determining the complexity of perfect information trick taking card games is a long standing open problem. This question is worth addressing not only because of the popularity of these games among human players, e.g., DOUBLE DUMMY BRIDGE , but also because of its practical importance as a building block in state-of-the-art playing engines for CONTRACT BRIDGE, SKAT, HEARTS, and SPADES.

We define a general class of perfect information two-player trick taking card games dealing with arbitrary numbers of hands, suits, and suit lengths. We investigate the complexity of determining the winner in various fragments of this game class.

Our main result is a proof of PSPACE-completeness for a fragment with bounded number of hands, through a reduction from Generalized Geography.
Combining our results with W&auml;stlundâ€™s tractability results gives further insight in the complexity landscape of trick taking card games."448,Just-In-Time Compilation of Knowledge Bases,"Since the first principles of Knowledge Base Compilation, most of the work
    has been focused in finding a good compilation target language in terms of
    compromises between compactness and expressiveness. The central idea
    remains unchanged in the last fifteen years: an off-line, very hard stage,
    allows to ``compile'' the initial theory in order to guarantee
    (theoretically) a efficent on-line stage, based on a set of queries and
    operations to be performed. In this paper, we propose a new approach of
    Knowledge Compilation, based on a Just-in-Time approach. Here, any
    Knowledge Base will be immediatly available for queries, and the effort
    spent on past queries will be partly amortized for future ones. Among other
    advantages, this allows us to propose a set of new queries/operations
    allowed on Knowledge Bases that is out of the scope of previous approaches,
    like the removing of clauses.

    To guarantee efficient (practical) answers, we rely on the tremedeous
    progresses made in the last ten years in the practical SAT solving of
    applicative problems. We introduce a set of new incremental SAT-based
    queries and operations that, even if theoretically NP-Hard, outperform
    previous Knowledge Compilation approaches on the set of classical problems
    used in the field.  In most of the cases, queries are solved in zero
    seconds (three digits precision). It is possible that, in order to be
    decomposable in practice, real-life problems must be trivial for SAT
    engines. If it casts new lights on problems used in the community, we also
    believe that our approach is particularly well suited for hard knowledge
    compilation problems, where compiling itself can be impossible, or leading
    to Knowledge Bases of large size, where only sub-polynomial queries may be
    practical.  We end the paper by a study of the amortization of previous
    queries over large set of queries.
"452,Online Hashing,"Hashing function learning has been recently received more and more attentions for fast search for large scale data. However, existing popular learning based hashing methods are batch model learning models and thus incur large scale computational problem for learning an optimized model on a large scale of labelled data. In this paper, we address the problem by develop an online hashing learning algorithm based on passive-aggressive strategy. To accommodate new coming data, the proposed online learning hashing update gets hashing model adapted to those new data and at the same time the newly updated model is penalized by the most previously learned model in order to retain important information learned in previous rounds. We also derive a tight bound for loss of our proposed online learning algorithm. We demonstrate the proposed online hashing model on searching both metric distance neighbors and semantical similar neighbors in the experiments."454,Bargaining for Revenue Shares on Tree Trading Networks,"We study trade networks with a tree structure, where a seller with a single indivisible good is connected to buyers, each with some value for the good, via a unique path of intermediaries. Agents in the tree make multiplicative revenue share offers to their parent nodes, who choose the best offer and offer part of it to their parent, and so on; the winning path is determined by who finally makes the highest offer to the seller. In this paper, we investigate how these revenue shares might be set via a natural bargaining process between agents on the tree.

We define a bargaining game where the agents at the endpoints of each edge in the tree negotiate the share on that edge using egalitarian bargaining, taking shares elsewhere in the tree as given. We investigate the fixed point of this system of bargaining equations and show that: (i) a fixed point always exists, and is  unique, (ii) the winner is always a buyer with highest value (efficiency), (iii) the payoff vector specified by the fixed point belongs to the core of the associated cooperative game, (iv) if the bargaining power of an agent on the winning path increases, then her final payoff strictly increases as well (strict monotonicity), and (v) the fixed point can be efficiently computed in polynomial time.  Finally, we present numerical evidence that asynchronous dynamics with randomly ordered updates always converges to the fixed point, indicating that the fixed point shares might arise from decentralized bargaining amongst agents on the trade network."461,On the Supremal Realizability of Behaviors with Uncontrollable Exogenous Events,"The behavior composition problem involves the automatic synthesis of a controller that is able to ""realize"" (i.e., implement) a desired target behavior specification by suitably coordinating a set of already available behaviors. 

While the problem has been thoroughly studied, one open issue has resisted a principled solution for a long time:  if the  target specification is not fully realizable, is there a way to realize it ""at best""?

In this paper we answer positively, by showing that there exists an unique supremal realizable target behavior satisfying the specification. More importantly we give an effective procedure to actually
compute such a target.

Then, we introduce exogenous events, and show that the supremal can again be computed, though this time, it comes into two variants, depending on the ability to observe such events. "1548,Exchanging OWL 2 QL Knowledge Bases,"The problem of exchanging knowledge between a
source and a target ontology connected through a
mapping has attracted attention recently both in
databases and in knowledge representation. In this
paper, we study this fundamental problem for on-
tologies and mappings expressed in the description
logic DL-LiteR, which is the formal counterpart of
OWL 2 QL. More specifically, we first consider the
problem of computing universal solutions, which
have been identified as one of the most desirable
translations to be materialized, providing a novel
automata-based exponential-time algorithm and a
PSPACE lower bound for this problem. Then we focus on the task of translating the implicit knowledge in a source ontology, that is, we consider the
problem of representing a source TBox by means
of a target TBox that captures at best the intensional
source information according to a mapping, show-
ing that the associated decision problems are NLOGSPACE-complete."467,Automating Quantified Conditional Logics in HOL,"A notion of quantified conditional logics is pro- vided that includes quantification over individual and propositional variables. The former is supported with respect to constant and variable domain seman- tics. In addition, a sound and complete embedding of this framework in classical higher-order logic is presented. Using prominent examples from the literature it is demonstrated how this embedding enables effective automation of reasoning within (object-level) and about (meta-level) quantified con- ditional logics with off-the-shelf higher-order theo- rem provers and model finders."471,Boosting Cross-lingual Knowledge Linking via Concept Annotation,"Cross-lingual links in Wikipedia have become an important resource for sharing knowledge across different languages. In order to enrich this resource, several knowledge linking approaches have been proposed for finding missing cross-lingual links between wikis. In these approaches, seed cross-lingual links and the inner link structures in wikis are two important factors for finding new cross-lingual links. When there are not enough seed cross-lingual links and inner links, discovering new cross-lingual links becomes a challenging problem. In order to solve this problem, we propose an approach that uses concept annotation to boost cross-lingual knowledge linking. Our approach first annotates each wiki article with concepts corresponding to the articles in the same wiki, so that missing inner links can be identified. Then, a pair-wise regression model is trained to predict new cross-lingual links based on six link-based similarity features.  The concept annotation and cross-lingual link prediction are two mutually enforced steps, therefore can be excuted iteratively to incrmentally find new cross-lingual links. Evaluation on the English and Chinese Wikipedia data shows that both the concept annotation method and cross-lingual link prediction method perform effectively. In the iterative running of our approach, more cross-lingual links can be found when concept annotation is performed. "476,Active Learning for Teaching a Robot Grounded Relational Symbols ,"The present paper investigates an interactive teaching scenario, where a human aims to teach the robot symbols that abstract geometric (relational) features of objects. There are multiple motivations for this scenario: First, state-of-the-art methods for relational Reinforcement Learning demonstrated that we can successfully learn abstracting and well-generalizing probabilistic relational models and use them for goal-directed object manipulation. However, these methods rely on a given grounded action and state symbols and raise the classical question: Where do the symbols come from? Second, existing research on learning from human-robot interaction has focussed mostly on the motion level (e.g., imitation learning). However, if the goal of teaching is to enable the robot to autonomously solve sequential manipulation tasks in a goal-directed manner, the human should have the possibility to teach the relevant abstractions to describe the task and let the robot eventually levarage powerful relational RL methods. In this paper we formalize human-robot teaching of grounded symbols as an Active Learning problem, where the robot actively generates geometric situations that maximize his information gain about the symbol  to be learned. We demonstrate that the learned symbols can be used by a robot in a relational RL framework to learn probabilistic relational rules and use them to solve object manipulation tasks in a goal-directed manner.
"481,Bounded Epistemic Situation Calculus Theories,"We define an interesting class of theories in the epistemic situation calculus, called e-bounded theories, and show that  for them verification of a very expressive class of first-order mu-calculus temporal epistemic properties is decidable. In e-bounded theories, the number of ground fluent atoms that the agent thinks may be true is bounded by a constant.  Such theories can still have an infinite domain and an infinite set of states.  We argue that many application domains can be modelled as e-bounded theories.  Intuitively, this is the case because facts do not persist indefinitely and because one forgets some facts eventually, as one learns new ones. We also show that if the agent's knowledge in the initial situation is e-bounded and the objective part of an action theory preserves boundedness (in the sense that if the number of ground fluent atoms that are true initially is bounded it remains bounded in later situations), then the entire epistemic theory is e-bounded."486,Backdoors to Abduction,"Abduction is among the most fundamental reasoning methods, with many important applications. Unfortunately, the computational complexity of this problem is very high (\SigmaTwo-complete). This complexity barrier rules out the existence of a polynomial reduction to SAT. In this work we use structural properties (backdoor sets) to break this complexity barrier. We present efficient (fixed-parameter tractable) transformations from Abduction to SAT, which allow us to utilize the power of SAT solvers directly. Furthermore, we explain how to extend these transformations to enumerate all (subset-minimal) solutions and filter by constraints."496,Combining RCC5 relations with betweenness information,"RCC5 is an important and well-known calculus for representing and reasoning about mereological relations. Among many other applications, it is pivotal in the formalization of commonsense reasoning about natural categories. More in particular, it allows for a qualitative representation of conceptual spaces in the sense of G&auml;rdenfors. To further the role of RCC5 as a vehicle for conceptual reasoning, in this paper we combine RCC5 relations with information about betweenness of regions. The resulting calculus allows us to express, for instance, that some part (but not all) of region B is between regions A and C. We show how consistency for such networks can be decided in polynomial time for atomic networks, even when regions are required to be convex. From an application perspective, the ability to express betweenness information allows us to use RCC5 as a basis for interpolative reasoning, while the restriction to convex regions ensures that all consistent networks can be faithfully represented as a conceptual space.
"508,A global constrained optimization method for designing road networks with small diameters,"The road network design problem is to optimize the road network by selecting paths to improve or adding paths in the existing road network, under certain constraints, e.g., the weighted sum of modifying costs. Since its multi-objective nature, the road network design problem is often challenging for designers. Empirically, the smaller diameter a road network has, the more connected and efficient the road network is. Based on this observation, we propose a set of constrained convex models for designing road networks with small diameters. To be specific, we theoretically prove that the diameter of the road network, which is evaluated w.r.t the travel times in the network, can be bounded by the algebraic connectivity in spectral graph theory since that the upper and lower bounds of diameter are inversely proportional to algebraic connectivity. Then we can focus on increasing the algebraic connectivity instead of reducing the network diameter, under the budget constraints. The above formulation leads to a semi-definite program, in which we can get its global solution easily. Then, we present some simulation experiments to show the correctness of our method. At last, we compare our method with an existing method based on the genetic algorithm."515,The Parameterized Complexity of Planning: A Detailed Map,"The goal of this paper is a systematic parameterized complexity analysis of propositional STRIPS planning. We identify several natural problem parameters and study all possible combinations of 9 parameters in 4 different settings. These settings arise, for instance, from the distinction if negative effects of actions are allowed or not. In total, we thus end up with 2048 cases, for which we provide a complete picture: for each case, we establish either paraNP-hardness (i.e., the parameter combination does not help) or W[t]-completeness with t in {1,2} (i.e., fixed-parameter intractability), or FPT (i.e., fixed-parameter tractability)."516,Affine Decision Trees for Model Counting,"Counting the models of a propositional formula is a key issue for a number of AI problems, but few propositional languages offer the possibility to count models efficiently. In order to fill the gap, we introduce the language EADT of (extended) affine decision trees. An extended affine decision tree simply is a tree with affine decision nodes and some specific decomposable conjunction or disjunction nodes. Unlike standard decision trees, the decision nodes of an EADT formula are not 
labelled by variables but by affine clauses. We study EADT, and several subsets of it, along the lines of the knowledge compilation map. We also describe a CNF-to-EADT compiler and present some experimental results. Those results show that the EADT compilation-based approach is competitive with (and in some cases is able to outperform) the model counter Cachet and the d-DNNF compilation-based approach to model counting."512,On the Complexity of Probabilistic Abstract Argumentation,"Probabilistic abstract argumentation is an interesting extension of Dung's abstract argumentation framework, combining probability theory with argumentation. 
In this setting, we address the fundamental problem of computing the 
probability that a set of  arguments is an extension according to a given semantics. 
We focus on the most popular semantics
(i.e., admissible, stable, complete, grounded, preferred, ideal), 
and show the following dichotomy result:
computing the probability that a set of arguments is an extension is
either PTIME or FP^#P-complete depending on the semantics adopted.
Our PTIME results are particularly interesting, 
as they hold for some semantics for which no polynomial-time 
technique was known so far."517,Bridging the Gap Between Refinement and Heuristics in Abstraction,"There are two major uses of abstraction in planning and search:
refinement (where abstract solutions are extended into concrete
solutions) and heuristics (where abstract solutions are used to
compute heuristics for the original search space).  These two
approaches are usually viewed as unrelated in the literature.  It is
reasonable to believe, though, that they are related, since they are
both intrinsically based on the structure of abstract search spaces.
We take the first steps towards formally investigating their
relationships, empolying the framework by B&auml;ckstr&ouml;m and
Jonsson for analysing and comparing abstraction methods.  By adding
some mechanisms for expressing metric properties, we can capture
concepts like admissibility and consistency of heuristics.  We present
an extensive study of how such metric properties relate to the
properties in the original framework, revealing a number of
connections between the refinement and heuristic approaches.  This
also provides new insights into, for example, Valtorta\'s theorem and
spurious states.
"523,Preserving partial solutions while relaxing constraint networks,"This paper is about transforming constraint networks to accommodate additional constraints. The
focus is on two speci&amp;#64257;c intertwined issues. First,
we investigate how partial solutions to an initial
network can be preserved from the potential impact
of additional constraints. Second, we study how
more permissive constraints, which are intended to
enlarge the set of solutions, can be accommodated
in a constraint network. These two problems are
studied in the general case and the light is shed on
their relationship. A case study is then investigated
where a more permissive additional constraint is
taken into account through a form of network relaxation, while some previous partial solutions are
preserved at the same time."528,"Action Translation in Extensive-Form Games: Axioms, Pathologies, and Optimality","When solving extensive-form games with large action spaces, typically significant abstraction is needed to make the problem manageable from a modeling or computational perspective. When this occurs, a procedure is needed to interpret actions of the opponent that fall outside of our abstraction (by mapping them to actions in our abstraction). This is called an action translation mapping. Prior action translation mappings have been based on heuristics without theoretical justification. We show that the prior mappings are highly exploitable and that most of them violate certain natural desiderata. We present a new mapping that satisfies these desiderata and has significantly lower exploitability than the prior mappings. Furthermore, we observe that the cost of this worst-case performance benefit (low exploitability) is not high in practice; our mapping performs competitively with the prior mappings against no-limit Texas Hold'em agents submitted to the 2012 Annual Computer Poker Competition. We also observe counterintuitive pathologies that can arise when performing action abstraction and translation; for example, we show that it is possible to improve performance by including suboptimal actions in our abstraction and excluding optimal actions."534,Audience-Based Uncertainty in Abstract Argument Games,"The paper generalizes abstract argument games to cope with cases where proponent and opponent argue in front of an audience whose composition or type is known only with uncertainty. The generalization, which makes use of basic tools from probability theory, is motivated by several examples and proven adequate for a probabilistic version of the grounded extension. "537,How to Persuade a Group to Change its Collective Decision?,"Persuasion is a common social and economic activity. It usually arises when conflicting interests among agents exist, and one of the agents wishes to sway the opinions of others. This paper considers the problem of an automated agent which needs to influence the decision of a group of self-interested agents that must reach an agreement on a joint action. For example, consider an automated agent that aims to reduce the energy consumption of a nonresidential building, by convincing a group of people who share an office to agree on an economy mode of the air-conditioning and low light intensity. In this paper we present four problems that address issues of minimality and safety of the persuasion process. We present the relationships to similar problems from social choice, and show that if the agents are using Plurality or Veto as their voting rule all of our problems are in P. We also show that with $k$-Approval, Bucklin and Borda voting rules some problems become intractable. We thus present heuristics for an efficient persuasion with Borda, and evaluate them through simulations."540,Improving Traffic Prediction with Tweet Semantics,"Nowadays, online social media has become ubiquitous in our daily
life, and there has been growing interest in mining useful
information therein. Twitter, for example, can be viewed as a
distributed network of human sensors who may provide localized and
timely information that can be analyzed and aggregated to provide
broader insight. For example, people are inclined to tweet that they
are currently stuck in traffic, or to tweet other information (such
as their near-term travel plans) that may impact traffic patterns.

In this paper, we examine whether it is possible to use such information to develop improved near-term traffic predictions. We first analyze the correlation between traffic volume and tweet counts with various granularities. We then propose an optimization framework to extract traffic indicators based on tweet semantics using a sparse matrix, and incorporate them into traffic prediction via linear regression. Experimental results using traffic and Twitter data originated from the San Francisco Bay area of California demonstrate the effectiveness of our proposed framework.
"541,Decidability of Model Checking Non-Uniform Artifact-Centric Quantified Interpreted Systems,"Artifact-Centric Systems are a novel paradigm in service-oriented computing, which is typically analysed by means of formalisms based on multiagent systems. The state-of-the-art in the literature states that, while their model checking problem is undecidable in general, conditions such as boundedness and uniformity can make verification decidable through finite abstractions. In the present contribution we relax these conditions and obtain two results. Firstly, we show that model checking non-uniform artifact-centric systems is undecidable even if we assume boundedness. Secondly, we provide a partial decision procedure for the universal fragment of a first-order version of the logic CTL. We obtain this result by introducing a counterpart semantics and developing an abstraction methodology operating on these structures. This enables us to generate finite abstractions of infinite artifact-centric systems, hence perform the verification on abstract models."543,Controlling the Hypothesis Space in Probabilistic Plan Recognition,"The ability to understand the goals and plans underlying the observed behaviour of other agents is an important characteristic of intelligent behaviours in many contexts.  One of the approaches used to endow agents with this capability is the weighed model counting approach. Given a plan library and a sequence of observations, this approach exhaustively lists plan executions models that are consistent with the observed behaviour. The probability that the agent might be pursuing a particular goal is then computed as a proportion of plan execution models satisfying the goal. This approach combines many interesting features that have proven difficult with alternative plan-library based approaches, such as recognizing multiple interleaved plans or handling temporal constraints and unobserved actions. However, the approach suffers from a combinatorial explosion of plan execution models, which impedes its application to real-world domains.  In this paper, we present a heuristic weighted model counting algorithm that limits the number of generated plan execution models in order to recognize most likely goals quickly, albeit initially with lower and upper bound estimates in terms of likelihood. "545,Predicting the Size of Depth-First Branch and Bound Search Tree,This paper provides algorithms for predicting the size of the Expanded Search Tree (EST) of Depth-first Branch and Bound algorithms for optimization tasks. The prediction algorithm is implemented and evaluated in the context of solving combinatorial optimization over graphical models such as Bayesian and Markov networks. Our method extends to DFBnB the approaches provided by Knuth-Chen schemes that were designed and applied for predicting the size of ESTs of backtracking search algorithms. Our empirical results demonstrate good predictions which are superior to competing schemes.546,Bayesian Joint Inversions for the Exploration of Earth Resources,"In this paper we propose a machine learning approach to geophysical inversion problems for the exploration of earth resources. Our approach is based on nonparametric Bayesian methods, in particular, Gaussian processes, and it provides a full distribution over the predicted geophysical properties and enable the incorporation of data from different modalities. We assess qualitatively and quantitatively our machine learning methods in a real dataset from South Australia containing gravity and drill hole data and in simulated experiments involving gravity, drill hole and magnetics, with the goal of characterizing rock densities for geothermal energy exploration. We show that our approach is more informative and effective than widely used techniques in the geophysics community. The significance of our methods extends to general exploration problems and can dramatically modify the industry."602,Online Egocentric Models for Citation networks,"With the emergence of large-scale evolving (time-varying) networks, dynamic network analysis~(DNA) has become a very hot research topic in recent years. Although a lot of DNA methods have been proposed by researchers from different communities, most of them can only model snapshot data recorded at a very rough temporal granularity. Recently, models have been proposed for DNA which can be used to model large-scale citation networks at a fine temporal granularity. However, they suffer from a significant decrease of accuracy over time because the learned parameters or node features are static (fixed) during the prediction process for evolving citation networks. In this paper, we propose a model, called online egocentric model (OEM), to learn time-varying parameters and node features for evolving citation networks. Experimental results on real-world citation networks show that our OEM can not only prevent the prediction accuracy from decreasing over time but also uncover the evolution of topics in the citation networks."566,Smart Hashing Update for Fast Response,"Recent years have witnessed the growing popularity of hashing in large-scale vision problems. Although most existing hashing-based methods have been proven to obtain high accuracy, they are regarded as passive hashing and assume that the labelled pairs are provided in advance. In this paper, we consider updating a hashing model upon gradually increased labeled data in a fast response to users, called smart hashing update (SHU), which selects a small set of hashing functions to relearn and only updates the corresponding hash bits of all data points. More specifically, we put forward two selection methods for performing efficient and effective update. In addition, in order to accelerate the speed of convergence and reduce the response time for a stable hashing algorithm, we also propose an accelerated method in order to further reduce interactions between users and computer. We evaluate our proposals on two benchmark data sets. Our experimental results show it is not necessary to update all hash bits in order to adapt the model for new input data and meanwhile we obtain better or similar performance without sacrificing much accuracy against the batch mode update."567,Pareto-Based Multi-Objective AI Planning,"Real-world problems, including AI planning problems, generally involve several contradictory objectives (e.g. makespan and cost). The only approaches to multi-objective AI Planning rely on metrics, that can incorporate several objectives in some linear combinations, and metric-sensitive planners, that are able to give different plans for different metrics, and hence to eventually approximate the Pareto front of the multi-objective problem, i.e. the set of optimal trade-offs between the contradictory objectives. Divide-and-Evolve (DaE) is an evolutionary planner that embeds a classical planner and feeds it with a sequence of sub-problems of the problem at hand. Like all Evolutionary Algorithms, DaE can be turned into a Pareto-based multi-objective solver, even though using an embedded planner that is not metric-sensitive. The Pareto-based multi-objective planner MO-DaE thus avoids the drawbacks of the aggregation method. Furthermore, using YAHSP as embedded planner, it obtains a better spread of the solutions along the Pareto front than the metric-based approach using LPG metric-sensitive planner, as witnessed by experimental results on multi-objective instances built from IPC7 domains, mixing cost and temporal corresponding instances."579,Robust Multiple Task Regression via Reweighted Least Square,"Multiple task learning (MTL) is to improve
generalization performance by exploiting
the intrinsic relationship among
related tasks. One common assumption
in most MTL algorithms is that all the
tasks are related definitely. However, it
is usually not such in real applications.
In this paper, we propose a novel robust
multiple task learning algorithm to
. We also theoretically analysis the convergence
and effectiveness of the proposed
method. Experimental results on
toy data as well as two benchmark data
set demonstrate the effectiveness of the
proposed method."572,Multi-agent Epistemic Explanatory Diagnosis via Reasoning about Actions,"The task of explanatory diagnosis conjectures actions to explain observations. This is a common task in real life and an essential ability of intelligent agents. It become more complicated in multi-agent scenarios, since agents' actions may be partially observable to other agents, and observations might involve agents' beliefs about the world or other agents' beliefs or even common beliefs of a group of agents. For example, we might want to explain the observation that p does not hold, but Ann believes p, or the observation that Ann, Bob, and Dan commonly believe p. In this paper, we formalize the multi-agent explanatory diagnosis task in the framework of dynamic epistemic logics, where Kripke models of actions are used to represent agents' partial observability of actions. Since this task is undecidable in general, we identify important decidable fragments via techniques of reducing the potentially infinite search spaces to finite ones of epistemic states or action sequences."581,Lazy Paired Hyper-Parameter Tuning,"In virtually all machine learning applications, hyper-parameter tuning
is an essential component of maximizing predictive accuracy for the
problem at hand.  Such tuning is computationally expensive.  The cost
is further exacerbated by the need for multiple evaluations (via
cross-validation or bootstrapping) at each configuration setting to
guarantee statistical significance.  This paper presents a simple,
general technique for improving the efficiency of hyper-parameter
tuning by minimizing the number of resampled evaluations at each
configuration.  We exploit the fact that train-test resampling can
be matched for all evaluations across the candidate
hyper-parameter configurations, inviting the use of paired hypothesis
tests.  This allows statistically sound early elimination of
suboptimal candidates, as well as minimizing the number of evaluations
based on power analysis.  Results on synthetic and real-world datasets
demonstrate that our method is more efficient than competitors at
selecting optimal hyper-parameter configurations for popular machine
learning algorithms.
"1169,Automated Grading of DFA constructions," One challenge in making online education more effective is to
develop automatic grading software that can provide meaningful feedback.
This paper provides a solution to automatic grading of the standard
computation-theory problem that asks a student to construct a deterministic finite automaton (DFA) from the given description of its language. We focus on how to assign partial grades for incorrect answers. Each student\'s answer is compared to the correct DFA using
a hybrid of three techniques devised to capture different classes of
errors. First, in an attempt to catch syntactic mistakes, we compute edit distance between the two DFA descriptions. Second, we consider the entropy of the symmetric difference of the languages of the two DFAs, and compute a score that estimates the fraction of the number of strings on which the student answer is wrong. Our third technique is aimed at capturing mistakes in reading of the problem description. For this purpose, we consider a description language Mosel, which adds syntactic sugar to the classical Monadic Second Order Logic, and allows defining regular languages in a concise and natural way. We provide algorithms, along with optimizations, for transforming Mosel descriptions into DFAs and vice-versa. These allow us to compute the syntactic edit distance of the incorrect answer from the correct one in terms of their logical representations. We report an experimental study that evaluates hundreds of answers submitted by (real) students by comparing grades/feedback computed by our tool with human graders. Our conclusion is that the tool is able to assign partial grades in a meaningful way, and should be preferred over human graders for both scale and consistency."584,Improving Function Word Alignment with Frequency and Syntax,"In statistical word alignment for machine translation, function words usually have poor alignment precision because they do not have clear correspondence between different languages. This paper proposes a novel post-processing approach to improve word alignment by removing improbable function word alignment from a state-of-the-art word alignment result with high recall. We firstly identify function word alignment using monolingual and bilingual frequency characteristics and then filter them according to content word alignment and information extracted from constituency-based parse trees. Our approach improves both the quality of word alignment and the performance of statistical machine translation on Chinese-to-English, German-to-English and French-to-English language pairs."586,Co-Regularized Ensemble for Feature Selection,"Supervised feature selection determines feature relevance by evaluating feature's correlation with the classes. As collecting high-quality labeled data is difficult, the performance of supervised feature selection is restrained for the cases when only a few labeled data per class are available. In such cases, over-fitting would be a potential problem. Furthermore, the appropriate feature subset learned from different algorithms may be different. Given the disagreement of selected feature subset learned from the same training set, we can expect better performance by the ensemble of these feature selection methods. In this paper, we propose an algorithm of co-regularized ensemble for feature selection. Given different regularized feature selection methods, we add a joint $\ell_{2,1}$-norm on multiple feature selection matrices to ensemble different criteria into one optimization problem. This added co-regularization term has twofold role in enhancing the effect of regularization for each criterion and uncovering common irrelevant features. The problem of over-fitting can be alleviated and thus the performance of feature selection is improved. Extensive experiment on different data types demonstrates the effectiveness of our algorithm."596,Learning Visual Symbols for Parsing Human Pose in Images,"Abstract Parsing human poses in images is one of the most important computational problems in deriving critical visual information for artificial intelligent agents. This paper focuses the learning of the body part models, in this parsing process, which we call visual symbols. These visual symbols are self-contained, the context between symbols are well defined, and they can be easily manipulated and intepreted. The key idea is to compute symbols effectively by categorizing visual information using cross validation, and then define geometric context by the agreements between symbols. In our experiment, we present a three level hierarchy representation. Following human kinematic strucuture, we derive an approach to effectively estimate human poses. Our method outperform the state of art method, yet the parsing results are still simple enough to easily intepretate and understand."601,Relation Regularized Collaborative Topic Regression for Tag Recommendation,"As collaborative tagging systems become increasingly popular in recent years, accurate and scalable tag recommending methods are more and more required. In practice, only several basic recommendation strategies are applied, most known as collaborative filtering methods. In reality, however, CF tends to suffer from the sparsity problem. In this paper, we propose a novel model based on Collaborative Topic Regression model(CTR) and seamlessly exploit the related information among items. Our evaluation shows that our model achieves significantly better recall than classical collaborative filtering approaches and CTR model even in extremely sparse condition."607,Exploiting Local and Global Social Context for Recommendation,"With the development of social media, information overload problem becomes increasingly severe and recommender systems play an important role in helping online users find relevant information by suggesting them information of potential interests. Social activities for online users produce abundant social relations. Social relations provide an independent source for recommendation, presenting both opportunities and challenges for traditional recommender systems. Users are likely to seek suggestions from both their local friends and users with high global reputations, motivating us to exploit social relations from local and global perspectives for online recommender systems in this paper. We develop approaches to capturing local and global social relations, and propose a novel framework Locabel taking advantage of both local and global social context for recommendation. The empirical results on real-world datasets demonstrate the effectiveness of our proposed framework and further experiments are conducted to understand how the local and global social context work for the proposed framework Locabel. 
"608,Sequential Equilibrium in Computational Games,"We examine sequential equilibrium in the context of computational games
[Halpern and Pass, 2011a], where agents are charged for computation.  In such games, an agent can rationally choose to forget, so issues of imperfect recall arise.  In this setting, we consider two notions of imperfect
equilibrium.  One is an \emph{ex ante} notion, where a player chooses his
strategy before the game starts and is committed to it, but he chooses
it in such a way that it remains optimal even off the equilibrium path.
The second is an \emph{interim} notion, where players can reconsider at
each information set whether they are doing the ``right'' thing, and can
change their strategies if they should choose to.  
The two notions agree in games of perfect recall, but not in games of
imperfect recall.   Although the interim notion seems more appealing,
Halpern and Pass [2011b] argue that there are some deep
conceptual problems with it in standard games of imperfect recall.  We
 show that the conceptual problems largely disappear in the
computational setting.  Moreover, in this setting, 
under natural assumptions, the two notions coincide.
"906,Revisiting Regression in Planning,"Heuristic search with reachability-based heuristics is arguably the most successful paradigm in Automated Planning to date. In its earlier stages of development, heuristic search was proposed as both forward and backward search. Due to the disadvantages of backward search, in the last decade researchers focused mainly on forward search, and backward search was abandoned for the most part as a valid alternative. In the last years, important advancements regarding both the theoretical understanding and the performance of heuristic search have been achieved, applied mainly to forward search planners. In this work we revisit regression in planning with reachability-based heuristics, trying to extrapolate to backward search current lines of research that were not as well understood as they are now."614,Robust Tensor Clustering with Non-Greedy Maximization,"Tensors are increasingly common in modern applications dealing with
complex heterogeneous data. Clustering them is a fundamental tool
for data analysis and pattern discovery. However, the data in real
life is usually with some outliers, which will reduce the
performance of current clustering algorithms. This urges us to
develop a tensor clustering algorithm that is robust to the
outliers. We call it as the Robust Tensor Clustering (RTC)
algorithm. The RTC firstly obtains a lower rank approximation of the
original tensor data. Thanks to the L1 norm used in optimization
function, the approximation is robust to the outliers. Next, we
compute the HOSVD decomposition of this approximate tensor to obtain
the final clustering results. Different from the traditional
algorithm solving the approximation function with a greedy strategy,
we extend a non-greedy strategy to obtain a better solution.
Experimental results on five public datasets and comparisons with
five state-of-the-art clustering methods demonstrate the
effectiveness of our method."621,Where You Like to Go Next: Successive Point-of-Interest Recommendation,"Personalized point-of-interest (POI) recommendation is a significant task in location-based social networks (LBSNs) as it can help provide better user
experience as well as enable third-party services, e.g., launching advertisement. To provide good recommendation, various research has been conducted in the literature. However, pervious work only consider the â€œcheck-insâ€ù in a whole and omit their temporal relation. They can only recommend POI
globally and cannot know where the user would like to go tomorrow or in next few days. In this paper, to the best of our knowledge, we are the first to propose the task of next POI recommendation in LBSNs, a much harder task than standard POI recommendation or POI prediction. To solve this task, we observe two prominent properties in the â€œcheck-inâ€ù sequence: personalized Markov chain and region localization. Hence, we propose a novel matrix factorization method, namely, FPMC-LR, to embed the personalized Markov chains and the localized regions. Our proposed FPMC-LR not only exploit the personalized Markov chain in the checkin sequence, but also take into account usersâ€™ movement constraint, i.e., moving around a local region. More importantly, utilizing the localized region information, we can not only discard much redundant information, but also reduce the computation cost. We demonstrate the effectiveness and efficiency of our FPMC-LR on two real-world LBSNs datasets."623,Efficient Latent Variables Perceptron for Semantic Parsing,"Discriminative structured prediction methods have been widely used in many natural language processing tasks, but semantic parsing is still dominated by generative methods. In this paper, by introducing a latent structure variables to close the gap between the input sentences and output representations, we formulate semantic parsing as a structured prediction problem, based on the latent variable perceptron model incorporated with a tree edit-distance loss as optimization criterion. The proposed approach maintains the advantage of a discriminative model in accommodating flexible combination of features and naturally incorporates an efficient decoding algorithm in learning and inference. Further, we design an efficient approach based on vector space model to extract a much smaller set of relevant MR productions for test instances, improving the accuracy and efficiency of inference. Experimental results on publicly available corpus show that our approach significantly outperforms previous systems, producing state-of-the-art semantic parsing results."624,Reasoning about State Constraints in the Situation Calculus,"In dynamic systems, state constraints are formulas that hold in every reachable state. It has been shown that state constraints can be used to greatly reduce the planning search space. They are also useful in program verification. In this paper, we propose a sound but incomplete method for automatic verification and discovery of \forall*\exists* state constraints for a class of action theories that include many planning benchmarks. Our method is formulated in the situation calculus, theoretically based on Skolemization and Herbrand Theorem, and implemented with SAT solvers. Basically, we verify a state constraint by strengthening it in a  novel and smart way so that it becomes a state invariant. We experimented with the blocks world, logistics and satellite domains, and the results showed that, almost all known state constraints can be verified in a reasonable amount of time, and meanwhile succinct and intuitive related state constraints are discovered."627,Robust Constraint Satisfaction and Local Hidden Variables in Quantum Mechanics,"Motivated by considerations in quantum mechanics, we introduce the class of robust constraint satisfaction problems in which the question is whether every partial assignment of a certain length can be extended to a solution, provided the partial assignment does not violate any of the constraints of the given instance. We explore the  complexity of specific robust colorability and robust satisfiability problems, and show that they are NP-complete. We then use these results to establish the computational intractability of detecting local hidden-variable models in quantum mechanics."628,Fully Proportional Representation as Resource Allocation: Approximability Results,"We study the complexity of (approximate) winner determination under Monroeâ€™s and Chamberlin-Courantâ€™s multiwinner voting rules, where we focus on the total (dis)satisfaction of the voters (the utilitarian case) or the (dis)satisfaction of the worst-off voter (the egalitarian case). We show good approximation algorithms for the satisfaction-based utilitarian cases, and inapproximability results for the remaining settings."639,Automatic Name-Face Alignment to Enable Cross-Media News Retrieval,"A new algorithm is developed in this paper to support automatic name-face alignment for achieving more accurate cross-media news retrieval. We focus on extracting valuable information from large amounts of news images and their captions, where multi-level image-caption pairs are constructed for characterizing both significant names with higher salience and their cohesion with human faces that are extracted from news images. To remedy the issue of lacking enough related information for rare name, Web mining is introduced to acquire the extra multimodal information. We emphasize on an efficient optimization mechanism by our Improved Self- Adaptive Simulated Annealing Genetic Algorithm to verify the feasibility of alignment combinations. Our experiments on a large number of public data have obtained very positive results."646,Forgetting for  Answer Set Programs Revisited,"A new semantic forgetting for general logic programs under stable model semantics, called SM-forgetting, is proposed in the paper. The forgetting result exists, is unique up to strong equivalence and
preserves nonmonotonic consequence relation over unforgotten atoms.
It preserves knowledge of logic programs that is irrelevant to being forgotten variables. Its properties, algorithm and computational complexities are explored as well. For instance, it is proved that the forgetting result of Horn logic programs can be captured by Horn ones and computing the forgetting result is generally intractable."645,Guarantees of Augmented Trace Norm Models in Tensor Recovery,"  This paper studies the recovery guarantees of the models of minimizing $\|\mathcal{X}\|_*+\frac{1}{2\alpha}\|\mathcal{X}\|_F^2$ where $\mathcal{X}$ is a tensor and $\|\mathcal{X}\|_*$ and $\|\mathcal{X}\|_F$ are the trace and Frobenius norm of respectively. We show that they can efficiently recover low-rank tensors.
In particular, they enjoy exact guarantees similar to those known for minimizing
$\|\mathcal{X}\|_*$ under the conditions on the sensing operator such as its null-space property, restricted
isometry property, or spherical section property. To recover a low-rank tensor
$\mathcal{X}^0$, minimizing $\|\mathcal{X}\|_*+\frac{1}{2\alpha}\|\mathcal{X}\|_F^2$ returns the same solution as minimizing $\|\mathcal{X}\|_*$ almost whenever
$\alpha\geq10\mathop {\max}\limits_{i}\|X^0_{(i)}\|_2$."649,Randomized Load Control: A Simple Distributed Approach for Scheduling Smart Appliances,"A significant portion of the electricity network capacity is built to run only a few days a year when demand peaks. As a result, expensive power generation plants and equipment costing millions of dollars are sitting idle most of the time, which increases costs for everyone. We present ""randomized load control"", a simple distributed approach for scheduling smart appliances. Randomized load control schedules the start time of appliances that are programmed to run within a specified time window, so that the aggregate load achieves a given ideal load. Our results show that we do achieve the given ideal load to a great extent. This is remarkable, because the approach is completely distributed and preserves customer privacy as the scheduling happens within each house or building separately."651,On the Variance of the Shapley value in Weighted Voting Games,"Weighted voting games (WVGs) model decision making bodies such as parliaments and councils. One is often interested in measuring the influence
a player has on the vote. Two highly popular measures of influence are the Shapley value [Shapley, 1953], and the Banzhaf power index [Banzhaf,
1964]. Given a power measure, proportional representation is the property of having playersâ€™ voting power proportional to their weight. Approximate
proportional representation (w.r.t. the Banzhaf power index) can be ensured by changing player weights [Penrose, 1946]; however, a simpler way
of achieving approximate proportional representation is by changing the quota, i.e. the number of votes required in order to pass a bill [S&amp;#322;omczynski and
&amp;#729; Zyczkowski, 2006].
It is known that when one chooses a quota at random, proportional representation w.r.t. the Shapley value holds in expectation [Mann and Shapley,
1960]. In our work, we show bounds on the variance of the Shapley value when the quota is chosen at random; our bounds imply conditions that ensure
that the variance is low under certain assumptions.

Finally, we provide asymptotic and empirical analysis of the case when weights are sampled i.i.d. from a binomial distribution."665,Joint and Coupled Bilingual Topic Model Based Sentence Representations for Language Model Adaptation,"This paper is concerned with data selection for adapting language model (LM) in statistical machine translation (SMT), and aims to find the LM training sentences that are topic similar to the translation task. Although the traditional approaches have gained significant performance, they ignore the topic information and the distribution of words when selecting similar training sentences. In this paper, we propose two bilingual topic model (BLTM) (joint and coupled BLTM) based sentence representations for cross-lingual data selection. We map the data selection task into cross-lingual semantic representations that are language independent, and rank sentences in the target LM training corpus for a sentence in the translation task by the semantics-based likelihood. The semantic representations are learned from the parallel corpus, with the assumption that the bilingual pair shares the same or similar distribution over semantic topics. Large-scale experimental results demonstrate that our approaches significantly outperform the state-of-the-art approaches on both LM perplexity and SMT performance."667,A Clause-level Hybrid Approach to Chinese Empty Element Recovery,"Empty elements (EEs) play a critical role in Chinese syntactic, semantic and discourse analysis. Previous studies employ a language-independent sentence-level approach to EE recovery, by casting it as a linear tagging or structued parsing problem. In comparison, this paper proposes a clause-level hybrid approach to address specific problems in Chinese EE recovery, which recovers EEs in Chinese language from the clause perspective and integrates the advantages of both linear tagging and structured parsing. In particular, a comma disambiguation method is employed to improve syntactic parsing and help determine clauses in Chinese. In this way, the noise introduced by sentence-level syntactic parsing and multiple EEs in the same position of a linear sentence can be well addressed. Evaluation on Chinese Treebank 6.0 shows the significant performance improvement of our clause-level hybrid approach over the state-of-the-art sentence-level baselines, and its great impact on a state-of-the-art Chinese syntactic parser."680,Improve Coding with Bilevel VisualWords for Image Classification,"Improve coding for Bag-of-Words approach has played an important role in recent works for image classification. In consideration of efficiency, most methods use k-means clustering to generate the codebook, which weakens codebookâ€™s ability to describe images. Though some efforts have been made to optimize codebook using sparse coding, they usually incur higher computational cost. Moreover, they ignore the correlations between codes in the coding stage, that leads to low discriminative ability of the final representation. In this paper, we propose a bilevel codebook generation rule in consideration of representation ability, efficiency and discriminative ability. k-means and an efficient spectral clustering are respectively run in each level by taking both class information and the shapes of each visual word cluster into account. To obtain discriminative representation, we design a certain localized coding rule with bilevel codebook to select local bases. To further achieve an efficient coding referring to this rule, an online method is proposed to efficiently learn a projection of local descriptor to the visual words in the codebook. As a result, coding can be simply achieved by a low dimensional localized soft-assignment. Experimental results demonstrate that our proposed bilevel visual words improves the Bag-of-Words model and outperforms the state-of-the-art approaches."689,Short-term Wind Power Forecasting using Gaussian Processes,"Since wind has an intrinsically complex and stochastic nature, accurate wind power forecasts are necessary for the safety and economics of wind energy utilization. In this paper, we investigated a combination of numeric and probabilistic models: Gaussian Process (GP) models were applied to one-day-ahead wind power forecasting, by processing data from Numerical Weather Prediction (NWP) model. Firstly the wind speed data from NWP was corrected by a GP, then as there is always a defined limit on power generated in a wind turbine due the turbine controlling strategy, wind power forecasts were realized by modeling the relationship between the corrected wind speed and power output using a Censored GP. To verify the proposed approach, two real world datasets were used for model construction and testing. The simulation results were compared with persistence method and Artificial Neural Network (ANN) models, and as calculated by Mean Absolute Error (MAE), the proposed model presents around 11% improvement in forecasting accuracy comparing to ANN model on one dataset, and nearly 5% improvement on another."691,Forecasting Multi-Appliance Usage for Smart Home Energy Management,"We address the problem of forecasting the usage of multiple electrical appliances by homeowners where a key challenge is to model the everyday routine of homeowners and the interâ€“dependency between the use of different appliances. By so doing we develop one of the key building blocks of home energy management systems that aim to suggest homeowners when is the best time to run appliances in order to save money, without violating their preferred everyday habits. To this end, we propose a graphical model based prediction algorithm that captures the everyday habits and the interâ€“dependency between appliances by exploiting their periodic features. We demonstrate through extensive empirical evaluations that our approach outperforms existing methods by up to 30% on realâ€“world data from a prominent database of home energy usage."692,A Text Scanning Mechanism Simulating Text Reading Process of Human,"The purposes of previous techniques on text information processing are limited in one or several applications because of neglecting the general text reading process of human. This paper proposes a general text scanning mechanism simulating the text reading process of human. The mechanism consists of recall process, association process and forgetting process. Experiments show that the mechanism is effective for dealing with multiple applications."693,Human Activity Recognition Using Covariance of 3D Joint Features,"Human activity recognition is a difficult, and yet a very importance machine vision task, with applications in human-robot/machine interaction, interactive entertainment, multimedia information retrieval, and surveillance. Recently, the availability of cheap and accurate 3D sensors, such as Microsoft's Kinect, elevated hope for practical and accurate solution to the problem. We present a novel approach to human activity recognition from 3D skeleton sequences obtained through a Kinect sensor. Our approach relies on using the covariance matrix for skeleton joint locations over a temporal sequence as a discriminative feature vector for the sequence. The descriptor's length's size is independent from the length of the sequence. Our experiments show that using the covariance descriptor with off-the-shelf classification tools outperforms the state of the art in multiple datasets."715,An approach to abductive reasoning in equational logic ,"Abduction has been extensively studied in propositional logic because of its many applications in artificial intelligence. However, its intrinsic complexity has been a limitation to the implementation of abductive reasoning tools in more expressive logics. We have devised such a tool in ground flat equational logic, in which literals are equations or disequations between constants. Our tool is based on the computation of prime implicates and it uses a relaxed paramodulation calculus, designed to generate all prime implicates of a formula, together with a carefully defined data structure storing the implicates and able to efficiently detect, and remove, redundancies. In addition to a detailed description of this method, we present an analysis of some experimental results.
"720,Opinion Target Extraction Using Partial-Supervised Word Alignment Model,"Mining opinion targets from online reviews is an important and challenging task in opinion mining. This paper proposes a novel approach to extract opinion targets by using partial-supervised word alignment model (PSWAM). At first, we apply PSWAM in a monolingual scenario to mine opinion relations in sentences and estimate the associations between opinion target candidates and opinion words. Then, a graph-based algorithm is exploited to estimate the confidence of each candidate where the candidates with higher confidence will be extracted as the opinion targets. Compared with previous syntax-based methods, PSWAM can effectively avoid errors from parsing when dealing with informal sentences in online reviews. Compared with the methods using alignment model, PSWAM can capture opinion relations more precisely by using partial alignment links as constrains when performing alignment. Moreover, when estimating candidate confidence using our graph-based algorithm, we make penalties on higher-degree vertices in order to decrease the probability of the random walk running into the unrelated regions in the graph. As a result, the precision can be improved. The experimental results on three data sets with different sizes and languages show that our approach can achieve superior performance on the state-of-the-art methods."719,Are there any nicely structured preference profiles nearby?,"We investigate the problem of deciding whether a given preference profile is close to a nicely structured preference profile of a certain type, as for instance single-peaked, single-caved, single-crossing, value-restricted, best-restricted, worst-restricted, medium-restricted, or group-separable profiles. We measure this distance by the number of voters or alternatives that have to be deleted so as to reach a nicely structured profile. Our results classify all considered problem variants with respect to their computational complexity, and draw a clear line between computationally tractable (polynomial time solvable) and computationally intractable (NP-hard) questions."721,Dimensionality reduction with generalized linear model,"In this paper, we propose a probabilistic model called Generalized Linear Principal Component Analysis (GLPCA) for dimensionality reduction problems. In this model, different distributions and reconstruction (link) functions can be used to model data of different domains. For example, real valued data can be modeled by the Gaussian distribution with a linear reconstruction function, whereas binary valued data may be more appropriately modeled by the Bernoulli distribution with a logit or probit function. Many useful models as special cases of GLPCA, including the standard PCA and exponential family PCA. A unified iterative algorithm to obtain the maximum likelihood solutions of the models is also provided. Some specific models derived from GLPCA are presented. Experimental results of these models on several data sets are shown for the validation of GLPCA."731,Towards a Knowledge Compilation Map for Heterogeneous Representation Languages,"The knowledge compilation map introduced by Darwiche and Marquis takes advantage of a number of concepts (mainly queries and transformations, expressiveness, and succinctness) to compare the relative adequacy of representation languages to some AI problems. However, the corresponding framework is limited to the comparison of languages that are interpreted in a homogeneous way (formulae are interpreted as Boolean functions.) This prevents one from comparing on a formal basis, for the representation purpose, languages which are close in essence, such as OBDD, MDD, and ADD.
To fill the gap, we present a generalized framework into which comparing formally heterogeneous representation languages becomes feasible. In particular, we explain how the key notions of queries and transformations, 
expressiveness, and succinctness can be lifted to the generalized setting."736,An ensemble of Bayesian networks for multilabel classification,"We present a novel approach for multilabel classification based on an ensemble of Bayesian networks. The class variables are connected by a tree; each model of the ensemble uses a different class as root of the tree. We assume  the features to be conditionally independent given the classes, thus generalizing the naive Bayes assumption to the multiclass case. This assumption allows to optimally identify  the arcs linking class and features; such arcs are moreover shared across all models of the ensemble. The inferences of the models of the ensemble are combined via logarithmic opinion pool.
The marginal probability of the classes are computed by running standard inference on each Bayesian Network, and then pooling the inferences: this is the approach used for minimizing the risk under decomposable loss function.
We instead adopt an integer linear programming approach for detecting the most probable joint configuration of the classes given the observed features.
Experiments show that the model is competitive with state of the art approaches for multilabel classification."742,Why so hard to say sorry: evolution of apology with commitment in the iterated Prisonerâ€™s Dilemma,"When making a mistake, individuals apologize for their wrongdoing in order to secure further cooperation, even if the apology is  costly. Similarly, individuals arrange commitments to guarantee that  an action such as a cooperative one is in the others\\\' best interest, and thus will be carried out to avoid eventual penalties for commitment failure. Hence, both apology and commitment should go side by side in behavioral evolution. Here we provide a computational model showing that apologizing acts are rare in non-committed interactions,  especially whenever cooperation is very costly, and that arranging prior commitments can considerably increase the frequency of such behavior. In addition, we show that in both cases, with or without commitments, apology works only if it is sincere, i.e. costly enough. Most interestingly, our model predicts that individuals tend to use much costlier  apology in committed relationships than otherwise, because it helps better identify free-riders such as fake apologizers: `commitments bring on sincerity\\\'. Furthermore, we show that this strategy of apology supported by commitments outperforms  the famous existent strategies of the iterated Prisoner\\\'s Dilemma. "745,Retweet Behavior Understanding through Influence Locality Analysis," When you retweet a tweet in a microblogging network, will the behavior influences your friends to also retweet the same tweet? We study the influence locality on users' retweet behaviors and find that the previous retweet behaviors on the same tweet from the followees do not tightly control the retweet likelihood, and meanwhile, the circles formed by those followees take a strong negative influence on the retweet likelihood with more circles. We propose a generative model to learn the influence locality effects as well as other factors from tweet, publisher, receiver and social tie. The experimental results on the task of retweet behavior prediction reveal that incorporating influence locality from followees can indeed lead to statistically significant improvements."748,C-Link: a hierarchical clustering approach to coalition formation,"Coalition formation is a crucial problem for a wide range of applications that require agents to form collectives and operate as groups (e.g., emergency management, surveillance and security, collective purchasing of goods). In this paper we address the specific problem of coalition structure generation,  proposing a novel heuristic approach that is based on data clustering methods. In more detail, we propose a hierarchical agglomerative clustering approach (C-Link), which uses a similarity measure between coalitions based on the gain that the system achieves if two coalitions merge together. We empirically evaluate C-Link on a synthetic benchmarking data-set as well as on a specific coalition formation problem were users can group together to buy energy at better prices. Our results show that the C-link approach performs very well against an optimal benchmarking algorithm based on Linear Programming, achieving solutions which are in the worst case about 80\\% of the optimal in the synthetic data-set, and 98\\% of the optimal in the energy data-set. Moreover, we are able to provide solutions for problems involving thousands of agents (2.732) in few minutes (about 4)."1386,Intention-Aware Routing to Minimise Delays at Electric Vehicle Charging Stations,"En-route charging stations allow electric vehicles to charge away from home, thus greatly extending their range. However, as a full charge takes a considerable amount of time, there may be significant congestion and long waiting times at peak times. To address this problem, we propose a novel navigation system, which uses intentions (i.e. routing policies) of other agents employing the system to accurately predict congestion at charging stations, and suggest the most efficient route. We achieve this by extending time-dependent stochastic routing algorithms from the transportation literature to include the battery\'s state-of-charge and charging stations. Furthermore, we combine historic information with agent intentions to model the queues at charging stations and predict their waiting times. Through simulations we show that an agent significantly benefits from using the intention-aware system compared to using only historic information, and that average waiting times decrease as more agents use the system. 
"749,DeQED: an Efficient Divide-and-Coordinate Algorithm for DCOP,"This paper presents a new DCOP algorithm called DeQED (Decomposition with Quadratic Encoding to Decentralize).&amp;#12288;DeQED is based on the Divide-and-Coordinate (DaC) framework, where the agents repeat solving their updated local sub-problems (the divide stage) and exchanging coordination information that causes to update their local sub-problems (the coordinate stage). Unlike other DaC-based DCOP algorithms, DeQED does not essentially increase the complexity of local sub-problems and allows agents to avoid exchanging (primal) variable values in the coordinate stage. Our experimental results show that DeQED significantly outperformed other incomplete DCOP algorithms for both random and structured instances."751,Do Hard SAT-Related Reasoning Tasks Become Easier in the Krom Fragment?,"We look at ways to make hard, SAT-related reasoning tasks easier by restricting the form of the underlying propositional formulas. While the restriction to Horn formulas (i.e, conjunctive normal form where every clause has at most one positive literal) is usually well studied, other restrictions in Schaefer\'s famous framework for identifying tractable fragments of SAT have received much less attention. In this work, we investigate the restriction to Krom formulas (i.e., the clauses have at most two literals) in the context of several reasoning tasks. We first consider a variant of the SAT problem to obtain some technical tools, which we then apply to derive new complexity results in the areas of Belief Revision and Logic-Based Abduction. It turns out that in some cases the restriction to Krom already decreases the complexity while in others it does not. We thus also consider additional restrictions to Korm formulas in order to achieve tractability for these problems."754,Efficient Exploration for Accelerating Human-Robot Interactive Learning in Multiclass Prediction,"A robot continually learns from interacting with humans in multiclass prediction tasks. The human feedbacks over the robot predictions naturally vary from giving the true labels, to just partially as correct/incorrect replies. Our algorithms allow the robot to efficiently explore informative predictions, while trying to predict as correctly as it has learned. Good classification accuracies are obtained when the interactive learning proceeds, outperforming recent algorithms running in the same partial feedback setting, and even beating existing algorithms running in full feedback. Furthermore, our algorithms allow an informed way for knowledge transfer between robots learning in parallel on the same task, thus reducing duplicate learning efforts considerably. While our experiments focus on several benchmark datatsets, our methods apply to any multi-class visual learning and recognition task in robotics."755,Tractable Upper Bounds on Lengths of Transition Sequences,"We describe an approach to computing upper bounds on the lengths of solutions to reachability problems in transition systems, based on a decomposition of state-variable dependency graphs.
Our approach is able to find practical upper bounds in a number of planning benchmarks. Moreover it is tractable, running in polynomial time in the number of actions and propositional variables that characterize the problem."896,Opponent Modelling in Persuasion Dialogues,"A strategy is used by a participant in a persua-
sion dialogue to select from the possible locutions
it may make, one which is most likely to achieve its
objective of persuading its opponent. Such strate-
gies often assume that the participant has a model
of its opponents; however, how such a model is constructed is not normally considered. We define mechanisms: 1) for updating an opponent model based on their dialogue behaviour, and; 2) for augmenting that model with arguments likely to be dialectally related to information already contained in it, based on the participantâ€™s past dialogue experience. Precise computation of this likelihood is exponential in the volume of related information. We thus describe and evaluate an approximate approach for computing these likelihoods based on Monte-Carlo simulation.
"777,Path Integral Control by Reproducing Kernel Hilbert Space Embedding,"We present an embedding of stochastic optimal control problems, of the so called path integral form, into reproducing kernel Hilbert spaces. Using consistent, sample based estimates of the embedding leads to a model-free, non-parametric approach for calculation of an approximate solution to the control problem. This formulation admits a decomposition of the problem into an invariant and task dependent component. Consequently, we make much more efficient use of the sample data compared to previous sample based approaches in this domain, e.g., by allowing sample re-use across tasks. Numerical examples on test problems, which illustrate the sample efficiency, are provided."784,Improving Question Retrieval in Community Question Answering Using World Knowledge,"Community question answering (cQA), which pro-
vides a platform for people with diverse back-
ground to share information and knowledge, has
become an increasingly popular research topic. In
this paper, we focus on the task of question re-
trieval. The key problem of question retrieval is to
measure the similarity between the queried ques-
tions and the historical questions which have been
solved by other users. The traditional methods
measure the similarity based on the bag of words
(BOW) representation. This representation neither
captures dependencies between related words, nor
handles synonyms or polysemous words. In this
work, we first propose a way to build a concept
thesaurus based on the semantic relations extracted
from the world knowledge of Wikipedia. Then, we
develop a unified framework to leverage these se-
mantic relations in order to enhance the question
similarity in the concept space. Experiments con-
ducted on a real cQA data set show that with the
help of Wikipedia thesaurus, the performance of
question retrieval is improved as compared to the
traditional methods."785,Annealed Importance Sampling for Structure Learning in Bayesian Networks,"We present a new sampling approach to Bayesian inference for structural features in Bayesian networks. Like some earlier sampling methods, we sample linear orders on nodes rather than directed acyclic graphs (DAGs). The key difference is that we replace the Markov chain Monte Carlo (MCMC) method by the method of annealed importance sampling (AIS). We show that AIS not only tends to be at least as efficient as MCMC but also that it is superior to MCMC in two ways: it enables easy and efficient parallelization, due to the independence of the samples, and lower-bounding of the marginal likelihood of the model with good probabilistic guarantees. We also provide a principled way to correct for the bias from the uniform prior on DAGs, by implementing an efficient algorithm for counting the linear extensions of a given partial order."794,How to predict human strategic decisions using facial expressions,"Facial expressions play a vital role in social communication. 
People\\\'s facial expressions, whether done consciously or subconsciously, continuously signal their state of mind. 
Understanding these signals can be highly beneficial in strategic scenarios. 
Advances in the field of automatic real-time facial recognition and facial expression measurement enable computer systems to deploy real-time algorithms that use this information. 
Ekman and Friesen\\\'s Facial Action Coding System (FACS) encodes facial muscles movements from changes in facial appearance. 
Automated system can track faces in a video, and produces temporal profiles of each facial muscle movement (i.e. Action Units). 
However, extracting Action Units in strategic situations where people have incentive to hide their true emotion and present only slight muscle movements can be highly dubious. 
For these  strategic situations, we propose an innovative methodology  that uses correlation of temporal changes in distinct facial points and machine learning methods to predict people\\\'s strategic decisions. 
We have conducted an extensive study with human subjects using a variant of a well studied strategic game, the Centipede game, including video logs of their movements. 
The results show that our method outperforms human experts and state of the art machine learning method in predicting subjectsâ€™ strategic decision. 
To the best of our knowledge, this is the first study to present a methodology for predicting human strategic decision when there is an incentive to hide facial expressions and therefore people do not present distinct facial expressions but only slight muscle movements.
"799,Positive Subsumption in Fuzzy EL with General t-norms,"The Description Logic (DL) EL is used to formulate several large biomedical ontologies. Fuzzy extensions of EL can express the vagueness inherent in many biomedical concepts. We study the reasoning problem of deciding positive subsumption in fuzzy EL with semantics based on general t-norms. We show that the complexity of this problem depends on the specific t-norm chosen. More precisely, if the t-norm has zero divisors, then the problem is NP-hard; otherwise, it can be decided in polynomial time."802,Multi-Agent Subset Space Logic,"Subset space logics have been introduced and studied as a framework for reasoning about a notion of effort in epistemic logic. The seminal Subset Space Logic (SSL) by Moss and Parikh modeled a single agent, and most work in this area has focused on different extensions of the language or different model classes resulting from restrictions on subset spaces while still keeping the single-agent assumption. In this paper we argue that the few existing attempts at multi-agent versions of SSL are unsatisfactory, and propose a new multi-agent subset space logic which is a natural extension of single-agent SSL. The main results are a sound and complete axiomatization of this logic, as well as an alternative and equivalent relational semantics for SSL.
"803,Measuring Statistical Dependence via the Mutual Information Dimension,"We propose to measure statistical dependence between two random variables by the mutual information dimension (MID), and present a scalable parameter-free estimation method for this task. Supported by sound dimension theory, our method gives an effective solution to the problem of detecting interesting relationships of variables in massive data, which is nowadays a heavily studied topic in many scientific disciplines. Different from classical Pearson's correlation coefficient, MID is zero if and only if two random variables are statistically independent and is translation and scaling invariant. We experimentally show superior performance of MID in detecting various types of relationships in the presence of outliers. Moreover, we illustrate that MID can be effectively used for feature selection in regression."804,Forward Perimeter Search with Controlled Use of Memory,"There are many problems that neither A* nor IDA* can solve, because A* runs
out of memory and IDA* runs out of time. \emph{Forward Perimeter Search
(FPS)} is a compromise between the two extremes. It builds a perimeter
around the start node and tests for each perimeter node whether it lies on
a shortest path to the goal. The testing is done in parallel with
breadth-first iterative-deepening A* (BF-IDA). Its information is used to
refine the perimeter, so that the goal will be found early in the last
iteration.

Our empirical results on two application domains, 24-puzzle and 17-pancake,
indicate that FPS expands fewer nodes than BF-IDA* while requiring several
orders of magnitude less memory space.

Additionally, we present a hard problem instance of the 24-puzzle that needs
at least 26 more moves to solve than the currently hardest instance. We prove
with FPS that its solution requires either 140 or 142 moves."808,Valuated paths as condensed pattern in a single attributed DAG,"Directed acyclic graphs can be used accross many application domains. They especially fit to spatio-temporal problems. In this paper, we study a new pattern domain for supporting their analysis. Therefore, we propose the pattern language of weithed paths, primitive constraints that enable to specify their relevancy (e.g., frequency and compactness constraints), and  algorithms that can compute the  specified collections. It leads to a condensed representation setting whose efficiency and scalability are empirically studied."811,Combine Constituent and Dependency Parsing via Reranking,"This paper presents a reranking approach for combining constituent and dependency parsing, for the purpose of improving parsing performance on both sides. Most previous combination methods rely on complicated joint decoding to integrate together graph- and transition-based dependency models. Instead, our approach first makes use of a high-performance PCFG model to output k-best candidate constituent trees, and then a dependency parsing model to rerank the trees by summing up their scores from both models, so as to get the most probable parse. We evaluate this approach using the English (WSJ) and Chinese (CTB5.1) treebanks. Experimental results show that our reranking approach achieves the highest accuracy of constituent and dependency parsing on CTB5.1 and is comparable to the state-of-the-art systems on WSJ."821,Automated reasoning to infer all minimal keys," Wastl introduced  for first time a Tableaux-like method based on an inference system for deriving all minimal keys from a relational  schema.  He introduced two inference rules and built an automated method over them: one rule is used to built the root of the tree and the other is applied to each functional dependency belonging to the relational schema, providing a new branch in the tableaux. In this work we tackle the key finding problem with a tableaux method, but we will use two inference rules inspired by the Simplification Logic for Functional Dependencies. The main novelty of these rules is that they deal with generalized formulas, avoiding the fragmentation needed in the former tableaux, which  requires atomic right hand sides in the functional dependencies. This characteristic provides a pruning in the branching process and drastically reduces the tableaux size. Besides that, we also introduce a reduction of the original problem by using some algebraic results which allow us to transform the original problem into an equivalent but more simpler one. Finally we illustrate the advantages of our new tableaux method with an experiment."827,Mining for Analogous Tuples from an Entity-Relation Graph,"The ability to recognize analogies is an important factor that is closely
 related to human intelligence. Verbal analogies have been used for evaluating
 both examinees at university entrance exams as well as algorithms for measuring
 relational similarity. However, relational similarity measures proposed so far
 are confined to measuring the similarity between pairs of words.
 Unfortunately, such pairwise approaches ignore the rich relational structure
 that exists in real-world knowledge bases involving millions of entities and semantic relations.
 We propose a method to efficiently
 identify analogous entity tuples from a given entity-relation graph.
 First, we present an efficient approach for extracting potential analogous tuples
 from a given entity-relation graph.
 Second, to measure the structural similarity between two tuples, 
 we propose two types of kernel functions: vertex-feature kernels, and
edge-feature kernels. Moreover, we combine those kernels to construct
 rich composite kernels that consider both vertex and edge features. 
 Experimental results show that the proposed method accurately identifies
 analogous tuples and significantly outperforms the current state-of-the-art
 pairwise relational similarity measure when used to measure the relational similarity
 between tuples."828,Evolution of Common-Pool Resources and Social Welfare in Structured Populations ,"The Common-Pool Resource (CPR) game is a social dilemma where players have to decide how to consume a shared CPR. Either they each take their cut, completely destroying the CPR, or they restrain themselves, gaining less immediate profit but sustaining the resource and future profit.  When no consumption takes place the CPR, modeled here by a classic Ricker model, simply grows to its carrying capacity. As such, this CPR dilemma provides a framework to study the sustainability of resources whose size adjusts dynamically due to consumption and its own implicit dynamics. The present study provides a detailed analysis of the evolutionary dynamics of the CPR game, focusing on the interplay between the resource levels and the competition between selfish and restrained consumers. First, we show analytically when the restrained consumers win against the selfish ones, and how this affects the resourcesâ€™ carrying capacity. Second, we show the different effects of population structures on the levels of cooperation, its average payoff and the resulting average carrying capacity of the resources. Interestingly, our results show that in heterogeneous populations, cooperation thrives and CPR grows sustainably, leading to a significantly higher social welfare compared to that obtained in homogeneous or well-mixed populations. "830,Answer Extraction from Passage Graph for Factoid Question Answering,"In question answering, answer extraction aims to pin-point the exact answer from retrieved passages. However, most previous methods perform such extraction on each passage in a separate way, without considering the clues provided by other passages. Motivated by the finding that answer extraction could be improved by using consensus information among all passages, this paper presents a novel approach to extract answers by leveraging passage graphs, that are constructed by adding links between multiple passages on the basis of lexical consensus, and modeled by factor graph. Experimental results on multiple QA data sets from different domains demonstrate that our method significantly improves the performance of answer extraction component."838,Learning Community-based Preferences via Dirichlet Process Mixtures of Gaussian Processes,"Bayesian approaches to preference learning using Gaussian Processes (GPs) are attractive due to their ability to explicitly model uncertainty in users\' latent utility functions over items and to learn kernels over the joint user and item space. Since preferences depend heavily on the social class and the community users belong to, it is essential to model this aspects of users\' attributes that are not explicitly collectible. We will further use this model for prediction based on users\' community. We empirically demonstrate the effectiveness of our approach."843,A Hidden Markov Model-Based Cicada Detector for Crowdsourced Smartphone Biodiversity Monitoring,"The New Forest cicada is the only species of cicadidae present in the UK and is in great danger of becoming extinct. Despite this, few entomologists are searching for it and, as its call is difficult for humans to hear, the chances of saving it are slim. We propose a smartphone-based citizen science approach is proposed to equip visitors to the New Forest with a smartphone app that can detect the presence of the cicada in real-time. However, current automated insect detection systems are aimed at batch classification and not suited for real-time detection in a noisy environment. To address this shortcoming, we propose an efficient approach based on a hand crafted hidden Markov model (HMM), to which we feed as a single feature vector a narrow band DFT computed with a Goertzel filter on the central frequency of the insectâ€™s song. We compare this approach to a complex state-of-the-art multi-species system for batch classification. The evaluation is performed on a large set of raw recordings taken with a smartphone, all unprocessed and subject to different kinds of noise. Our results show the robustness of the proposed approach for individual or few species, and the considerable gain in computational complexity that can be achieved."850,Ontology-Based Data Access with Closed Predicates is Inherently Intractable (Sometimes),"When using ontologies to access instance data, it can be useful to make a closed world assumption for some predicates and an open world assumption for others. A main problem with such a setup is that conjunctive query (CQ) answering becomes intractable regarding data complexity already for inexpressive description logics such as DL-Lite and EL. We take a non-uniform perspective to analyze this situation in more detail, that is, we consider the data complexity of conjunctive query (CQ) answering on the level of individual ontologies.  It turns out that, whenever CQ answering with closed and open predicates w.r.t. a DL-Lite or EL ontology is tractable, then it coincides with CQ answering where all predicates are open.  In this sense, CQ answering with closed predicates is inherently intractable in these logics. Our analysis also yields a dichotomy between AC0 and coNP for CQ answering w.r.t. ontologies formulated in DL-Lite and a dichotomy between PTime and coNP for EL. Interestingly, the situation is less dramatic for ontologies formulated in the more expressive description logic ELI, where we find ontologies for which CQ answering is in PTime, but does not coincide with CQ answering where all predicates are open."855,Parametric Local Multimodal Hashing for Cross-view Similarity Search,"Recent years have witnessed the growing popularity of hashing for efficient large-scale similarity search. It has been shown that the hashing quality could be boosted by hashing function learning (HFL).In this paper, we study HFL in the context of multimodal data for cross-view similarity search. We present a novel multimodal HFL method, called Parametric Local Multimodal Hashing (PLMH), which learns a set of hash functions to locally adapt to the data structure of each modality. To balance locality and computational efficiency, the hashing projection matrices of each instance is parameterized, with guaranteed approximation error bound, as a linear combination of basis hashing projections of a small set of anchor points. A local optimal conjugate gradient algorithm is designed to learn the hash function for each bit, and the overall hash codes are learned in sequential manner to progressively minimize the bias. Experimental evaluations on cross-media retrieval tasks demonstrate that PLMH performs competitively against the state-of-the-art methods."873,The Route to Success - A Performance Comparison of Diagnosis Algorithms,"Diagnosis, i.e., the identification of root causes for failing or unexpected system behavior, is an important task in practice. Within the last three decades, many different AI-based solutions for solving the diagnosis problem have been presented and have been gaining in attraction. This leaves us with the question of which algorithm to prefer in a certain situation. In this paper we contribute to answering this question. In particular, we compare two classes of diagnosis algorithms. One class exploits conflicts, i.e., sets of system components whose correct behavior contradicts given observations, in their search. The other class ignores conflicts and derives diagnoses from observations and the underlying model directly. In our study we use different reasoning engines ranging from an optimized horn-clause theorem prover to general SAT and constraint solvers. Thus we also address the question whether publicly available general reasoning engines can be used for an efficient diagnosis."857,First Order-Rewritability of Atomic Queries in Horn Description Logics,"One of the most advanced approaches to querying data in the presence
of ontologies is to make use of relational database systems,
rewriting the original query and the ontology into a new query that
is formulated in SQL or, equivalently, in first-order logic
(FO). For ontologies written in many standard description logics
(DLs), however, such FO-rewritings are not guaranteed to exist. We
study FO-rewritings and their existence for a basic class of queries
and for ontologies formulated in Horn DLs such as
Horn-SHI and EL.  Our results include characterizations
of the existence of FO-rewritings, tight complexity bounds for
deciding whether an FO-rewriting exists (ExpTime and PSpace), and
tight bounds on the (worst-case) size of FO-rewritings,
when presented as a union of conjunctive queries.
"867,Behavioral Diagnosis of LTL Specifications at Operator Level,"Product defects and rework efforts due to flawed specifications represent major issues for a project\'s performance, so that there is a high motivation for providing effective means that assist designers in assessing and ensuring a specification\'s quality. Recent research in the context of formal specifications, e.g. on coverage and vacuity, offers important means to tackle related issues. In the currently underrepresented research direction of diagnostic reasoning on a specification, we propose a scenario-based diagnosis at a specification\'s operator level using weak or strong fault models. Drawing on efficient SAT encodings, we show in this paper how to achieve that effectively for specifications in LTL. Our experimental results illustrate our approach\'s validity and attractiveness.
"869,Employing Batch Reinforcement Learning to Control Gene Regulation Without Explicitly Constructing Gene Regulatory Networks,"The goal of controlling a genetic regulatory network (GRN) is to generate an intervention strategy, i.e. a control policy, such that applying it, the system avoids undesirable states. In this work, we propose a novel algorithm for controlling GRNs based on Batch Mode Reinforcement Learning (Batch RL). Our idea is that time series gene expression data can actually be interpreted as a sequence of experience tuples collected from the environment. Hence, instead of using a computational model for gene regulation and obtaining a control policy over the constructed model, we can directly use the available gene expression data to obtain an approximated control policy for gene regulation, which provides to reduce time and space requirements of the problem greatly. Results show that we can obtain controlling policies for gene regulation systems of several thousands of genes just in several seconds while existing solutions cannot obtain controlling policies for even several tens of genes. Interestingly, results also show that our model independent control solution produces policies with almost same quality with the existing model dependent control solutions. "874,Opponent Models with Uncertainty for Strategic Argumentation,"This paper deals with the issue of strategic argumentation in the setting of Dung-style abstract argumentation theory. Such reasoning takes place through the use of opponent models---recursive representations of an agent's knowledge and beliefs regarding the opponent's knowledge. Using such models, we present three approaches to reasoning. The first directly utilises the opponent model to identify the best utterance to advance in a dialogue. The second extends our basic approach through the use of quantitative uncertainty over the opponent's model. The final extension introduces virtual arguments into the opponent's reasoning process. Such arguments are unknown to the agent, but presumed to exist and interact with known arguments. They are therefore used to add a primitive notion of risk to the agent's reasoning. We have implemented our models and we have performed an empirical analysis that shows that this added expressivity improves the performance of an agent in a dialogue."884,Basic Level in Formal Concept Analysis: Interesting Concepts and Psychological Ramifications,"We continue our initial study regarding basic level of concepts in conceptual 
categorization. Basic level of concepts is an important phenomenon studied 
in the psychology of concepts. We propose to utilize this phenomenon in 
formal concept analysis (FCA) to select important formal concepts. This is 
a well-known critical problem because, as a rule, the number of all 
concepts extracted from data is large. For this purpose, we review and 
formalize the main existing psychological approaches to basic level which 
are presented only informally and are not related to any particular formal 
model of concepts in the psychological literature. We argue and demonstrate 
by examples that basic level concepts may be regarded as  the most interesting 
formal concepts from user viewpoint. Interestingly, our formalization and 
experiments reveal some previously unknown relationships between the
existing approaches to basic level. Thus, we  argue that a formalization of basic 
level in the framework  of FCA is beneficial for the psychologal investigations 
themselves because it puts them on solid ground."887,Granular Description of Qualitative Change,"Qualitative representations of spatial knowledge
have been widely studied and a variety of frameworks
are used to express relationships between
static regions. Dynamic regions present a much greater challenge,
but are important in practical applications such as 
describing crowds of people moving over time.
Previous work has analysed changes as regions
merge and split and as new regions are created and existing ones
disappear. We present a novel framework for the qualitative
description of spatial regions based on two levels of granularity.
Introducing granularity  yields
significantly more informative
qualitative descriptions than are available from
a single level of detail.
The formal model represents a region, which may have multiple 
components, as a bipartite graph where the nodes are the components
of the region at a fine level of detail and at a coarse level.
The edges of the graph model the way that a component in the
coarse view can be made up of parts of components at the more detailed level.
We show that all graphs of this form can be realized as regions in a discrete
space of pixels, and we develop a theory of relations between these graphs
to model the dynamic behaviour of regions."888,Parameter Learning for Latent Network Diffusion,"Diffusion processes in networks are increasingly used to model the spread of wildlife, information or social influence. Our work addresses the crucial problem of learning the underlying parameters that govern such a diffusion process by observing the time at which nodes become active. A key advantage of our approach is that, unlike previous work, it can tolerate missing observations for some nodes in the diffusion process. Having incomplete observations is characteristic of offline networks used to model the spread of wildlife. We develop the EM algorithm to address parameter learning in such settings. Since both the E and M steps are computationally challenging, we employ a number of optimization methods such as nonlinear and DC programming to address these challenges. Evaluation of the approach on the Red-cockaded Wood- pecker conservation problem shows that it is highly robust and accurately learns parameters in various settings, even with more than 80% missing data."893,Temporal Description Logic for Ontology-Based Data Access,"In ontology-based data access (OBDA), the most important technique for answering queries is to rewrite the ontology and the query into a single first-order query that can be evaluated directly over the data. The current W3C standard for OBDA is OWL 2 QL, which is based on the DL-Lite family of description logics. The aim of this paper is to extend OBDA to data with validity time and ontologies that allow temporal conceptual modelling. To this end, we design a temporal description logic TQL that extends OWL 2 QL, provides basic means for temporal conceptual modelling and ensures first-order rewritability of conjunctive queries for suitably defined data instances with validity time."898,Identifying Useful Human Feedback from an On-line Translation Service,"On-line translation systems process a huge number of translation requests per day. This great interaction with users of all type holds a strong potential for collecting information able to dynamically improve translation systems. Actually, on-line translation platforms implement procedures for gathering users\' feedback on the output of their translation requests (e.g., error fixing or post-editing the output sentences). In practice, however, such feedback is very noisy for various reasons and must be automatically filtered in order to identify the small fraction of user corrections that are useful for system improvement.
In this paper we present a study on automatic feedback filtering in a real weblog collected from Reverso.net. Building from previous work, we extend and re-annotate a training corpus, define an extended set of simple features and approach the problem as a binary classification task, experimenting with linear and kernel-based classifiers and feature selection. Results on the feedback filtering task show a significant improvement over the majority class, but at the same time show a precision ceiling around 70-80%. This reflects the inherent difficulty of the problem and evinces the fact that shallow features cannot fully capture the semantic nature of the problem. Despite the modest results on the filtering task, the resulting classifiers are shown to be useful in an application-based evaluation. The feedback instances selected from a new dataset with 6.5K instances, allow to consistently improve the performance of a phrase-based translation system, according to a set of standard evaluation metrics."907,Self-Organized Neural Learning of Statistical Inference from High-Dimensional Data ,"With information about the world implicitly embedded in complex, high-dimensional neural population responses, the brain must perform statistical inference on a large scale to form beliefs about the state of the environment. This ability is, in part, acquired after birth and in many instances with very little explicit feedback to guide learning. This is a very difficult problem considering the little information about the meaning of individual neural responses available at birth. In the present paper, we address the question of how the brain might solve this problem: We present an unsupervised artificial neural network algorithm which takes from the popular self-organizing map (SOM) algorithm the ability to learn a latent variable model from its input. It extends the SOM algorithm by learning about noise in the input and computing probability density functions over the latent variables which it represents in a population code. This is done with very few assumptions about the distribution of noise. Our simulations indicate that our algorithm can learn to perform similar to a maximum likelihood estimator with the added benefit of requiring no a-priori knowledge about the input and computing not only a best hypothesis, but also probabilities for alternatives. "910,Integrating Syntactic and Semantic Analysis into the Open Information Extraction Paradigm,"In this paper we present a computationally efficient approach to enrich the Open Information Extraction paradigm with syntactic and semantic features. To obtain this goal, we combine deep syntactic analysis and distributional semantics using a kernel method and soft clustering. The output of our system is a set of automatically discovered and ontologized semantic relations."911,Syntactic Labelled Tableaux for Lukasiewicz Fuzzy ALC,"Fuzzy description logics (DLs) serve as a tool to
handle vagueness in real-world knowledge. There is particular
interest in logics implementng Lukasiewicz semantics, which has a
number of favourable semantic properties. Current decision
procedures for Lukasiewicz fuzzy DLs work by reducing ABox
satisfiability to an exponentially large mixed integer linear
programming problem. Here, we present a decision method that stays
closer to logical syntax, a labelled tableau algorithm for
satisfiability of acyclic ABoxes in Lukasiewicz fuzzy ALC that
calls only on (pure) linear programming, and this only to decide
atomic clashes. The algorithm realizes the best known complexity  bound, NEXPTIME. Our analysis of completeness and complexity relies
on invertibility of all rules, a method that we believe may be
widely applicable in logics of nondeterministic time complexity.
"917,Using Conditional Restricted Boltzmann Machine for Highly Competitive Negotiation taks,"Learning in automated negotiations, while useful, is hard because of the indirect way the target function can be observed and the limited amount of experience available to learn from.  Transfer learning is a branch of machine learning research concerned with the reuse of previously acquired knowledge in new learning tasks to, for example, reduce the amount of learning experience required to attain a certain level of performance. This paper proposes a novel and powerful regression method --- CRBM, that can be used in a complex multi-issue negotiation setting, and moreover design the mechanism how to effectively do model transfer among different negotiation tasks. The empirical evaluation of this suggested approach outperforms existing state-of-the-art strategies in complex domains, in which negotiating parties are highly competitive to each other. Its robustness are also reported to be strong by means of empirical game theoretic analysis.  "919,A matroid approach to the worst case allocation of indivisible goods,"Demko and Hill addressed the problem of equitably allocating a set of indivisible goods to $n$ agents and focused on how good the least happy agent can value his share [S. Demko and T.P. Hill. Equitable distribution of indivisible objects. Mathematical Social Sciences, 16:145â€“158, 1988.].
Following [T. P. Hill. Partitioning general probability measures. The Annals of Probability, 15(2):pp. 804â€“813, 1987.], they derived a family of functions $V_n$ which are non-increasing in a parameter $\gamma$ which is the maximum value assigned by an agent to a single good. Then, it is possible to allocate the indivisible goods in such a way that every agent values his share at least $V_n(\gamma)$.

Interestingly, $V_n(\gamma)$ is tight for some values of $\gamma$, ie. it is the  best lower bound on the valuation of the least happy agent. However, it is not true for the entire interval where $\gamma$ lies. 
Recently, a deterministic algorithm returning such an allocation in polynomial time was proposed [E. Markakis and C.A. Psomas. On worst-case allocations in the presence of indivisible goods. Internet and Network Economics, pages 278â€“289, 2011.]. This algorithm comes with a slight strengthening of the  guarantee since they prove that every agent $k$ values his share at least $V_n(\gamma_k)$, where $\gamma_k$ is the maximum value assigned by agent $k$ to a single good.

Our contribution is twofold: $(i)$ We propose a family of functions $W_n$ such that $W_n(x) \geq V_n(x)$ for all $x$, and $W_n(x) &gt; V_n(x)$ for values of $x$ where $V_n(x)$ is not tight. $(ii)$ The new functions $W_n$ apply on a problem which generalizes the allocation of indivisible goods. It is to find a solution (base) $B$ in a matroid which is common to $n$ agents and every agent $k$'s value for $B$ is at least $W_n(\gamma_k)$; $\gamma_k$ is the maximum value given by agent $k$ to an element of the matroid.

Our results are constructive, they are achieved by analyzing an extension of the algorithm of Markakis and Psomas."921,An alternative axiomatization of DEL and its applications,"In this paper, we provide a new axiomatization of Dynamic Epistemic Logic, based on the completeness proof method proposed in [Wang and Cao, 2013]. This axiomatization does not use any of the reduction axioms, but naturally specifies the essential properties of the update product. We demonstrate the use of our new axiomatization and the corresponding proof techniques by the following three sets of results: characterization theorems of the update operations, representation theorems of the DEL-generatable epistemic temporal structures given a fixed event model, and a completeness theorem for DEL with protocols."924,An Epistemic Halpernâ€“Shoham Logic,"Several studies have been conducted on the satisfiability of various fragments of interval-based Halpern-Shoham logic. We define a family of epistemic extensions of Halpern-Shoham logic for reasoning about temporal-epistemic properties of multi-agent systems. We exemplify their use and study the complexity of their
model checking problem. We show a range of results ranging from PTime to PSpace-hard depending on the logic considered.
"927,Problem Splitting using Heuristic Search in Landmark Orderings," In this paper, we revisit the idea of splitting a planning problem into subproblems hopefully easier to solve with the help of landmark analysis. This technique initially proposed in the first approaches related to landmarks in classical planning has been outperformed by landmark-based heuristics. But we believe that it is still a promising research direction. To this end, we propose a new method for problem splitting based on landmarks, which has two advantages over the original technique: it is complete (if a solution exists, the algorithm finds it), and it uses the precedence relations over the landmarks in a more flexible way. We lay in this paper the foundations of a meta best-first      search algorithm, which explores the landmark orderings and can use any embedded planner to solve each subproblem. It opens up avenues for future research: among them are new heuristics for guiding the meta search towards the most promising orderings, different policies for expanding nodes of the meta search, influence of the embedded subplanner, and parallelization strategies of the meta search.
"928,Run-Time Improvement of Point-Based POMDP Policies,"The most successful recent approaches to partially observable Markov
decision problem (POMDP) solving have largely been point-based
approximation algorithms. These work by selecting a finite number of belief
points, computing alpha-vectors for those points, and using the resulting
policy everywhere. However, if during execution the belief state is far
from the points, there is no guarantee that the policy will be good. This
case occurs either when the points are chosen poorly or there are too few
points to capture the whole optimal policy, for example in domains where
there are many low probability transitions, such as faults or exogenous
events. In this paper we explore the use of an on-line plan repair approach
to overcome this difficulty. The idea is to split computation between
off-line plan creation and, if necessary, on-line plan repair. We evaluate
a variety of heuristics used to determine when plan repair might be useful,
and then repair the plan by sampling a small number of additional belief
points and recomputing the policy. We show in several domains that the
approach is more effective than either off-line planning alone even with
much more computation time, or a purely on-line planning based on forward
search. We also show that the overhead of checking the heuristics is very
small when replanning is unnecessary."929,A Formal Account of Nondeterministic and Failed Actions,"In all but the simplest action domains, nondeterminism is pervasive:
an agent may flip a coin or pick up a different object than intended,
or an action may fail and may fail in different ways.
In this paper we provide a qualitative theory of nondeterminism (in
contrast to the more usual probability-based approaches).
The account is based on an epistemic extension to the situation calculus
that also accommodates sensing actions.
Our position is that nondeterminism is an epistemic phenomenon, and that the
world is most usefully regarded as deterministic.
Nondeterminism arises from an agent's limited awareness and perception.
The account offers several advantages:
an agent has a set of categorical (as opposed to probabilistic)
beliefs, yet can deal with equally-likely outcomes (such as in flipping a
fair coin) or with outcomes of differing plausibility (such as an action
that may on rare occasion fail).
"932,MiningZinc: A Language for Constraint-based Mining,"We introduce MiningZinc, a modeling language for constraint-based mining, one of the most popular tasks in data mining. MiningZinc provides high-level and natural modeling of pattern mining problems, the resulting  models closely resemble the definitions found in the data mining literature. MiningZinc also supports user-defined constraints and optimization criteria, is solver-independent and supports the use of standards constraint solvers as well as data mining systems. It is effective on a wide range of tasks.
MiningZinc builds upon and extends the state-of-the-art in constraint programming, more specifically, the constraint language, framework and system of MiniZinc, a variant of the well-known Zinc family of languages. The MininZinc approach to data mining provides a novel perspective for data mining in which the model is separated from the solver. Promising results are obtained and some implications for data mining as well as challenges for constraint programming are discussed."934,Using Double-oracle Method and Serialized Alpha-Beta Search for Pruning in Simultaneous Moves Games,"We focus on solving two-player zero-sum extensive-form games with perfect information and simultaneous moves. In these games, both players fully observe the current state of the game where they simultaneously make a move determining the next state of the game. We solve these games by a novel algorithm that relies on two factors: (1) it iteratively solves the games that correspond to a single simultaneous move by a double-oracle approach, and (2) it prunes the states of the game using bounds on the sub-game values obtained by classical Alpha-Beta search on a serialized variant of the game. We experimentally evaluate our algorithm on card game Goofspiel, a pursuit-evasion game, and randomly generated games. The results show that our novel algorithm provides significant running-time improvements and reduction in the number of evaluated nodes compared to the full search algorithm."936,Symbolic Merge-and-Shrink for Cost-Optimal Planning,"Merge-and-Shrink (M&amp;S) and Symbolic search are two related approaches to derive admissible heuristics for optimal planning. We present a new combination of these techniques, Symbolic Merge-and-Shrink (SM&amp;S) which uses M&amp;S abstractions as a relaxation criteria for a symbolic backward search. Empirical evaluation shows that SM&amp;S combines the strengths of both techniques deriving heuristics at least as good as the best of them for most domains."937,Meta-interpretive Learning of Higher-Order Dyadic Datalog: Predicate Invention revisited,"Within recent years the topic of Predicate Invention
has been under-explored within Inductive Logic Programming
due to difficulties in formulating efficient search mechanisms.
However, in a recent paper it was demonstrated that both
predicate invention and the learning of recursion
can be efficiently implemented for regular and context-free grammars,
by way of higher-order abduction with respect to a meta-interpreter.
New predicate symbols are introduced as
constants representing existentially quantified higher-order variables.
In this paper we generalise the approach of Meta-Interpretive
Learning to that of learning Higher-Order Dyadic Datalog
programs. We show that with an infinite signature
the Higher-order Dyadic Datalog class $H^2_2$ has universal Turing
expressivity though $H^2_2$ is decidable given a finite signature.
Our experiments indicate that the use of a Knuth-Bendix style total
ordering over the Herbrand Base together with logarithmic clause bounding
allows our implementation, $\\mbox{Metagol}_D$,
to efficiently learn compact $H^2_2$ definitions in challenging
application areas including the NELL language learning domain
and robot plan learning.  In conclusion we discuss possible extensions
of this work, including the use of a less restrictive hypothesis class
and the incorporation of probabilities into the representation.
"953,A Cutoff Technique for the Verification of Parameterised IS with Parameterised Environments,"While considerable progress has been made in the area of verification of multi-agent systems, most model checking techniques currently in use do not support open multi-agent systems where the number of components is unbounded. We put forward a cutoff technique for determining the number of agents that is sufficient to consider when checking temporal-epistemic specifications on a system of any size. We identify a special class of interleaved interpreted systems for which we give a parameterised semantics and an abstraction methodology. This enables us to overcome the significant limitations in expressivity present in the state-of-the-art. 
We present an implementation and discuss experimental results.
"955,A KNN Based Kalman Filter Gaussian Process Regression,"The standard Gaussian process (GP) regression is often intractable when the data set is large or reflects strongly spatial nonstationarity. In this paper, we address these challenges by designing a novel K nearest neighbor based Kalman filter Gaussian process (KNN-KFGP) regression. Based on a state space model established by the KNN driven data grouping, our KNN-KFGP recursively filters out the latent function values in a computationally efficient and accurate Kalman filtering framework. Moreover, KNN allows each test point to find its strongly correlated local training subset, so our KNN-KFGP provides a suitable way to deal with spatial nonstationary problems. We evaluate the performance of our KNN-KFGP on several synthetic and real data sets to show its validity."958,Target-Value Search Revisited,"This paper addresses the target valued search (TVS) problem, which is the
problem of finding a path between two nodes in a graph whose cost is as close as
possible to a given target value, T. This problem has been previously addressed 
only in directed acyclic graphs. In this work we develop the theory required to 
solve this problem optimally for any type of graph. We adapt traditional heuristic 
search algorithms for this setting, and propose a novel bidirectional search 
algorithm that is specifically suited for TVS. The benefits of this bidirectional 
search algorithm are discussed both theoretically and experimentally on several 
domains. "959,Interpolative reasoning with default rules,"Default reasoning and interpolation are two important forms of commonsense rule-based reasoning. The former allows us to draw conclusions from incompletely specified states, by making assumptions on normality, whereas the latter allows us to draw conclusions from states that are not explicitly covered by any of the available rules. Although both approaches have received considerable attention in the literature, it is at present not well understood how they can be combined to draw reasonable conclusions from incompletely specified states and incomplete rule bases. In this paper, we introduce an inference system for interpolating default rules, based on a geometric semantics in which normality is related to spatial density and interpolation is related to geometric betweenness. In particular, we view default rules and information on the betweenness of natural categories as particular types of constraints on qualitative representations of G&auml;rdenfors conceptual spaces. We propose an axiomatization, extending the well-known System P, and show its soundness and completeness w.r.t. the proposed semantics. Subsequently, we analyze how our extension of preferential reasoning can be further refined by adapting two classical approaches for handling the irrelevance problem in default reasoning: rational closure and conditional entailment."964,Topic Extraction from Online Reviews for Classification and Recommendation,"With the rapid growth of online reviews and popular products often attract hundreds or even thousands of user reviews. Consequently automatically identifying useful, informative and helpful reviews is important. In this paper, we describe and evaluate techniques for identifying and recommending helpful product reviews by using a combination of review features including topical and sentiment information mined from a review corpus."965,Probabilistic Equivalence Verification Approach for Automatic Mathematical Solution Assessment,"Assessing the knowledge proficiency of learners is important to provide relevant feedback
and suggestion to enhance their learning process. However, most of the current e-learning systems support automatic assessment using multiple choice questions only as it is a great challenge to support other question formats. For mathematical solution assessment, it checks the equivalence of mathematical expressions in the user answer and standard solution. It is particularly difficult for mathematical solution assessment as the semantics of mathematical expressions are highly symbolic and equivalent mathematical expressions can be expressed in different forms. Computer Algebra Systems such as Maple and Mathematica provide the mathematical equivalence checking function. However, these systems have generated far too many false negative errors if the mathematical expressions are written in a different form from the standard solution form. In this research, we propose an effective Probabilistic Equivalence Verification (PEV) approach for automatic mathematical solution assessment. The proposed PEV approach is a randomized method based on the probabilistic numerical equivalence testing of two mathematical expressions. It can avoid false negative errors completely while guaranteeing a small probability of false positive errors to occur. The performance results have shown that the proposed PEV approach has outperformed other techniques. The proposed PEV approach can achieve 100% in precision while Maple and Mathematica can only achieve about 65% and 58% respectively."966,Computing Datalog Rewritings Beyond Horn Ontologies,"Query answering over large data sets structured using an
OWL 2 ontology is an important reasoning problem, 
albeit computationally hard. Efficient algorithms ensuring
tractability in data complexity have so
far been developed for Horn fragments of OWL. These fragments, however,
cannot capture disjunctive knowledge and are thus limited in practice.
In this paper, we present novel techniques
for answering queries over non-Horn ontologies via rewriting
into datalog.
Towards this goal, we  explore the limitations of datalog
rewritability and formulate
non-rewritability results that hold independently of 
any complexity-theoretic assumptions.
Then, we present a (possibly nonterminating) rewriting procedure 
based on resolution and show that it   
computes a datalog rewriting for the given input whenever it terminates. 
We show termination for
ontologies in DL-Lite_{Bool}^H,+: the extension of the logic underlying the
QL profile of OWL 2 with disjunction, negation, and transitivity axioms.
Ours is the first goal-oriented algorithm for a non-Horn fragment of OWL 2
ensuring tractability in data complexity."970,Learning Topical Translation Model for Microblog Hashtag Suggestion,"In microblogs, many tweets are marked with hashtags, which usually represent groups or topics of posts.  Hashtags may provide valuable information for lots of applications, such as retrieval, opinion mining, classification, and so on. To ease the annotation process, in this paper, we propose a topical translation model for microblog hashtag suggestion. Our model assumed the content and hashtags of the tweet are talking about the same themes but written in different languages. Under the assumption, hashtag suggestion is modeled as a translation process from content to hashtags. Moreover, in order to cover the topic of tweet, our model regard the translation probability to be topic-specific. On one hand, our model uses topic-specific word trigger to bridge the vocabulary gap between the words in tweets and hashtags. On the other hand, it can discover the topics of tweets by a topic model designed for microblogs.
 Experimental result on dataset crawled from real world microblogging service demonstrates that the proposed method can outperform some state-of-the-art methods and two degenerate variations of our model."972,Detecting and Exploiting Subproblem Tractability,"Constraint satisfaction problems may be nearly tractable. For instance, when the set of relations of a problem is a known tractable language with a few extra relations outside of this language. However, such an observation is in general of little use. 

  In this paper we introduce a method to take advantage of the fact that removing only a few constraints yields a tractable subproblem. This method can be viewed as computing a backdoor and can be applied to many tractable classes, providing that membership test for the class is itself tractable.
  
  We introduce two polynomial detection algorithms, to check if a language is closed under a majority or Mal'tsev polymorphism, respectively. Then we show that computing a minimal backdoor for such classes is fixed parameter tractable (FPT) if the tractable subset of relations is given, and W[2]-complete otherwise. Finally, we report preliminary empirical results showing that some (although, very few) problems in the XCSP repository are nearly closed under a majority polymorphism and small backdoors can be computed for them."973,Most Specific Generalizations w.r.t. General EL-TBoxes,"In the area of Description Logics the least common subsumer (lcs) and the most specific concept (msc) are inferences that generalize a set of concepts or an individual, respectively, into a single concept. If computed w.r.t. a general EL-TBox neither the lcs nor the msc need to exist. So far in this setting no exact conditions for the existence of lcs- or  msc-concepts are known. This paper provides necessary and suffcient conditions for the existence of these two kinds of concepts. For the lcs of a fixed number of concepts and the msc we show decidability of the existence in PTime and polynomial bounds on the maximal role-depth of the lcs- and msc-concepts. The latter allows to compute the lcs and the msc, respectively.
"975,Active Learning for Level Set Estimation,"In many applications, rather than estimating the value of some unknown function over its entire domain, it is of interest to accurately determine the set of points, for which the function value lies above or below some given threshold level. We formalize this task as a classification problem with sequential measurements, where the unknown function is sampled from a Gaussian process (GP). We propose LSE, an algorithm that guides both sampling and classification based on GP-derived confidence bounds, and provide theoretical guarantees about the number of samples required to achieve a certain classification accuracy. Furthermore, we extend the algorithm and its theory to two more settings that naturally arise in practice: the setting where the threshold level is implicitly defined as a percentage of the (unknown) maximum of the target function and the setting where samples are selected in batches. We evaluate the effectiveness of our proposed methods on two problems of practical interest, namely monitoring the population of algae in a lake environment and geolocating network latency."979,A Scalable Approach to Column-Based Low-Rank Matrix Approximation,"In this paper, we address the column-based low-rank matrix approximation problem through the use of a novel parallel approach. Our approach is based on the divide-and-combine method. We first perform column selection on submatrices of the input matrix in parallel, and then combine the selected  columns into the final output. Our approach enjoys a theoretical relative-error upper bound.  In addition, our column-based low-rank approximation partitions data in a deterministic way and makes no assumptions about matrix coherence. Compared with other traditional methods, our approach is scalable on large-scale matrices. Finally,  experiments on both simulated and real world data show that our approach is both efficient and effective."981,Towards Understanding Global Spread of Disease from Everyday Interpersonal Interactions,"Monitoring and forecast of global spread of infectious diseases is difficult, mainly due to lack of fine-grained and timely data. Previous work in computational epidemiology has shown that mining data from the web can improve the predictability of high-level aggregate patterns of epidemics. By contrast, this paper explores how individuals \emph{contribute} to the global spread of disease. We consider the important task of predicting the prevalence of flu-like diseases in a given city based on interpersonal interactions of the city's residents with the outside world. We use the geo-tagged status updates of traveling Twitter users to infer properties of the flow of individuals between cities.  While previous research considered only the volume of passenger flow, by data mining the Twitter stream we can estimate a number of latent variables, including the number of sick (symptomatic) travelers and the number of sick individuals to whom each traveler was exposed. Our experiments involve over 51,000 individuals traveling between 75 cities before and during a severe ongoing flu epidemic (November 2012 - January 2013). We show that 73\% of variance in local flu indices of individual cities is explained by tracking infected travelers---a significant improvement over baseline alternatives and past results. Our model leverages the text and interpersonal interactions recorded in over 6.5 million online status updates without any active user participation, enabling scalable public health applications."984,Unlearning from Demonstration,"When doing learning from demonstration, it is often the case that the demonstrator provides corrective examples to fix errant behavior by the agent or robot.  We present a set of algorithms which use this corrective data to identify and remove noisy samples in datasets which caused errant classifications, and ultimately errant behavior.  The objective is to actually modify the source datasets rather than solely rely on the noise-insensitivity of the classification algorithm.  This is particularly useful in the sparse datasets often found in learning from demonstration experiments.  Our approach tries to distinguish between noisy misclassification and mere under-samping of the learning space.  If errors are a result of misclassification, we potentially remove misclassifying points and update the classifier.  We demonstrate our method on a sparse learning from demonstration problem, and on several UCI Machine Learning datasets, using decision trees, K-Nearest-Neighbor, and support vector machines."987,Tractable Queries for Lightweight Description Logics,"It is a classic result in database theory that conjunctive query answering, which is NP-complete in general, is feasible in polynomial time when restricted to acyclic queries. Subsequent results identified more general structural properties of conjunctive queries (like bounded treewidth) which ensure tractable query evaluation. In this paper, we lift these tractability results to knowledge bases formulated in the  lightweight description logics DL-Lite and EL. The proof exploits known properties of query matches in these logics and involves a query-dependent modification of the data. To obtain a more practical approach, we also propose a concrete polynomial-time algorithm for answering bounded treewidth conjunctive queries over DL-Lite knowledge bases, based on a rewriting into non-recursive datalog programs with rules of small arity. A preliminary evaluation of our proof-of-concept implementation suggests the interest of our approach for handling large acyclic conjunctive queries."998,On Robust Estimation of High Dimensional Generalized Linear Models,"We study \emph{robust high-dimensional estimation} of generalized linear models (GLMs); where a small number $k$ of the $n$ observations can be arbitrarily corrupted, and where the true parameter is high dimensional in the ``$p \gg n$'' regime, but only has a small number $s$ of non-zero entries. There has been some recent work connecting robustness and sparsity, in the context of linear regression with corrupted observations, by using an explicitly modeled outlier response vector that is assumed to be sparse. Interestingly, we show, in the GLM setting, such explicit outlier response modeling can be performed in \emph{two distinct ways}. For each of these two approaches, we give $\ell_2$ error bounds for parameter estimation for general values of the tuple $(n,p,s,k)$. The first approach, based on modeling gross errors in the parameter space, leads to a convex problem but requires more stringent conditions for the bounds to hold. The second approach, based on modeling gross errors in the output space, requires more benign conditions but leads to a non-convex problem in general. However, it turns out that non-convexity is not a big impediment and a simple projected gradient method converges to one of the global optima up to statistical precision."1002,Compact Rewritings for Existential Rules,"Querying large databases while taking ontologies into account is currently a very active domain research. In this paper, we consider ontologies described by existential rules (also known as Datalog+/-), a framework that generalizes lightweight description logics. A common approach is to rewrite a conjunctive query w.r.t the ontology into a union of conjunctive queries (UCQ) which can be directly evaluated against a database. However, the practicability of this approach is questionable due to 1) the weak expressivity of classes for which efficient rewriters have been implemented 2) the large size of optimal rewritings using UCQ. We propose to use semi-conjunctive queries (SCQ) , which are a restricted form of positive existential formulas, to compute sound and complete rewritings, which are union of SCQ (USCQ). A novel algorithm for query rewriting, {\sc Compact}, is presented. It computes sound and complete rewritings for large classes of ontologies, which are shown to be smaller than the optimal UCQ rewritings. First experiments show that USCQ are both efficiently computable and more efficiently evaluable than their equivalent UCQ."1005,Conjunctive Regular Path Queries in Lightweight Description Logics,"Conjunctive regular path queries are an expressive extension of the well-known class of conjunctive queries and have been extensively studied in the database community. Somewhat surprisingly, there has been little work aimed at using such queries in the context of description logic (DL) knowledge bases, and all existing results target expressive DLs, even though lightweight DLs are considered better-suited for data-intensive applications.
This paper aims to bridge this gap by providing algorithms and tight complexity bounds for answering two-way conjunctive regular path queries over DL knowledge bases formulated in lightweight DLs of the EL and DL-Lite families. Our results demonstrate that it is possible to move to this richer query language without sacrificing polynomial data complexity, although the combined complexity rises to PSPACE-complete. For two-way regular path queries (without conjunction), query answering is tractable even with respect to combined complexity."1007,Sufficiency-Based Selection Strategy for MCTS,"Monte-Carlo Tree Search (MCTS) has proved a remarkably effective decision mechanism in many different game domains, including computer Go and general game playing (GGP).  However, in GGP, where many disparate games are played, certain type of games have proved to be particularly problematic for MCTS.  One of the problems are game trees with so-called optimistic moves, that is, bad moves that superficially look good but potentially require much simulation effort to prove otherwise.  Such scenarios can be difficult to identify in real time and can lead to suboptimal or even harmful decisions. In this paper we investigate a selection strategy for MCTS to alleviate this problem. The strategy, called sufficiency threshold, concentrates simulation effort better for resolving potential optimistic move scenarios.  The improved strategy is evaluated empirically in an n-arm-bandit test domain for highlighting its properties as well as in a state-of-the-art GGP agent to demonstrate its effectiveness in practice.  The new strategy shows significant improvements in both domains."1011,Coalitional Games via Network Flows,"We introduce a novel scheme for representing coalitional games, termed Coalition-Flow Networks (CF-NETs). Specifically, this representation is based on the observation that the coalition formation process can be viewed as the problem of directing the flow through a network where every edge has certain capacity constraints. We show that our new way of describing this process is intuitive, fully expressive, and allows for capturing certain patterns in a significantly more concise manner compared to the conventional approach. Furthermore, we show that CF-NETs have the flexibility to express various game classes, namely characteristic function games, coalitional games with overlapping coalitions, and coalitional games with agent types. Finally, we demonstrate the power of this  representation on the Coalition Structure Generation problem, by  showing that near-optimal solutions for large instance of the problem can be found in a matter of seconds."1024,Efficient Algorithm for the Minimal Labeling Problem of Temporal and Spatial Qualitative Constraints,"Spatial and temporal reasoning is a crucial task for certain Artificial Intelligence applications. In this context, various formalisms representing the information through qualitative constraint networks (QCN) have been proposed among which the Interval Algebra and the RCC8 calculus. In this paper we are concerned by  the minimal
labeling problem  of QCNs and propose an efficient algorithm aiming at caracterizing the feasible base relations of a QCN.
On one hand, this algorithm is based on triangulation of QCNs and a new partial consistency.
On an other hand, this algorithm uses tractable subclasses of relations having the patchork propery for which the property of $\diamond$-consistency
implies the consistency of the QCN.  The experimentation that we have conducted on $\QCN$s from the Interval Algebra and RCC-8 calculus shows the interest of these new algorithm."1028,Three generalizations of the Focus constraint,"The FOCUS constraint expresses the opposite notion than balancing. In practice, this constraint suffers from the rigidity of its semantics. To tackle this issue, we propose three constraints that generalize FOCUS. We provide for each one a complete filtering algorithm and we discuss decompositions."1029,Semi-Supervised Learning for Integration of Aerosol Predictions from Multiple Satellite Instruments,"Aerosol Optical Depth (AOD), recognized as one of the most important quantities in understanding and predicting the Earth's climate, is measured daily on a global scale by several Earth-observing satellites, each with different sensitivity to atmospheric conditions. Thus, the availability, as well as the quality of retrieved AOD from individual satellites varies across the globe. Moreover, ground-truth AOD, retrieved by ground-based AERONET instruments installed at several hundred worldwide locations, is scarce as the instrument is very sensitive to inclement weather, making missing data a problem inherent to the AOD estimation task. We present a method for aggregation of noisy experts' retrieval into a single, more accurate AOD retrieval, while accounting for missing predictions and unlabeled data. Further, as the quality of satellite retrievals depends on observed conditions, during training we also learn how to cluster the aerosol data. Each cluster captures satellite expertise for different observed conditions, resulting in a more accurate, interpretable model. We validated our method on a $5$-year aerosol data from $5$ satellites, showing the benefits of the proposed approach."1031,Learning Spatio-Temporal Features from RGB-D Video Data,"Recently, the low-cost Microsoft Kinect sensor,
which can capture real-time high-resolution RGB
and depth visual information, has attracted increasing
attentions for a wide range of applications
in computer vision. Existing techniques extract
hand-tuned features from the RGB and the depth
data separately and heuristically fuse them, which
would not fully exploit the complementarity of both
data sources. In this paper, we introduce an adaptive
learning methodology to automatically extract
spatio-temporal features, simultaneously fusing the
RGB and depth information, from RGB-D video
data for visual recognition tasks. We address this
as an optimization problem using our proposed restricted
graph-based genetic programming (RGGP)
approach, in which a group of primitive 3D operators
are first randomly assembled as graph-based
combinations and then evolved generation by generation
by evaluating on a set of RGB-D video samples.
Finally the best-performed combination is
selected as the (near-)optimal program for a predefined
task.
We systematically assess the proposed method on
a new hand gesture dataset, SCORD, which we
collected ourselves. Extensive experimental results
show that our approach leads to significant advantages
compared with state-of-the-art hand-crafted
and machine-learned features."1038,Statistical tests for the detection of the arrow of time in vector autoregressive models,"The problem of detecting the direction of time in vector Autoregressive (VAR) processes using statistical techniques is considered.  By analogy to causal AR(1) processes with non-Gaussian noise, we conjecture that the distribution of the time reversed residuals of a linear VAR model is closer to a Gaussian than the distribution of the residuals of a linear fit in the forward direction.  Extensive experiments with simulated data provide strong empirical evidence for the validity of the conjecture of Gaussianization of the residuals of a linear VAR model upon time-reversal of the series."1040,A Generalization of SAT and #SAT for Policy Evaluation,"Both SAT and #SAT are languages capable of representing many difficult problems in a number of seemingly dissimilar areas such as planning, verification, and probabilistic inference. In this paper, we examine an expressive new language called #&amp;#8707;SAT that generalizes both of these languages. #&amp;#8707;SAT can succinctly represent an interesting class of problems that require counting the number of satisfiable formulas in a concisely-describable set of existentially-quantified, propositional formulas. These problems include, for example, many natural questions about the optimal value of policies under a particular kind of uncertainty.

We characterize the expressiveness and worst-case difficulty of #&amp;#8707;SAT by proving that it is complete for a complexity class called #P^NP[1] , and relating this class to more familiar complexity classes. We also provide a thorough empirical analysis of four new, general-purpose, #&amp;#8707;SAT solvers on a battery of problems distributions including a simple logistics domain. Our experiments show that despite the formidable worst-case complexity of the #PNP[1] class, many of the problem instances that we examine can be solved efficiently by noticing and exploiting a particular type of structure that occurs frequently in our problem distributions."1041,The Impact of Disjunction on Query Answering Under Guarded-based Existential Rules,"We study the complexity of conjunctive query answering under guarded-based disjunctive existential rules, i.e., existential rules extended with disjunction in rule-heads. We focus our attention on (weakly-)(frontier-)guarded rules, and their main subclasses, i.e., linear rules and inclusion dependencies (IDs). Our main result states that conjunctive query answering under a fixed set of disjunctive IDs is 2EXPTIME-hard. This surprising result together with a 2EXPTIME upper bound for weakly-frontier-guarded disjunctive rules, obtained by exploiting results on the satisfiability of guarded negation first-order logic, gives us a complete picture of the computational complexity of our problem. We also study two natural subclasses of disjunctive IDs, namely frontier-one (i.e., only one variable is propagated), and full-identity (i.e., we are only allowed to copy a tuple), for which the combined complexity decreases to EXPTIME and coNP, respectively."1051,Towards a second generation random walk planner: an experimental exploration,"Random walks have become a popular component of recent planning systems. The increased exploration is a valuable addition to more exploitative search methods such as Greedy Best First Search (GBFS). A number of successful planners which incorporate random walks have been built. The work presented here aims to exploit the experience gained from building those systems. It begins a systematic study of the design space and alternative choices for building such a system, and develops a new random walk planner from scratch, with careful experiments along the way. The major insights gained are that first, a high state evaluation frequency is usually superior to the endpoint-only evaluation used in earlier systems, second, adjusting the restarting parameter according to the progress speed in the search space performs better than any fixed setting, third, biasing the action selection towards preferred operators of only the current state is better than Monte Carlo Helpful Actions that also considers the number of times an action has been a preferred operator in previous walks, and forth, even very simple forms of random walk planning can compete with GBFS."1052,Taxi and Car sharing Online: A Framework and Heuristics for the Optimization Problem,"	In this paper, we study a dynamic problem of car and taxi sharing with time windows. We consider a scenario where people needing a car or interesting
	in get a ride, uses a phone app to designate their source and destination points in a city, as well other restrictions (such as maximum allowable time
	to be at the destination). On the other hand we have taxis and people interesting in giving a ride, with their current positions and other restrictions as well (such
	as vehicle capacity, destination, maximum time to destination). We consider the problem of maximizing the number of shared trips: in the case of taxis,
	people going to close locations can share the costs of the trip, and in case of car rides the driver and passengers can share costs as well. This problem is
	dynamic, since new call for taxis or call for rides arrive on demand. This gives rise to an interesting optimization problem which we prove to be NP-Hard.
	We then propose heuristics to deal with the problem.
	 In this paper we focus on the taxi sharing problem, but we show that our model is easily  extendable to model the  ride-sharing situation or even a situation
	  where there are both taxis and car owners who want to share their vehicles to reduce costs, and in both cases, we take in account the shared costs.
	In addition, we propose a framework which consists basically of a cell app and a server that process all incoming information in order to match routes that can reduce costs
	of taxis users, by doing taxi sharing and splitting the trip costs, and by matching users interest to sell trips with users wanting to buy a cheap trip.  
	The entire system can be used by taxi companies or people interesting in shared trips in a way to reduce the traffic in the cities and to reduce
	the emission of greenhouse gases."1055,Implicit learning of common sense for reasoning,"We consider the problem of how enormous databases of ""common sense"" knowledge can be both learned and utilized in reasoning in a computationally efficient manner. We propose that this is possible if the learning only occurs implicitly, i.e., without generating an explicit representation. We show that it is feasible to invoke such implicitly learned knowledge in essentially all natural tractable reasoning problems. This implicit learning also turns out to be provably robust to occasional counterexamples, as appropriate for such common sense knowledge."1059,Graph Classification with Imbalanced Class Distributions and Noise,"Recent years have witnessed an increasing applications where data reflects structural dependency and can be modeled as graphs. In many graph applications, class distribution is inherently imbalanced, which imposed great challenges to the classification task. This problem would be further complicated when noise or outliers presents in the graph datasets. In this paper, we proposed a linear programming boosting algorithm, igBoost, that progressively selects informative subgraph patterns from the imbalanced graph data directly.
To handle the imbalance and noise presenting in graph data, we assign different costs to individual examples that considers not only the between class imbalance but also within class importance. The instance costs are integrated into the feature selection procedure and affects the maximum margins during the learning process. Experiments on real-life graph datasets with different degrees of imbalance and noise demonstrates the effectiveness of our algorithm."1064,Macau: A Basis for Evaluating Reputation Systems,"Reputation is a crucial concept in dynamic multiagent environments. Despite the large body of work on reputation systems, no metrics exist to directly and quantitatively evaluate and compare them.  We present a common conceptual interface for reputation systems and a set of four measurable desiderata that are broadly applicable across multiple domains.  These desiderata employ concepts from dynamic systems theory to measure how a reputation system reacts to a strategic agent attempting to maximize its own utility.  We study a diverse set of well-known reputation models from the literature in a moral hazard setting and identify a rich variety of characteristics that they support.  We discuss the implications, strengths, and limitations of our desiderata."1066,Getting the Most Out of Pattern Databases for Classical Planning,"The iPDB procedure by Haslum et al. is the state-of-the-art method for computing additive abstraction heuristics for domain-independent planning. It performs a hill-climbing search in the space of pattern collections, combining information from multiple patterns in the so-called canonical heuristic.
  
We present three improvements over iPDB. First, we speed up the canonical heuristic evaluation by eliminating redundant computations. Second, we show how stronger heuristic estimates can be obtained through linear programming. Third, we propose a different approach for selecting pattern collections for PDB-based heuristics. An experimental evaluation demonstrates the strength of these techniques on the IPC benchmark suite."1071,A Bayesian Factorised Covariance Model for Image Analysis,"Abstract This paper presents a specialised Bayesian model for analysing the covariance of data that are observed in the form of matrices, which is particularly suitable for images. Compared to existing general-purpose covariance learning techniques, we exploit the fact that the variables are organised as an array with two sets of ordered indexes, which induces innate relationship between the variables. Specifically, we adopt a factorised structure for the covariance matrix. The covariance of two variables is represented by the product of the covariance of the two corresponding rows and that of the two columns. The factors, i.e. the row-wise and column-wise covariance matrices are estimated by Bayesian inference with sparsity encouraging priors. 

Abstract Empirical study has been conducted on image analysis. The model first learns correlations between the rows and columns in an image plane. Then the correlations between individual pixels can be inferred by their locations. This scheme utilises the structural information of an image, and benefits the analysis when the data are damaged or insufficient."1076,Computational Analysis of Connectivity Games with Applications to Terrorist Networks,"  We study a recently developed centrality metric to identify key  players in terrorist organisations due to Lindelauf et al. [2011]. This metric, which involves computation of the Shapley value for \\textit{connectivity games on graphs} proposed by Amer and Gimenez [2004], was shown to produce substantially better results than previously used centrality measures. In this paper, we present the first computational analysis of this class of coalitional games, and propose two algorithms for computing Lindelauf et al.\'s centrality metric. Our first   algorithm is exact, and runs in time linear in the number of  connected subgraphs in the network. Our simulations demonstrate that the algorithm identifies key players in the WTC 9/11 terrorist network, (containing 36 members and 125 links), in less than 40 minutes. In contrast, a general-purpose Shapley value algorithm would require weeks to solve this problem. Our second algorithm is   approximate and can be successfully used to study much larger networks."1077,FQHT: the logic of stable models for logic programs with intensional functions,"Based on ideas from non monotonic causal logic, Vladimir Lifschitz (2012) recently proposed a mechanism for defining intensional functions within the stable model semantics for logic programs. A second, revised approach to defining intensional functions has been suggested by Bartholemew and Lee (2012). In this paper we study a logical system that is appropriate for reasoning about logic programs with intensional functions as treated in the approach of Bartholemew and Lee. It turns out that the system is closely related to the quantified logic of here-and-there, QHT, that provides a foundation for ordinary logic programs under the answer set semantics. The paper proposes an axiomatic system, FQHT, a Gentzen style proof theory and establishes completeness results. The adequacy of the approach is demonstrated by showing that it captures the Bartholemew/Lee semantics and satisfies a strong equivalence property.
"1079,Fair LTL Synthesis for Non-Deterministic Systems using Strong Cyclic Planners,"We consider the problem of planning in environments where the state is fully observable, actions may have non-deterministic effects, and plans must generate infinite state trajectories for achieving a large class of LTL goals. Formally, this is the control synthesis problem under the assumption that the LTL formula to be realized can be mapped into a deterministic Buchi automaton. We show then that by assuming that action non-determinism is fair, namely that infinite executions of a non-deterministic action in the same state yield each possible successor state an infinite number of times, the (fair) synthesis problem can be reduced to a standard strong cyclic planning task over reachability goals. Since strong cyclic planners are built on top of efficient classical planners, the transformation reduces the non-deterministic, fully observable, extended planning task into the solution of classical planning problems. A number of experiments are reported showing the potential benefits of this approach to synthesis, in comparison with state-of-the-art symbolic methods."1086,Action Language BC: Preliminary Report,"The action description languages B and C have significant common
core. Nevertheless, some expressive possibilities of B are difficult
or impossible to simulate in C, and the other way around.  The main
advantage of B is that it allows the user to give Prolog-style
recursive definitions, which is important in applications.  On the
other hand, B solves the frame problem by incorporating the commonsense
law of inertia in its semantics, which makes it difficult to talk
about fluents whose behavior is described by defaults other than
inertia.  In C and in its extension C+, the inertia assumption is
expressed by axioms that the user is free to include or not to include.
This paper defines a new action description language, called BC,
that combines the attractive features of B and C+.  Examples of
formalizing commonsense domains discussed in the paper illustrate the
expressive capabilities of BC and the use of answer set solvers for
the automation of reasoning about actions described in this language."1092,Value of Information with Streaming Evidence,"A key question facing autonomous systems that perform continual sensing is whether to act based on current evidence or wait for additional evidence that could potentially improve the action selection at the cost of delay. In this paper we address the challenge of computing the expected value of information in settings with streaming, high-dimensional sensory evidence. We propose an approach based on belief projection models, i.e. direct conditional models that predict beliefs over future states given the recent history of streaming evidence. These models can be trained from data in a self-supervised fashion and fit naturally in modular, hierarchical state inference architectures. We demonstrate a practical implementation of this methodology in a situated conversational agent. The approach enables the agent to proactively engage people in its vicinity. We illustrate the operation of the method via runtime traces."1097,Scaling Up Joint Activity Based Security Games,"Despite recent successful real-world deployments of Stackelberg Security Games (SSGs), scale-up remains a fundamental challenge to advancements in this field. 
The latest techniques do not scale-up to domains where multiple defenders must coordinate time dependent joint activities.
To address this challenge, this paper presents two branch-and-price algorithms, SmartO and SmartH, with three novel features: (i) a column-generation approach that uses an ordered network of nodes (determined by solving the traveling salesman problem) to generate individual defender strategies; (ii) exploitation of iterative mutual reward shaping of multiple coordinating defender units to generate coordinated strategies; (iii) generating tighter upper-bounds for pruning by solving security games that only abide by key scheduling constraints. 
We provide extensive experimental results using analyses from a maritime security domain."1098,The Inclusion-Exclusion Rule and its Application to the Junction Tree Algorithm,"In this paper, we consider the inclusion-exclusion rule -- a known yet forgotten rule of probabilistic inference. Unlike the widely used sum rule of probabilistic inference which requires easy access to all joint probability values (i.e., a value proportional to the probability of the assignment of values to all variables), the inclusion-exclusion rule requires easy access to several marginal probability values (i.e., the probability of the assignment of values to a subset of variables). We therefore develop a new representation of the joint distribution that is amenable to the inclusion-exclusion rule. We compare the relative strengths and weaknesses of the inclusion-exclusion rule with the sum rule and develop a hybrid rule called the inclusion-exclusion-sum (IES) rule, which combines their power. We apply the IES rule to junction trees, treating the latter as a target for knowledge compilation and show that it greatly reduces query complexity (online step) in some cases at increased compilation cost (offline step). Our experiments demonstrate the power of our new approach. In particular, at query time, our new scheme was an order of magnitude faster than the junction tree algorithm on several benchmark probabilistic graphical models."1099,Continuously Relaxing Over-constrained Conditional Temporal Problems,"Over-constrained temporal problems are commonly encountered while operating autonomous and decision support systems. Many systems may unilaterally apply preferences and suspend constraints to resolve the problem, or simply signal a failure and wait for human assistance. A better solution could result from a system that learns the human preferences over the problem to generate resolutions that minimize perturbations. We present the Best-first Conflict-directed Relaxation (BCDR) algorithm for enumerating the best continuous relaxation for an over-constrained Conditional Temporal Problem (CTP) with controllable discrete choices. It reformulates such a CTP by replacing its relaxable constraints with fuzzy temporal constraints and solves the problem using a conflict-directed approach. Our approach views relaxation as an instance of multiple fault diagnosis applied to temporal problems: it identifies conflicts and resolves them by relaxing fuzzy temporal constraints. BCDR extends traditional Con-flict-directed A* (CD-A*) algorithm, by first general-izing the conflict learning process to include all discrete variable assignments and continuous temporal constraints, and then by resolving these conflicts to guide the forward search away from known infeasible regions. Evaluated empirically on a range of test prob-lems of coordinated car sharing networks, BCDR demonstrates a substantial improvement in perfor-mance compared to a traditional CD-A* algorithm and in solution quality compared to discrete relaxation algorithms."1106,Syntactic Computation of Hybrid Possibilistic Conditioning under Uncertain Inputs,"Recently, a new form of possibilistic conditioning, called hybrid possibilistic conditioning, has been proposed for revising possibility distributions.  It  has only been defined at the semantics level and allows the combination of different standard conditioning operators on models and countermodels of some formulas.  

This paper first proposes an extension of hybrid possibilistic conditioning where the input is no longer a single weighted formula but a set of triplets $\\mu=\\{(\\lambda_i, a_i, \\Diamond_i), i=1,\\ldots,n\\}$. The propositional formulas $\\lambda_i$ induce a partition of a set of interpretations and  $\\Diamond_i$ indicates the way in which models of $\\lambda_i$ should be revised in order to have $\\lambda_i$ accepted to a level $a_i$. 

Then it provides a characterization of hybrid possibilistic conditioning using elementary operations on possibility distributions such as : $a$-constant possibility distributions, unnormalized conditioning, $a$-discountings and $a$-enhancements.  

Lastly, it tackles a difficult issue that concerns the computation of the revision of possibilistic knowledge bases, made of weighted formulas, using hybrid conditioning. 
We show that our syntactic computations of the revision of possibilistic bases are in full agreement with the definiton of the semantics of hybrid possibilistic conditioning. An important result is that there is no extra computational cost in using hybrid possibilistic conditioning and in particular the size of the revised possibilistic base is polynomial with respect to the size of the initial base and the input."1107,Maximizing Flexibility in Simple Temporal Networks,"Scheduling problems occur in many application domains. A scheduling problem arises if we have a set of temporal events whose starting times are subject to temporal constraints, and we have to set these starting times in such a way that all constraints are satisfied. In many cases, disturbances are bound to occur during schedule execution. It such situations, it makes more sense to assign a starting-time interval, rather than a fixed starting time, to each temporal event. During execution, we can then pick starting times from these intervals adaptively, based on any outside disturbances that may occur. We define the `flexibility' we have in executing an event as the length of the interval assigned to the event, and the flexibility of an interval schedule as the sum of the flexibilities of all events. Our goal is to find schedules with maximum flexibility.

We want the intervals assigned to events to be independent, so that the starting time we choose from the interval of any one event does not affect the possibilities we have for other events. This requirement is also motivated by another scenario where it makes sense to assign intervals to events, namely when the scheduling problem is distributed across multiple agents. A popular approach to solving the resulting coordination problem is to `temporally decouple' the agents, by adding constraints to the agents' individual problems, such that any schedule they come up with that satisfies their local constraints, can be merged with other agents' schedules to yield a feasible schedule for the overall problem. The problem here is that the added constraints usually decrease the available flexibility.

In this paper, we focus on the Simple Temporal Network formalism for representing temporal scheduling problems. We show that the problem of assigning intervals to temporal events in an STN, in such a way that the sum of the lengths of the intervals is maximized, can be solved in polynomial time. Any STN S can be transformed into an STN S' in which dependencies between events are resolved. We show that in such an STN S', a maximum flexibility assignment of intervals can be found efficiently, and can serve as the basis for solving the problem in the original STN S. In addition, and surprisingly, we show that once a maximum flexibility assignment has been found, we can efficiently find a decoupling that does not decrease flexibility."1108,Domain Adaptation with Topical Correspondence Learning,"A serious and ubiquitous issue in machine learning is the lack of sufficient training data in a domain of interest. Domain adaptation is an effective approach to dealing with this problem by transferring information or model learned from distinct, albeit related, domains to the target domain. We develop a novel domain adaptation method for text document classification under the framework of Non-negative Matrix Factorization. Two key ideas of our method are to construct a latent topic space where a topic is decomposed into common words shared by all domains and words specific to individual domains, and then to establish associations between words in different domains through the common words as a bridge for knowledge transfer. The correspondence between cross-domain topics leads to more coherent distributions of source and target domains in the new representation while preserving the predictive power.  Our new method outperformed several state-of-the-art domain adaptation methods on several benchmark datasets."1113,Scaling-up Security Games with Boundedly Rational Adversaries: A Cutting-plane Approach,"To improve the current real-world deployments of Stackelberg security games (SSGs) it is critical now to efficiently incorporate models of adversary bounded rationality in large-scale SSGs. Branch-and-price was previously proposed to scale-up SSGs; but as we show with a realization called COCOMO, this approach falls short given the non-convexity of these complex adversary models. We therefore present as our main contribution a novel cutting-plane algorithm called BLADE to scale-up SSGs with complex adversary models. Fundamental novelties of BLADE include: (i) an efficient scalable separation oracle to generate deep cuts; (ii) a tunable heuristic that uses gradient descent to further improve the cuts; (iii) an approximation heuristic for quality-efficiency tradeoff."1115,Sequence of Mechanisms for Causal Reasoning in Artificial Intelligence,"We present a new approach to causal reasoning that we call {\em
Sequence Of Mechanisms (SoM)}, which treats causality as a dynamic
sequence of active mechanisms that chain together to propagate causal
influence through time. We motivate this approach using examples from
AI and robotics and show why existing approaches are inadequate for
this. We present the first algorithm for causal reasoning based on
SoM, which takes as input a database of mechanisms, and it
hypothesizes which mechanisms were active at what time. We show empirically that our algorithm produces plausible causal explanations of simulated observations generated from a causal model. 

We argue that the SoM approach is qualitatively closer to the human
causal reasoning process, for example, it will only include relevant
variables in an explanation, and we present new insights about causal
reasoning that become apparent with this view. One such insight is the
identification of the principle of {\em Observation-Manipulation
Commutability} (OMC) which we show to be a generalization of the
Equilibration-Manipulation Commutability of Dash, 2005, and which
we show does not hold in temporal causal models generally."1117,Sufficient Plan-Time Statistics for Decentralized POMDPs,"Optimal decentralized decision making in a team of cooperative agents as formalized in the framework of Decentralized POMDPs is a notoriously hard problem. A major obstacle is that the agents do not have access to a sufficient statistics during execution, which means that agents need to base their actions on their histories of observations. A consequence is that even during off-line planning the choice of decision rules for different stages is tightly interwoven: decisions of earlier stages affect how to act optimally at later stages, and the optimal value function for a stage is known to have a dependence on the decisions made up to that point. This paper makes a contribution to the theory of decentralized POMDPs by showing how this dependence on the `past joint policyâ€™ can be replaced by a probability distribution over histories and potentially states.  That is, it introduces sufficient statistics for the past joint policy during the optimal planning process. These results are extended to the case of k-steps delayed communication. We investigate the practical implications in a number of benchmark problems and discuss future avenues of research opened by these contributions."1121,Answer Set Programming Modulo Theories and Reasoning about Continuous Changes,"We propose a framework of tight integration of answer set programming (ASP) and satisfiability modulo theories (SMT), which we call ""Answer Set Programming Modulo Theories (ASPMT)."" Similar to the relationship between first-order logic and SMT, it is based on a recent proposal of functional stable model semantics by fixing the interpretation of background theories. Analogously to known relationship between ASP and SAT, we show that ""tight"" ASPMT programs can be turned into SMT instances. We demonstrate the usefulness of ASPMT by reformulating and extending action language C+ in terms of ASPMT, and use SMT solvers to compute the language. The extended language can represent cumulative effects on continuous resources, and can encode hybrid automata in a succinct way. 
"1132,Dominance Rules for the Choquet Integral in Multiobjective Dynamic Programming,"Multiobjective Dynamic Programming (MODP) is a general problem solving method used to determine the set of Pareto-optimal solutions in optimization problems involving discrete decision variables and multiple objectives. It applies to combinatorial problems in which Pareto-optimality of a solution extends to all its sub-solutions (Bellman principle). In this paper we focus on the determination of the preferred tradeoffs in the Pareto set where preference is measured by a Choquet integral. This model provides high descriptive possibilities but the associated preferences generally do not meet the Bellman principle, thus preventing any straightforward adaptation of MODP. To overcome this difficulty, we introduce here a general family of dominance rules enabling an early pruning of some Pareto-optimal sub-solutions that cannot lead to a Choquet optimum. Within this family, we identify the most efficient dominance rules and show how they can be incorporated into a MOPD algorithm.Then we report numerical tests showing the actual efficiency of this approach to find Choquet-optimal tradeoffs in multiobjective knapsack problems."1137,A Multi-objective Memetic Algorithm for Vehicle Resource Allocation in Sustainable Transportation Pl,"Sustainable supply chain management has been an increasingly important topic of research in recent years.  At the strategic level, there are computational models which study supply and distribution networks with environmental considerations.  At the operational level, there are, for example, routing and scheduling models which are constrained by carbon emissions.  Our paper explores work in tactical planning with regards to vehicle resource allocation from distribution centres to customer locations in a multi-echelon logistics network.  We model a bi-objective optimization problem and design a memetic algorithm to derive an approximate Pareto front efficiently."1138,Weight-Enhanced Diversification in Stochastic Local Search for Satisfiability," A key challenge in developing efficient local search solvers is to effectively balance between diversification and intensification. A majority of the state-of-the-art local search solvers perform random and/or Novelty-based walks to boost the diversification. At random walk scheme, a randomised unsatisfied clause is selected to performed Novelty$^+$ trap escape. In clause weighting scheme, clause weights of unsatisfied clauses are increased when the search encounters local minima. The high-weighted clauses have high priority to be solved in the next steps at intensification but Novelty$^+$ escape. In this paper, greediness in selecting clauses to performed the Novelty$^+$ is taken into account instead of randomly selecting unsatisfied clauses. In order to resolve the long-term unsatisfied clause in trap escape phases, the clauses with high weights are preferred to be selected to perform Novelty$^+$ escapes. The experimental results show that the new selecting variable approach significantly improves the performance of local search solvers on a wide range of structured and random benchmarks."1139,Central Clustering of Categorical Data with Automated Feature Weighting,"Clustering high-dimensional categorical data with attributes weighting is essential for many applications such as machine learning in bioinfomatics.  Currently, central clustering of categorical data is a difficult problem due to the lack of a geometrically interpretable definition of a cluster center. In this paper, we propose a novel kernel-density based definition using a Bayes-type probability estimator.  Then, a new algorithm called $k$-centers is proposed for central clustering of categorical data incorporating a new feature weighting scheme, with which each attribute is automatically assigned  with a weight measuring its individual contribution for the clusters. Experimental results on real-world data show outstanding performance  of the proposed algorithm, especially in recognizing the biological patterns in DNA sequences."1143,Learning Optimal Cepstral Features for Audio Classification,"Cepstral features have been widely used in audio applications. Despite the varieties of cepstral features proposed in the literature, their validity and effectiveness are largely based on domain knowledge and empirical evidences. In this paper, we present a novel approach for learning optimal cepstral features directly from the data set so as to better discriminate between different categories of audio signals and enhance the performance of audio classification tasks. This is based on a novel interpretation of the cepstral feature extraction process as a multi-layer feed-forward neural network. We then propose a wrapper approach for learning optimal audio features in the context of a Support Vector Machine (SVM) classifier.  This is achieved by incorporating the feature selection component represented by the neural network into the SVM objective function. We can formulate this as a single optimisation problem which alternates between learning the feature parameters (i.e. weights of the neural network) and SVM classifier parameters in an iterative framework. Experimental results show the efficacy of the proposed feature learning approach, outperforming competing methods by a large margin on benchmark data sets. "1146,Protein function prediction,Protein function prediction1153,Nominal Schema Absorption,"Nominal schemas have recently been introduced as a new approach for the integration of DL-safe rules into the Description Logic framework. The efficient processing of knowledge bases with nominal schemas remains, however, challenging.  We address this by extending the well-known optimisation of absorption and an extension to the tableau calculus that directly handles the absorbed nominal schema axioms. We implement the resulting extension of standard tableau calculi in a novel reasoning system with further optimisations and in our empirical evaluation, we show the effect of these optimisations and we find that the proposed approach performs well even when compared to other DL reasoners with dedicated rule support."1158,Cyclic Causal Models with Discrete Variables: Markov Chain Equilibrium Semantics and Sample Ordering,"We analyze the foundations of cyclic causal models for discrete
variables, and compare structural equation models (SEMs) to an
alternative semantics as the equilibrium (stationary) distribution
of a Markov chain. We show under general conditions, discrete cyclic SEMs cannot have
independent noise; even in the simplest case, cyclic structured
equation models imply constraints on the noise. We give a
formalization of an alternative Markov chain equilibrium semantics
which requires not only the causal graph, but also a sample
order. We show how the resulting equilibrium is a function of the
sample ordering, both theoretically and empirically."1161,Large Scale Spectral Clustering on Graphs,"Graph clustering is receiving growing attention in recent years as an important analytical technique, 
not only because of the prevalence of graph data, 
but also because the graph structure is very useful for exploiting the intrinsic characteristics of the data.
However, nowadays more and more graphs are appearing in large scale, making it challenging to find the clusters.
In this paper we propose an efficient clustering algorithm for large-scale graph data using spectral method.
Specifically, aggregations of nodes in the graph known as ""supernodes'' are repeatedly generated and help transform the original graph into bipartite structure on which we run spectral methods to solve clustering.
Considering the relatively small size of supernodes and sparsity of the new generated bipartite graph, we are able to achieve great efficiency without losing much clustering power. Extensive experiments show the effectiveness and efficiency of our approach.
"1163,Multi-prototype Label Ranking with Novel Pairwise to Total Rank Aggregation,"We propose a prototype-based algorithm for online learning of soft pairwise-preferences over labels. The algorithm learns the label preferences via minimization of the proposed soft rank-loss measure, and can learn from various types of orders, total orders, partial orders, bipartite orders or event partial pairwise preferences. The soft pairwise preference algorithm outputs are further aggregated to produce a total label ranking prediction using a novel greedy aggregation algorithm that outperforms existing aggregation solutions. Experiments on synthetic and real-life data show that our method achieves state-of-the-art performance in label ranking tasks."1170,"Uniform Convergence, Stability and Learnability for Ranking Problems","Most studies were devoted to the design of efficient algorithms and the evaluation and application on diverse ranking problems, whereas few work has been paid to the theoretical studies on ranking learnability. In this paper, we study the relation between uniform convergence, stability and learnability of ranking. In contrast to supervised learning where the learnability is equivalent to uniform convergence, we show that the ranking uniform convergence is sufficient but not necessary for ranking learnability with AERM, and we further present a sufficient condition for ranking uniform convergence with respect to bipartite ranking loss. Considering the ranking uniform convergence being unnecessary for ranking learnability, we prove that the ranking average stability is a necessary and sufficient condition for ranking learnability."1200,Towards Robust Co-Clustering,"Nonnegative Matrix Tri-factorization (NMTF) and its graph regularized
extensions have been widely used for co-clustering task to group data
points and features simultaneously. However existing methods are sensitive
to noises and outliers which is due to the squared loss function is used
to measure the quality of data reconstruction and graph regularization. In
this paper, we extend GNMTF by introducing a sparse outlier matrix into
the data reconstruction function and applying the $\ell_1$ norm to measure
graph dual regularization errors, which leads to a novel Robust
Co-Clustering (RCC) method. Accordingly, RCC is expected to obtain a more
faithful approximation to the data recovered from sparse outliers, and
achieve robust regularization by reducing the regularization errors of
unreliable graphs via $\ell_1$ norm. To solve the optimization problem of
RCC, an alternating iterative algorithm is provided and its convergence is
also proofed. We also show the connection between the sparse outlier
matrix in data reconstruction function and the robust Huber M-estimator.
Experimental results on real-world data sets show that our RCC
consistently outperforms the other algorithms in terms of clustering
performance, which validates the effectiveness and robustness of the
proposed approach."1180,A Monte-Carlo Approach for the Adaptive Control of Crowdsourcing ,"A crowdsourcing system can get more confident about the answer of a task and simultaneously extend its knowledge of the world by hiring additional workers, but hiring each worker incurs costs. We present an approach for control of crowdsourcing that can adaptively trade-off the cost of a worker with the expected improvement in task performance, as it continuously learns from worker contributions. The approach uses a set of predictive models as an implicit representation of the dynamics of the world, and it represents its uncertainty about its model of the world as distributions over the predictive models. It uses a Monte-Carlo algorithm to reason about its uncertainty about its model and future transitions simultaneously.  We evaluate the approach on a challenging citizen-science problem and demonstrate how it can successfully balance between learning and optimization during its life cycle.  "1182,Computing Stable Models for Nonmonotonic Existential Rules,"Function-free rules with existential quantifiers in
rule heads have attracted much recent interest in
knowledge representation. In this work, we extend
existential rules with nonmonotonic negation
under a stable model semantics. Computing entailments
under this semantics is difficult, since a set
of existential rules can have multiple, potentially
infinite stable models. To address this, we analyse
whether one rule relies on another, in the sense that
it might either be â€˜triggeredâ€™ or â€˜inhibitedâ€™ by the
other ruleâ€™s application. These relationships allow
us to define R-acyclicity and R-stratification, which
encompass large classes of rule sets that have finite,
unique stable models for arbitrary sets of input
facts. Realistic datasets are rarely arbitrary, and
we further extend our criteria to take advantage of
known constraints. Checking each of these criteria
is computationally feasible, and we provide tight
complexity bounds. We apply our results to a realworld
knowledge base from biochemistry that involves
complex relational structures. Our methods
allow us to solve reasoning problems over this data
using an off-the-shelf answer set programming engine.
The entailments expose various errors in the
original dataset, thus illustrating the practical utility
of logical reasoning in this domain."1247,Cross Lingual Entity Linking with Bilingual Topic Model,"Cross lingual entity linking means linking an entity mention in a background source document in one language with the corresponding real world entity in a knowledge base written in other language. The key problem is to measure the similarity score between the document of the entity mention and the candidate entity. This paper presents a general framework for doing cross lingual entity linking by leveraging a large scale and bilingual knowledge base, Wikipedia. We introduce bilingual topic model that mining bilingual topic from this knowledge base with the assumption that the same Wikipedia concept documents of two different languages share the same semantic topic distribution. The extracted topics have two types of representation, with each type corresponding to one language. Thus both the document of the entity mention and candidate entity can be represented in a space using the same semantic topics. We use these topics to do cross lingual entity linking. Experimental results show that the proposed approach can obtain the competitive results compared with the state-of-art approach."1187,Persistent Homology: An Introduction and a Text Application,"Persistent homology is a mathematical tool from topological data analysis.  It performs multi-scale analysis on a set of points and identifies clusters, holes, and voids therein.  Those latter topological structures complement standard feature representations, making persistent homology an attractive feature extractor for artificial intelligence.  Research on persistent homology for AI is in its infancy, and is currently hindered by two issues: the lack of an accessible introduction to AI researchers, and the paucity of applications.  In response, the first part of this paper presents a tutorial on persistent homology specifically aimed at a broader audience without sacrificing mathematical rigor.  The second part contains one of the first applications of persistent homology to natural language processing.  Specifically, our Similarity Filtration with Time Skeleton (SIFTS) algorithm identifies holes that can be interpreted as semantic ""tie-backs"" in a text document.  We illustrate our algorithm on documents ranging from nursery rhymes to novels, and on a corpus with child and adolescent writings."1195,Personalized Diagnosis for Over-Constrained Problems,"Constraint-based applications such as configurators, recommenders, and scheduling systems support users in complex decision making scenarios. Typically, these systems try to identify a solution that satisfies all articulated user requirements. If the requirements are inconsistent with the underlying constraint set, users have to be actively supported in finding a way out from the \\emph{no solution could be found} dilemma. In this paper we introduce techniques that support the calculation of personalized diagnoses for inconsistent constraint sets. These techniques significantly improve the diagnosis prediction quality compared to approaches based on the calculation of minimal cardinality diagnoses. In order to show the applicability of our approach we present the results of an empirical study and a corresponding performance analysis. "1203,Deep Sparse Coding based Recursive Disaggregation Model for Water Conservation,"The inadequate access to fresh water causes a wide range of ecological and human health crises. New techniques to long-term water conservation that incorporate principles of sustainability are expected. Existing disaggregation techniques focus on learning consumption patterns for individual devices. However, little attention has been given to the hierarchical decomposition structure of the aggregated consumption. In this paper, a Deep Sparse Coding based Recursive Disaggregation Model (DSCRDM) is proposed for water disaggregation task. We learn the recursive decomposition structure based on deep sparse coding model and separate consumption of devices iteratively. In particular, we develop an efficient and effective algorithm for discriminatively training the deep architectures of sparse coding model to maximize disaggregation performance.  We showed that the proposed model significantly improved the performance of state-of-the-art methods on a large scale disaggregation task and illustrated how these disaggregation results could provide practical device level information for water conservation."1226,Manifold Alignment Based on Sparse Local Structures of More Corresponding Pairs,"Manifold alignment is to extract the shared latent semantic structure from multiple manifolds. The joint adjacency matrix plays a key role in manifold alignment. To construct the matrix, it is crucial to get more corresponding pairs. This paper proposes an approach to obtain more and reliable corresponding pairs in terms of local structure correspondence. The sparse reconstruction weight matrix of each manifold is established to preserve the local geometry of the original data set. The sparse correspondence matrices are constructed via the sparse local structures of corresponding pairs across manifolds. On the other hand, a new energy function for manifold alignment is proposed to simultaneously match the corresponding instances and preserve the local geometry of each manifold. The shared low dimensional embeddings,
as better descriptions of the intrinsic geometry and relations between different manifolds, can be obtained by solving the minimization problem with closed-form solution. Experiments demonstrate the effectiveness of the proposed algorithm.
"1216,Positive and Unlabeled Learning for Robust Point Cloud Registration in Natural Environments,"Localization of a mobile robot is crucial for autonomous navigation. Using laser scanners, this can be facilitated by the pairwise alignment of consecutive scans. In this paper, we are interested in improving this scan alignment in challenging natural environments, using solely the scans as sensor data. For this purpose, local descriptors are generally effective as they facilitate point matching. However, we show that in some natural environments, many of them are likely to be unreliable, which affects the accuracy and robustness of the results. Therefore, we propose to filter the unreliable descriptors as a prior step to alignment. Our approach uses a fast machine learning algorithm, trained on-the-fly under the Positive and Unlabeled learning paradigm without the need for human intervention. Our results show that the number of descriptors can be significantly reduced, while increasing the proportion of reliable ones, thus speeding-up and increasing the robustness of the scan alignment process."1218,Audit Games,"Effective enforcement of laws and policies requires expending resources to  prevent and detect offenders, as well as appropriate punishment schemes to  deter violators. In particular, enforcement of privacy laws and policies in  modern organizations that hold large volumes of personal information (e.g.,  hospitals, banks, and Web services providers like Google and Facebook) rely  heavily on internal audit mechanisms. We study economic considerations in the  design of these mechanisms, focusing in particular on effective resource  allocation and appropriate punishment schemes. We present an audit game model  that is a natural generalization of a standard security game model for resource  allocation with an additional punishment parameter. Computing the Stackelberg  equilibrium for this game is challenging because it involves solving an  optimization problem with non-convex quadratic constraints.  We present an  additive FPTAS that efficiently computes a solution that is arbitrarily close  to optimal."1220,Functional Stable Model Semantics and Answer Set Programming Modulo Theories,"Recently there has been an increasing interest in incorporating ``intensional'' functions in answer set programming. Intensional functions are those whose values can be described by other functions and predicates, rather than predefined as in the standard answer set programming. We demonstrate that functional stable model semantics plays an essential in defining the framework of ``Answer Set Programming Modulo Theories (ASPMT)'' ---tight integration of answer set programming and satisfiability modulo theories, under which existing integration approaches can be viewed as special cases where the role of functions is limited. We reformulate and compare two representative stable model semantics for intensional functions, and show that, despite the differences in the intuitions behind them, they coincide on the syntactic classes of ASPMT programs that can be compiled into SMT. 
"1225,Efficient Extraction and Representation of Spatial Information from Video Data,"Vast amounts of video data are available on the web and are being generated daily using surveillance cameras or other sources. Being able to efficiently analyze and process this data is essential for a number of different applications. We want to be able to efficiently detect activites in these videos or be able to extract and store essential information contained in these videos for future use and easy search and access. 
Cohn et al (2012) proposed a comprehensive representation of spatial features that can be efficiently extracted from video and used for these purposes. In this paper, we present a modified version of this approach that is equally efficient and allows us to extract spatial information with much higher accuracy than previously possible. Our approach works in real time and is extremely memory efficient. We present efficient algorithms both for extracting and storing spatial information from video, as well as for processing this information in order to obtain useful spatial features. We evaluate our approach and demonstrate that the extracted spatial information is considerably more accurate than that obtained from existing approaches. "1234,Multi-View Embedding Learning for Incompletely Labeled Data,"With the advent of the Internet, large scale datasets are available.
The data may be high dimensional,  represented by multiple features,
and associated with more than one concepts. Embedding learning is an
effective strategy for dimensionality reduction and efficient
nearest neighbor search in massive datasets. We propose a novel
method to seek compact embedding that allows efficient retrieval
with multi-label multi-view data. Based on multi-graph Laplacian, we
achieve the optimal combination of heterogeneous features to
effectively describe multi-view data, which exploit the feature
correlations between different views. We learn  the  embedding which
preserves the neighborhood context in the original spaces by using
the incompletely labeled data, and obtain the refined multi-label
vectors at the same time. Inter-label correlations are sufficiently
captured to improve the performance of embedding learning with
multi-label data. The experimental evaluation on real-world datasets
demonstrates promising results that validate our method."1235,Instance Selection and Instance Weighting  for Cross-domain Sentiment Classification via PU Learning,"With the growing volume of online reviews available on the Internet, we can easily access to huge amounts of labeled reviews, but only some of them are beneficial for training a target-domain sentiment classifier, and we want to find those that are related to our interest. To address this problem, we propose a novel approach, based on instance selection and instance weighting. PU learning is used at first to identify the most target-domain-likely training samples, and assign an in-domain probability to each of the training samples. After an appropriate calibration, the in-domain probabilities are then used as sampling weights for training an instance-weighted naive Bayes model, within the framework of weighted maximum likelihood estimation. Experimental results prove the necessity and effectiveness of our approach, especially when the size of training data is large. It is also proved that the larger the Kullback-Leibler divergence between the training and test data is, the more effective our approach will be."1236,A Reputation Management Model for Resource Constrained Trustee Agents,"Trust is an important mechanism enabling agents to self-police open and dynamic multi-agent systems (ODMASs). Trusters evaluate the reputation of trustees based on their past observed performance, and use this information to guide their future interaction decisions. Existing trust models tend to concentrate trustersâ€™ interactions on a small number of highly reputable trustees to minimize risk exposure. When a trusteeâ€™s servicing capacity is limited, such an approach may cause long delays for trusters and subsequently damage the reputation of trustees. To mitigate this problem, we propose a reputation management model for trustee agents based on distributed constraint optimization. It helps a trustee to make situation-aware decisions on which incoming requests to serve and prevent the resulting reputation score from being affected by factors out of the trusteeâ€™s control. The model is evaluated through theoretical analysis and within a simulated, highly dynamic multi-agent environment. The results show that it can achieve close to optimally efficient utilization of the trustee agentsâ€™ collective capacity in an ODMAS, promotes fair treatment of trustee agents based on their behavior and significantly outperforms related work in enhancing social welfare."1238,Multi-winner Social Choice with Incomplete Preferences,"Multi-winner social choice considers the problem selecting a slate of $K$ options to realize some social objective. It has found application in the construction of policital bodies and committees, product recommendation, and related problems, and has recently attracted attention from a computational perspective. We address the multi-winner problem when facing incomplete voter preferences, using the notion of minimax regret to determine robust slates of options in the presence of preference uncertainty. We analyze the complexity of this problem and develop new exact and greedy robust optimization algorithms for its solution. Using these techniques, we also develop preference elicitation heuristics which, in practice, allow us to find near-optimal slates with considerable savings in required preference information vis-a-vis complete votes."1241,Integrate Edge and Node Features for Keywords Extraction,"Keywords extraction attracts much attention for its significant role in various natural language processing tasks. While some existing methods for keywords extraction have considered semantic relatedness between words and inherent features of words separately, almost all of them ignore two important issues:
1) how to fuse multiple semantic relations between words into a uniform
semantic measurement, and 2) how to integrate multiple semantic relations
between words and inherent various features of words into a unified model. In this work, we extend the supervised random walk method to tackle the above two issues, which combines the merits of semantic relatedness information and words' specific information simultaneously. We conducted extensive experimental study on established benchmarks and the experimental results demonstrate that our hybrid method outperforms both the state of the art supervised and unsupervised methods."1242,Rank-k Feature Selection,rank-k feature selection the abstract will be updated soon.1243,Large Scale Online Kernel Classification,"In this work, we present a new framework for large scale online kernel classification, making kernel methods efficient and scalable for large-scale online learning tasks. Unlike the regular budget kernel online learning scheme that usually uses different strategies to bound the number of support vectors, our framework explores a functional approximation approach to approximate kernel function/matrix in order to make the subsequent online learning task efficient and scalable. Specifically, we present two different online kernel machine learning algorithms: (i) the Fourier Online Gradient Descent (FOGD) algorithm that applies the random Fourier features for approximating kernel functions; and (ii) the Nystr\\\""{o}m Online Gradient Descent (NOGD) algorithm that applies the Nystr\\\""{o}m method to approximate large kernel matrices. We offer theoretical analysis of the proposed algorithms, including the high probability sub-linear regret loss bounds. Our large-scale online learning experiments where some data set has over 1 million instances validate the efficiency and effectiveness of the proposed algorithms, making them potentially more practical than the existing budget kernel online learning approaches.
"1245,Efficient Learning in Linearly Solvable MDP Models,"Linearly solvable Markov Decision Process (MDP) models are a powerful subclass of problems with a simple structure that allow the policy to be written directly in terms of the uncontrolled (passive) dynamics of the environment and the goals of the agent. However, there have been no learning algorithms for this class of models. In this research, we develop a robust learning approach to linearly solvable MDPs. To exploit the simple solution for general problems, we show how to construct passive dynamics from any transition matrix, use Bayesian updating to estimate the model parameters and apply approximate and efficient Bayesian exploration to speed learning. In addition, we reduce the computational cost of learning using intermittent Bayesian updating and policy solving.  We also gave a polynomial theoretical time complexity bound for the convergence of our learning algorithm, and demonstrate a linear bound for the subclass of the reinforcement learning problems with the property that the transition error depends only on the agent itself. Test results for our algorithm in a grid world were presented, comparing our algorithm with the BEB algorithm.  The results showed that our algorithm learned more than the BEB algorithm without losing convergence speed, so that the advantage of our algorithm increased as the environment got more complex. We also showed that our algorithmâ€™s performance is more stable after convergence. Finally, we show how to apply our approach to the Cellular Telephones problem by defining the passive dynamics."1252,Coupled Attribute Analysis on Numerical Data,"The usual representation of quantitative data is to formalize it as an information table, which assumes the independence of continuous attributes. But in real-world data sources, attributes are more or less associated via certain coupling relationships. Limited research has been conducted on attribute interaction analysis, and they only implicitly describe a local picture of relationships. This paper proposes a framework of coupled attribute analysis on continuous data. It integrates the intra-coupled interaction within an attribute (i.e. the correlations between attributes and their own powers) and inter-coupled interaction among different attributes (i.e. the correlations between attributes and the powers of others) to form a coupled representation for objects by Taylor expansion. This is the first work which explicitly addresses the global dependency of continuous attributes, verified by the applications in data structure analysis, clustering, and classification.  Substantial experiments on 13 UCI data sets demonstrate that the coupled representation can effectively capture the interactions of numerical attributes with better performance compared to the traditional way, supported by statistical analysis."1255,Stitching WebTables,"Recently, there has been quite much work on harvesting raw HTML tables from the Web [Cafarella et al., 2008a; 2008b; Venetis et al., 2011] (or HTML lists [Elmeleegy et al., 2009]). Despite the success of extracting a large number of tables as separate (mini-)databases, the relationship between them is vastly ignored in the literature except [Das Sarma et al., 2012] that studied how to find related tables. In this paper, we will present the first attempt, as far as we are aware of, of providing a unified view for a set of individual tables. Specifically, we identify tables that are highly relevant, infer the meta-data from their context, and finally merge them into a synthesized table with semantic consistency. We call the whole process, Table Stitching where the meta-data is the threads that hold all the tables together. This will enable users to explore the harvested databases with more powerful capabilities."1256,Accurate Estimates of Reference Evapotranspiration for Irrigation Management in the Texas High Plain,"Accurate estimates of daily crop evapotranspiration (ET) are needed for efficient irrigation management in regions where crop water demand exceeds rainfall. Daily grass or alfalfa reference ET values and crop coefficients are widely used to estimate crop water demand. Inaccurate reference ET estimates can hence have a tremendous impact on irrigation costs and the demands on freshwater resources. ET networks calculate reference ET using precise measurements of meteorological data. These networks are typically characterized by gaps in spatial coverage and lack of sufficient funding, creating an immediate need for alternative sources that can fill data gaps without high costs. Although non-agricultural weather stations provide publicly accessible meteorological data, there are concerns that the data may not be suitable for estimating reference ET due to factors such as weather station siting, data formats, and quality control issues. The objective of our research is to enable the use of alternative data sources by modeling the nonlinear relationships between non-ET weather station data and the reference ET computed by ET networks. This paper considers the representative example of the Texas High Plains region in the U.S., using sophisticated machine learning algorithms such as Gaussian process models and neural networks to formulate the problem of predicting reference ET based on data from non-ET weather stations.  Experimental results show significant improvement in prediction accuracy in comparison with the baseline regression models typically used for irrigation management applications."1260,Multiple Link Sign Prediction in Online Signed Social Networks,"Online social networks continue to witness a tremendous growth both in terms of the number of registered users and their mutual interactions. In this paper, we focus on online signed social networks where positive interactions among the users signify friendship or approval, whereas negative inter-
actions indicate antagonism or disapproval. A comprehensive analysis of the nature of these interactions is fundamental to success of the social network since it lends rich insights about the underlying behavior patterns and thus facilitates the network designers in gauging user preferences. We introduce a novel problem which we call multiple link sign prediction problem: Given the information about signs of certain links in a social network, we want to learn the nature of relationships that exist among the users by predicting the sign, positive or negative, of the remaining links.

We propose two techniques to address this problem, namely a matrix factorization based technique MF-LiSP and a logistic regression method. To the best of our knowledge, MF-LISP is the first algorithm that uses matrix factorization for multiple link prediction. The technique is amenable
to our appropriately defined pairwise loss function that elegantly reflects the behavior of each user relative to others and thereby makes the technique robust to fluctuations in individual behaviors unlike methods based on point-wise loss functions. We also extend the applicability of logistic regression (Leskovec et al. [6]) to predicting multiple links simultaneously. The results of our experiments on synthetic data sets and real benchmarks such as Epinions, Slashdot, and Wikipedia strongly corroborate the efficacy of the proposed approaches. "1264,Social Collaborative Filtering by Trust,"To accurately and actively provide users with their potentially interested information or services is the main task of a recommender system. Collaborative filtering is one of the most widely adopted recommender algorithms, whereas it is suffering the issues of data sparsity and cold start that will severely degrade quality of recommendations. To address such issues, this article proposes a novel methods, TrustMF, trying to improve the performance of collaborative filtering recommendation by means of elaborately integrating twofold sparse information, i.e., the conventional rating data given by users and the social trust network among the same users. TrustMF is a model-base method based on matrix factorization, which maps users into low-dimensional latent feature spaces in terms of their trust relationship, enabling to reflect usersâ€™ reciprocal influence on their own ratings more reasonably. The validations against real world datasets show that TrustMF performs much better than state-of-the-art recommendation algorithms of social collaborative filtering by trust."1265,Generalized Relational Topic Models with Fast Sampling Algorithms,"Relational topic models have shown promise on analyzing document network structures and discovering latent topic representations. This paper presents three extensions: 1) unlike the common link likelihood model with a diagonal weight matrix that allows the-same-topic interactions only, we generalize it to use a full weight matrix that captures all pairwise topic interactions and is applicable to asymmetric networks; 2) instead of doing standard Bayesian inference, we perform regularized Bayesian inference with a regularization parameter to deal with the imbalanced link structure problem in common sparse networks; and 3) instead of using variational approximation methods with normally strict mean-field assumptions, we present a collapsed Gibbs sampling algorithm for the generalized relational topic models without making any restricting assumptions. Experimental results show that these extensions are important and they can significantly improve the prediction performance and time efficiency."1269,What Users Care about: a Framework for Social Content Alignment,"As social media become increasingly popular, more and more people express their opinions and comments (or collectively social content) on daily news, products, movies and various Web documents across different social media platforms such as news portals, forums, discussion groups, and blogs. Given a popular Web document discussing a couple of topics, it can attract huge amount of social contents, making it impossible to manually read through all of them to check which comments align with the corresponding topics. In this paper, we propose a novel two-phase framework that can be used to automatically detect the topics from Web document as well as align the detected topics with the social content, which can provide an overall picture on which topics in the news are ""hot"" that many people are discussing, and allow users to focus on those topics/social content that they are interested. Extensive experimental results show that our proposed framework significantly outperforms the existing state-of-the-art methods in social content alignment."1270,Active Learning from Relative Queries,"Active learning has been extensively studied and shown to be useful in solving real problems. The typical setting of traditional active learning methods is querying labels from an oracle. This is only possible if an expert exists, which may not be the case in many real world applications. In this paper, we focus on designing easier questions that can be answered by a ""non-expert"" rather than an oracle. These questions poll relative information as opposed to absolute information and can be even generated from side-information rather than a human. In the context of label propagation, we propose an active learning approach that queries the ordering of the importance of an instance's neighbors rather than its label. We explore our approach on real data sets and make several interesting discovering including that querying neighborhood information is an effective question to ask and sometimes can even yield better performance than querying labels."1271,Isomorph-free branch and bound search for finite state controllers,"The recent proliferation of smart-phones and other wearable devices has lead to a surge of new mobile applications.  Partially observable Markov decision processes provide a natural framework to design applications that continuously make decisions based on noisy sensor measurements.  However, given the limited battery life, there is a need to minimize the amount of online computation.  This can be achieved by compiling a policy into a finite state controller since there is no need for belief monitoring or online search.  In this paper, we propose a new branch and bound technique to search for a good controller.  In contrast to many existing algorithms for controllers, our search technique is not subject to local optima. We also show how to reduce the amount of search by avoiding the enumeration of isomorphic controllers and by taking advantage of suitable upper and lower bounds.  The approach is demonstrated on several benchmark problems as well as a smart-phone application to assist persons with Alzheimer's to wayfind."1273,Leveraging Multi-Domain Prior Knowledge into Topic Models,"Topic models have been widely used to identify topics in text corpora. It is also known that purely unsupervised models often result in topics that are not meaningful in the application domain. In recent years, a number of knowledge-based models have been proposed, which allow the user to input prior knowledge of the domain to guide the inference process to produce more coherent and meaningful topics. In this paper, we go one step further to study whether the prior knowledge from other domains can be exploited to help modeling in the current domain. This is important from both the application and the learning perspectives because knowledge is inherently accumulative in nature. Human beings gain knowledge gradually and use the old knowledge to help solve new problems. For this purpose, all existing models have major difficulties. In this paper, we propose a new knowledge-based model, called MDK-LDA, for modeling with prior  knowledge from multiple domains. Our evaluation results will demonstrate its effectiveness."1275,Early Active Learning via Robust Representation and Structured Sparsity,"Labeling training data is quite timeconsuming
but essential for supervised learning
models. To solve this problem, the active
learning has been studied and applied to select
the informative and representative data
points for labeling. However, during the early
stage of experiments, only a small number
(or none) of labeled data points exist, thus
the most representative samples should be
selected first. In this paper, we propose a
novel robust active learning method to handle
the early stage experimental design problem
and select the most representative data
points. The robust sparse representation loss
function is utilized to reduce the effect of
outliers and the structured sparse regularization
is adopted to find the most representative
samples during the sparse representations.
A new efficient optimization algorithm
is introduced to solve our non-smooth
objective with low computational cost and
proved global convergence. Empirical results
on both single-label and multi-label classification
benchmark data sets show the promising
results of our method."1279,The Multi-feature Information Bottleneck with Application to Unsupervised Image Categorization,"We present a novel unsupervised data analysis method, Multi-feature Information Bottleneck (MfIB), which is an extension of the Information Bottleneck (IB) method. In contrast to the original IB, the new MfIB method can analyze the data from multiple feature variables simultaneously, which characterize the data from multiple cues. We apply the MfIB algorithm to unsupervised image categorization. In our experiments, by combining multiple sources of features, such as local shape, color, texture, the MfIB algorithm is found to be consistently superior to the original IB algorithm which takes into account  only  one source of feature. The proposed MfIB algorithm is also clearly outperforms the state-of-the-art unsupervised image categorization methods."1284,Robust Median Reversion Strategy for On-Line Portfolio Selection,"On-line portfolio selection has been attracting increasing interests from artificial intelligence community in recent decades. Mean reversion, as one most frequent pattern in financial markets, plays an important role in some state-of-the-art strategies. Though successful in certain datasets, existing mean reversion strategies do not fully consider noises and outliers in the data, leading to estimation error and thus non-optimal portfolios, which results in poor performance in practice. To overcome the limitation, we propose to exploit the reversion phenomenon by robust $L_1$-median estimator, and design a novel on-line portfolio selection strategy named ``Robust Median Reversion\"" (RMR), which makes optimal portfolios based on the improved reversion estimation. Empirical results on various real markets show that RMR can overcome the drawbacks of existing mean reversion algorithms and achieve significantly better results. Finally, RMR runs in linear time, and thus is suitable for large-scale trading applications."1287,Discovering Different Types of Topics: Multi-dimensional Membership Topics Models ,"In traditional topic models such as LDA, a word is generated by choosing a topic from a collection. However existing topic models do not identify different types of topics in a document, such as topics that represent the content and topics that represent the sentiment. In this paper, our goal is to discover such different types of topics, if they exist. We represent our model as several parallel topic models
(called topic dimensions), where each word is generated from topics from these dimensions jointly. Since the latent membership of the word is now a vector, the learning algorithms become challenging. We show that using a variational approximation still allows us to keep the algorithm tractable. Our experiments over several datasets show that our approach consistently outperforms many classic topic models while also discovering fewer, more meaningful, topics.
"1289,Multi-View K-Means Clusteriing for Heterogeneous Data Integration,"In recent computer vision research, multiple visual de-
scriptors have been designed for image understanding,
where different visual descriptors describe distinct perspec-
tives of the images. Although each type of visual descrip-
tor could be individually used for unsupervised image cat-
egorizations, the categorization results will be more ac-
curate by exploring rich information among multi-modal
descriptors. Several multi-modal clustering methods have
been proposed to unsupervised integrate multi-modal im-
ages. However, they are graph based approaches, e.g.
based on spectral clustering, such that they cannot han-
dle the big visual data. How to combine these multi-modal
visual features for unsupervised large-scale image catego-
rizations has become a challenging problem. In this paper,
we propose a new robust large-scale clustering method to
integrate heterogeneous visual features. We evaluate the
proposed new methods by six computer vision benchmark
data sets and compared the performance with several com-
monly used clustering approaches as well as the baseline
multi-modal clustering methods. In all experimental results,
our proposed methods consistently achieve superiors clus-
tering performances."1299,Better Generalization with Forecasts,"We compare a number of different methods on typical reinforcement-learning tasks with a large degree of hidden state.
Results show that *forecasts* (generalized value functions) provide better features, generally leading to better value function approximation when computed with linear function approximators, resulting in better generalization to as-yet-unseen parts of the state space."1312,Multi-Modal Image Annotation with Multi-Instance Multi-Label LDA,"This paper studies the problem of image annotation in a multi-modal setting where both visual and tex-tual information are available. We propose Multi-modal Multi-instance Multi-label Latent Dirichlet Allocation (M3LDA), where the model consists of a visual-label part, a textual-label part and a label-topic part. The basic idea is that the topic decided by the visual information and the topic decided by the textual information should be consistent, lead-ing to the correct label assignment. Particularly, M3LDA is able to annotate image regions, provid-ing a promising way to understand the relation be-tween input patterns and output semantics. Experiments on Corel5K and ImageCLEF validate the effectiveness of the proposed method ."1324,An Efficient Vector-Based Representation for Coalitional Games,"We propose a new, intuitive representation for coalitional games, called the coalitional skill vector games representation, where there is a set of skills in the system, and each agent has a skill vector --- a vector consisting of values that reflect the agents' strength in different skills. Furthermore, there is a set of goals, each with requirements expressed in terms of the minimum skill strength necessary to achieve the goal. Agents can form coalitions to aggregate their skills, and achieve goals otherwise unachievable. We show that this representation can represent any characteristic function game, and (for some of them) is significantly more compact than the classical representation. More importantly, we show that for some interesting classes of games, our representation facilitates the development of efficient algorithms to solve the coalition structure generation problem, as well as the problem of computing the core and/or the least core. While other exact algorithms can only solve those problems with no more than 30 agents or so, our solver can handle 500 agents."1325,Constraint Satisfaction and Fair Multi-Objective Optimization Problems,"The paper focuses on an extension of the CSP optimization framework tailored to identify ""fair"" solutions to instances involving multiple optimization functions. Two approaches are studied, based on selecting the solutions maximizing the minimum value over all the given functions (Max-Min approach) and on the lexicographical refinement where, over all solutions maximizing the minimum value, those maximizing the second minimum value are chosen, and so on, until all functions are considered (lexMax-Min). For both approaches, the computational complexity of computing an optimal solution is analyzed and the tractability frontier is charted, according to the structure of the constraint scope interactions and the number and the domains of the functions to be optimized. While the setting turns out to be NP-hard in general, the paper identifies large islands of tractability based on a notion of guardedness."1327,Constraint Acquisition via Partial Queries,"\cite{DBLP:conf/ijcai/BessiereCOP07} have  proposed a
framework for learning constraint networks via membership queries, that
is, by asking the user to classify total assignments of the variables
as positive or negative. In this paper we consider the case where the
user is able to answer partial queries, that is, to classify
assignments on subsets of the variables. We provide an algorithm that 
given a negative example is able to focus on a constraint
of the target network in a number of queries logarithmic in the size
of the example. We give lower bounds for learning some simple classes
of constraint networks and we show that our generic algorithm is optimal
on some of them. Finally we evaluate our algorithm on some
benchmark problems. "1328,Euler Clustering,"By always mapping data from lower dimensional space into higher or even infinite dimensional space (i.e. Reproducing Kernel Hilbert Space (RKHS)), kernel k-means is able to organize data into groups when data of different clusters are not linearly separable. However, like most of kernel machines, kernel k-means also incurs the large scale computation due to the representation theorem, i.e. keeping an extremely large kernel matrix in memory when using popular Gaussian and spatial pyramid match kernels, which largely limits its use for processing large scale data. In this paper, we introduce an Euler clustering, which can not only maintain the benefit of nonlinear modelling using kernel function but also significantly solve the large scale computational problem in kernel kmeans. This is realized by incorporating Euler kernel. Euler kernel is relying on a nonlinear and robust cosine metric, and more important it intrinsically induces an empirical map which maps data to a complex space of the same dimension. Euler clustering takes these advantages to measure the similarity between data in a robust way without increasing the dimensionality of data, thus solves the large scale problem in kernel k-means. We evaluate Euler clustering and show its superiority against related methods on five publicly available datasets."1332,Optimal Pricing for Improving Efficiency of Taxi Systems,"A long-standing problem in Beijing's taxi market is that most taxi drivers intentionally avoid working during peak hours despite of the huge customer demand within these peak periods.  The existence of this dilemma is mainly triggered by the ignorance of taxi drivers' congestion costs (i.e., wasting time, burning fuel, and risk of rear-end collision in congestion) in the present taxi fare structure.  To resolve this problem, we propose a new pricing scheme to provide taxi drivers extra incentives to work over peak hours, which differs from the previous studies of taxi market by considering market variance over multiple time periods, taxi drivers' choosing strategies to maximize their individual revenues, and taxi drivers' scheduling constraints with regard to the interdependence among different periods.  The major challenge of this research is to determine the optimal pricing structure to improve the taxi system's efficiency against taxi drivers' selfish decisions.  Another difficulty is the computational intensiveness to identify the optimal strategy of taxi drivers due to the exponentially large size of the taxi drivers' strategy space and constraints.  To fulfill this end, we develop an atom schedule method to overcome issues aforementioned.  As a result, the proposed methodology reduces the problem magnitude while preserves constraints to filter out infeasible or unrealistic pure strategies.  Simulation results based on real data show the effectiveness of the proposed methods, which open a new door to improved efficiency of megacities' (e.g., Beijing) taxi market."1337,A Tree-based Tabu Search for the Manpower Allocation Problem with Time Windows and Job-Teaming Const,"This paper investigates a manpower allocation problem with time windows and job-teaming constraints (MAPTWTC), a practical scheduling and routing problem that tries to synchronize work schedules to complete all tasks. We first provide an integer programming model and demonstrate its correctness to the problem. Subsequently, we show that the tree structure can be used to represent feasible solutions, and the optimal solution can be obtained from one of trees by solving a minimum cost flow model. Finally, we propose a tree-based tabu search algorithm. The computational results on two sets of instances show the effectiveness of our solution procedure.
"1338,Promoting Diversity in Recommendation by Entropy Regularizer,"We study the problem of diverse promoting recommendation task: selecting a subset of diverse items that can better predict a given user\\\'s preference. Recommendation techniques primarily based on user or item similarity can suffer from the risk that users cannot get expected information from the over-specified recommendation lists. In this paper, we propose an entropy regularizer to capture the notion of diversity. The entropy regularizer has good properties in that it satisfies monotonicity and submodularity, such that when we combine it with a modular rating set function, we get submodular objective function, which can be maximized approximately by efficient greedy algorithm, with provable constant factor guarantee of optimality. We apply our approach on the top-$N$ prediction problem and evaluate its performance on MovieLens data set, which is a standard database containing movie rating data collected from a popular online movie recommender system. We compare our model with the state-of-the-art recommendation algorithms. Our experiments show that (1) users do have wider interest that traditional recommendation techniques cannot cover, (2) the entropy regularizer effectively captures diversity and hence improves the performance of recommendation task."1340,Multi-Instance Multi-Label Learning with Weak Label,"Multi-Instance Multi-Label learning (MIML) deals with data objects that are represented by a bag of in-stances and associated with multiple class labels simultaneously. Previous studies assume that for every training example, all positive labels are given whereas the other labels are all negative. In many real applications such as image annotation, how-ever, the learning tasks often suffer from weak label; that is, users usually tag only a part of positive labels, and the untagged labels are not necessarily negative. In this paper, we propose the MIMLwel approach which works by assuming that highly relevant labels share some common instances, and the underlying class means of bags on each label are with a large margin. Experiments validate the effectiveness of MIMLwel in handling the weak label problem."1341,Adaptive thresholding in structure learning of a Bayesian network,"Thresholding a measure in conditional independence (CI) tests using a fixed value enables learning and removing edges as part of learning a Bayesian network structure. However, the learned structure is sensitive to the threshold that is commonly selected: 1) arbitrarily; 2) irrespective of characteristics of the domain; and 3) fixed for all CI tests. We analyze the impact on mutual information, a measure in CI testing, of factors, such as the sample size, degree of variable dependence, and cardinalities of the variables. Following, we suggest to adaptively threshold individual tests based on the factors. We show that adaptive thresholds better distinguish between pairs of dependent variables and pairs of independent variables and enable learning structures more accurately and quickly than when using fixed thresholds."1342,On the complexity of global scheduling constraints under structural restrictions,"In the paper we investigate computational complexity of two global constraints, the cumulative constraint and the InterDistance constraint,
that are key constraints in modeling and solving scheduling problems. Enforcing domain consistency on these constraints is known to be NP-hard.
However, in many practical applications, restricted versions of these constraints are suf&amp;#64257;cient. One example is a job-scheduling problem, where we only have a small number of distinct tasks or tasks are sparsely distributed over time. Another example comes from runway sequencing problems for air-traf&amp;#64257;c control, where we can often assume that landing periods have a certain regular pattern. Such cases can be characterized in terms of structural restrictions on the constraints. We identify a number of such structural restrictions and investigate how they impact the computational complexity of global constraints. We also investigate the case when precedence relations are present among
tasks. We prove that such restrictions often make the propagation tractable."1348,Undecidability of epistemic multi-agent planning,"Dynamic epistemic logic (DEL) provides a very expressive framework for multi-agent planning that can deal with non-determinism, partial observability, sensing actions, and arbitrary nesting of beliefs about other agents. However, as we show in this paper, this expressiveness comes at a price. The planning framework is undecidable, even if we allow only purely epistemic actions (actions that change only beliefs, not ontic facts). Undecidability holds already in the S5 setting with 2 agents, and even with 1 agent when allowing arbitrary frames. It shows that multi-agent planning is robustly undecidable if we assume that agents can reason with an arbitrary nesting of beliefs about beliefs. We also prove a corollary showing undecidability of model checking of DEL with the star operator on actions (iteration). 
"1351,Proportional Representation Under Preferences That Are Single-peaked on a Tree ,"We study the complexity of electing a committee under variants of the Chamberlin-Courant rule when the voters' preferences are single-peaked on a tree. We first show that this problem is easy for the Rawlsian, or ""minimax"" version of this problem, for arbitrary trees and misrepresentation functions. For the standard (utilitarian) version of this problem we provide an algorithm for an arbitrary misrepresentation function whose running time is polynomial in the input size as long as the number of leaves of the underlying tree is bounded by a constant. We argue that the constraint on the number of leaves cannot be removed, by proving that our problem remains computationally hard on trees that have bounded degree, diameter or pathwidth. Finally, we show how to modify Trick's (1989) algorithm to check whether an election is single-peaked on a tree whose number of leaves does not exceed a given parameter C. "1353,A Social Welfare Optimal Sequential Allocation Procedure,"We consider a simple sequential allocation  procedure for sharing indivisible goods between agents in which agents take turns to pick item.  Supposing additive utilities and independence between the agents, we  show that the expected social welfare can be computed in polynomial time. Using this result, we prove that the expected social welfare is maximized when agents take turns in a fixed order. We also argue that this mechanism remains optimal when 
agents behave strategically. "1360,Online Group Feature Selection,"Online feature selection provides a lot of advantages
over standard feature selection methods as it assumes
features flow in one by one and it does not need the
global feature space. However, in real-world applications,
features arrive in by groups. For example, an image
could be represented by multiple groups (descriptors),
each of which describes color, texture or shape.
In this case, existing online feature selection methods
will overlook the prior knowledge of predefined groups
because they only evaluate features individually. Meanwhile,
existing group feature selection methods do not
evaluate the features within the groups, which will lead
to the redundancy of the subset, and is not suitable
for online feature selection as other standard methods.
Motivated by this, we propose a novel approach for online
group feature selection in this paper. Our approach
consists of the online intra-group selection and the online
inter-group selection. In the intra-group selection,
we use the spectral analysis to select the features with
discriminative natures as a group of features arrive. In
the inter-group selection, we use the sparse linear regression
model of Lasso to select the global optimal
subset of features. The process mentioned above continues
until there are no more features arriving or the
predefined conditions are met. Extensive experiments
conducted on the benchmark and real-world data sets
demonstrate that our proposed approach outperforms
the other state-of-the-art online feature selection methods
such as Alpha-investing and Fast-OSFS on classification
accuracy."1366,Cross-Domain Collaborative Filtering via Bilinear Multilevel Analysis,"Cross-domain collaborative filtering (CDCF) is be-coming an emerging research topic in recent years, which aims to leverage data from multiple do-mains to relieve the data sparsity issue. However, current CDCF methods only consider user and item factors but totally neglect heterogeneous domain factors, which may lead to improper knowledge transfer issues. To address this problem, we pro-pose a novel CDCF model, namely Bi-Linear Mul-tilevel Analysis (BLMA), which seamlessly bridges multilevel analysis theory to the most successful CF method, matrix factorization. Specially, BLMA uses a hierarchical structure that jointly consider the domain, community and user effects over items, so it can always generate domain adaptive factors using such multilevel effects. Moreover, a parallel Gibbs sampler is also devised to learn these effects. Experiments conducted on the real-world dataset prove the superiority of BLMA against other comparative state-of-the-art methods."1367,Active Learning for Cross-domain Sentiment Classification,"In the literature, some approaches have been proposed to address the domain adaptation problem in sentiment classification (also called cross-domain sentiment classification). However, the adaptation performance greatly suffers when the data distributions in the source and target domains differ significantly. In this paper, we suggest to perform active learning for cross-domain sentiment classifcation by actively selecting a small amount of labeled data in the target domain            . Accordingly, we propose an novel actve learning approach for cross-domain sentiment classification. First, we train two individual classifiers, i.e., the source and target classifiers with the labeled data from the source and target respectively. Then, the two classifiers are employed to select informative samples with the selection strategy of Query By Committee (QBC). Third, the two classifier is combined to make the classification decision. Importatnly, the two classifiers are trained by fully exploiting the unlabled data in the target domain with the label propagation (LP) algorithm. Empirical studies demonstrate the effectiveness of our active learning approach for cross-domain sentiment classification"1373,Multi-view Discriminant Transfer Learning,"We study to incorporate multiple views of data in a percep-
tive transfer learning framework and propose a Multi-view
Discriminant Transfer (MDT) learning approach to improve
the cross-domain adaptation. The main idea is to find the
optimal discriminant weight vectors for each view such that
the correlation between the two-view projected data is maximized,
while both the domain discrepancy and the view disagreement
are minimized simultaneously. Furthermore, we
analyze MDT theoretically from discriminant analysis perspective
to explain the condition and reason, under which
the proposed method is not applicable. We demonstrate
that the total discriminant power can be decomposed into
the view-based discriminant powers. It allows us to investigate
whether there exist within-view and/or between-view
conflicts, and thus provides a deep insight into whether the
algorithm work properly or not in the view-based problems
and the combined learning problem, which is often overlooked
in previous work. Experiments show that MDT significantly
outperforms the state-of-the-art baselines including
the semi-supervised algorithm TSVM [1], the traditional
multi-view algorithm Co-Training [2], the multi-view discriminant
analysis method FDA2 [3], the Co-Training based
adaptation algorithm CODA [4], and the large-margin-based
multi-view transfer model MVTL-LM [5]."1384,Reasoning about normative update,"In this paper we consider the problem of updating  a multi-agent system with a set of conditional norms. A norm comes into effect when its condition becomes true, and imposes either an obligation or a prohibition on an agent which remains in force until a state satisfying a deadline condition is reached. If the norm is violated, a sanction is imposed on the agent. We define a notion of a normative update of a multi-agent system by a set of conditional norms, and study the problem of checking whether the agent(s) can bring about a state satisfying $\phi$ without incurring a specified number of sanctions.
"1393,Accurate Probability Calibration for Multiple Classifiers,"In many real-world classification problems, it is important to convert the prediction scores obtained from the classifier to posterior class probabilities. While previous studies mainly focus on the probability calibration of a single
classifier, it is well-known that different classifiers have different strengths and
thus the combination of a number of classifiers can lead to better results.
In this paper, we propose a novel approach for the probability calibration of such an ensemble of classifiers. We first construct isotonic constraints on the predicted probabilities based on voting from the multiple classifiers' scores.
Manifold information is then incorporated as an efficient regularizer, and
the calibration model is learned by an extended isotonic regression model. Computationally,
the resultant optimization problem can be efficiently solved by a novel solver
based on the alternative direction method of multipliers (ADMM). Experiments
on a number of real-world data sets demonstrate that the
proposed method consistently outperforms independent classifiers and other
combinations of the classifiers' probabilities."1402,An Ambiguity Aversion Framework of Security Game under Ambiguities,"Security is a critical concern around the world, which arises in protecting people and critical assets from the threat of terrorist attacks and criminal behaviours. Since resources for security are always limited, significant interest has arisen in using game theory to handle security resource allocation problems. However, most of the existing work do not address adequately how a defender chooses his optimal strategy in a game with absent, inaccurate, uncertain and even ambiguous strategy profilesâ€™ payoffs. To address this issue, we propose a general framework of security games under ambiguities based on Dempster-Shafer theory and the ambiguity aversion principle of minimax regret. Then, we present some properties of this framework in terms of: (i) defining a precise payoffs transformation rule that can consider the deviation range of the point value payoffs without changing the optimal strategy; (ii) establishing the relationship between the payoffs boundary and the risk attitude of a player. Finally, we present two methods to reduce the influence of complete ignorance.
Our investigation shows that this new framework is better in handling security resource allocation problems under ambiguities."1403,Semiring-Based Mini-Bucket Partition Heuristics,"Many important real-world problems can be expressed as graphical
models. Their underlying algebraic properties are captured by a
valuation structure that, for the most usual problems, is a
semiring. When problems are too difficult to be solved exactly,
approximation methods becomes the best option. Mini-Bucket
Elimination (MBE) is a well-known approximation algorithm for graphical
models. It relies on a procedure to partition a set of functions into
smaller subsets. In most previous works, the partitioning process
considers the scopes of the functions only. Very recently, a new class
of partitioning schemes based on the content of the functions was
proposed for the task of computing the probability of the evidence. In
this paper we introduce a general partitioning framework for defining content-based partitioning-heuristics for a variety of problems. The framework is
based on two elements: a semiring structure endowed with an additional
operation called division, and a valid metric. We then
show how this framework is instantiated to define content-based
heuristics for optimization tasks. Finally, we demonstrate their impact on
a number of benchmarks for the task of computing the Most Probable
Explanation. In particular, we show that the recent Moment-Matching technique for MBE benefits from using content-based heuristics."1411,Efficient Interdependent Value Combinatorial Auctions with Single-Minded  Bidders,"We study the problem of designing efficient auctions where bidders have interdependent values; i.e., values that depend on the signals of other agents. We consider a contingent bid model in which agents can explicitly condition the value of their bids on the bids submitted by others. In particular, we adopt the a linear contingent bid model for single minded combinatorial auctions (CAs), presented by Ito and Parkes, in which submitted bids are linear combinations of bids received from others. We extend the existing state of the art, by identifying constraints on the interesting bundles and contingency weights reported by the agents which allow the efficient second priced, fixed point bids auction to be implemented in single minded CAs. Moreover, for domains in which the required single crossing condition fails (which characterizes when efficient, IC auctions are possible), we design a two-stage mechanism in which a subset of agents (\""experts\"") are allocated first, using their reports to allocate the remaining items to the other agents."1418,Preference-based Query Answering in Datalog+/- Ontologies,"Though the study of preferences has a long tradition in many disciplines, it has only relatively recently entered the realm of data management through their application in answering queries to relational databases. The current revolution in data availability through the Web and, perhaps most importantly in the last few years, social media sites and applications, puts ontology languages at the forefront of data and information management technologies. In this paper, we propose the first (to the best of our knowledge) integration of ontology languages with preferences by developing PrefDatalog+/-, an extension of the Datalog+/- family of languages with preference management formalisms closely related to those previously studied for relational databases. We focus on two kinds of answers to queries that are relevant to this setting, top-k and skyline, and develop algorithms for computing these answers to both DAQs (disjunctions of atomic queries) and CQs (conjunctive queries). We show that DAQ answering in PrefDatalog+/- can be done in polynomial time in the data complexity, as in relational databases, as long as query answering can also be done in polynomial time (in the data complexity) in
the underlying classical ontology."1419,Information fusion based learning for frugal traffic state sensing,"Traffic sensing is a key baseline input for sustainable cities to plan and administer demand-supply management through better road networks, public
transportation etc., Humans sense the environment frugally using a combination of complementary information signals from different sensors. For example, by viewing and/or hearing traffic one could identify the state of traffic in a road. Since these sensor signals and processing techniques are also affected by uncorrelated \""noise\"", the fusion of features and decisions from these sensors should result in a more accurate learning system.  In this paper, we demonstrate a fusion based learning approach to classify the traffic states using low cost audio and image data analysis using real world data set. Road side collected traffic acoustic signal and traffic image snapshots obtained from fixed camera are used to classify the traffic condition into three broad classes viz., Jam, Medium and Free. The classification is done on {10 sec audio, image snapshot in that 10 sec} data tuples. We extract traffic relevant features from audio and image data to form a composite feature vector. In particular, we extract the audio features comprising of MFCC (Mel-Frequency Cepstral Coefficients) based classifier likelihoods, honk event based classifier likelihoods, honk frame percentage and energy peaks. A simple heuristic based image classifier is used, where vehicular density and number of corner points within the road segment are estimated and are used as features for traffic sensing. Finally the composite vector is tested for its ability to discriminate the traffic classes using Decision tree classifier, SVM classifier, Discriminant classifier and Logistic regression based classifier. Information fusion at multiple levels (audio, image, overall) shows consistently better performance than individual level decision making. Low cost sensor fusion based on complementary weak classifiers and \""noisy\"" features still generate high quality results with an overall accuracy of 93 - 96%."1420,Maintaining alternative values  in  constraint-based  configuration,"Constraint programming techniques are widely used to model and solve interactive decision
problems, an especially configuration problems. In this type of application, the configurable product is described by means
of a set of constraint bearing  on the configuration variables. The user then interactively solves the CSP by assigning (and possibly, relaxing)  the configuration variables according to her preferences. The aim of the system is then to keep the domains of the other variables consistent with these choices. Since  maintaining of the global inverse  consistency is generally not tractable, the domains are instead filtered according to some level of local consistency, e.g. arc-consistency.

In the  present work, we aim at offering a more  convenient interaction by providing the user with     possible alternative   values for   each of the already assigned variables  - i.e. the values that could replace the current one without leading to  the violation of some constraint. We thus present the new concept of {\\em alternative domains } in a (possibly) partially assigned CSP. We propose a propagation algorithm that computes   all the alternative domains in a single step. Its worst case complexity is comparable with the one of  the naive algorithm that would run a full propagation for each variable, but its experimental efficiency on real interactive configuration problems  is much better "1422,Automatically Generating Problems and Solutions for Natural Deduction,"Natural deduction, which is a method for establishing validity of propositional type arguments, helps develop important reasoning skills and is thus a key ingredient in a course on introductory logic. We present two core components, namely solution generation and practice problem generation, for enabling computer-aided education for this important subject domain. The key enabling technology is use of an offline-computed data-structure called Universal Proof Graph (UPG) that encodes all possible applications of inference rules over all small propositions abstracted using their bitvector-based truth-table representation. This allows an efficient forward search for solution generation. More interestingly, this allows generating fresh practice problems that have given solution characteristics by performing a backward search in UPG. We obtained 500+ natural deduction problems from various textbooks. Our solution generation procedure can solve many more problems than the traditional forward-chaining based procedure, while our problem generation procedure can efficiently generate several variants with desired characteristics. "1433,Efficient Kernel Learning from Side Information Using ADMM,"In kernel methods,the incorporation of side information is highly useful in the learning of a nonparametric kernel. However, computationally, this often leads to an expensive semidefinite program (SDP). In recent years, a number of dedicated solvers have been proposed. Though much better than using off-the-shelf SDP solvers, they still cannot scale to large data sets. In this paper, we propose a novel solver based on the alternating direction method of multipliers (ADMM). The main idea is to use a low-rank decomposition of the kernel matrix. The resultant optimization problem, though non-convex, can be efficiently solved and has favorable convergence properties. Experimental results on a number of real-world data sets demonstrate that the proposed method is as accurate as directly solving the SDP, but can be one to two orders faster."1542,Machine-Learning-Based Circuit Synthesis,"Multi-level logic synthesis is a problem of immense practical significance, and is a key to developing circuits that optimize a number of parameters, such as depth, energy dissipation, reliability, etc. The problem can be defined as the task of taking a collection of components from which one wants to synthesize a circuit that optimizes a particular objective function. This problem is computationally hard, and there are very few automated approaches for its solution. To solve this problem we propose an algorithm, called Circuit-Decomposition Engine (CDE), that is based on learning decision trees, and uses a greedy approach for function learning. We empirically demonstrate that CDE, when given a library of different component types, can learn the function of Disjunctive Normal Form (DNF) Boolean representations and synthesize circuit structure using the input library. We compare the structure of the synthesized circuits with that of well-known circuits using a range of circuit similarity metrics.
"1460,Heuristic Subset Selection,"The construction of a strong heuristic function is a central problem in heuristic search.  This paper studies the case where a set of heuristics are combined by maximizing over their values.  Due to resource constraints, this set of heuristics is often just a subset of a larger set of possible candidates.  Several algorithms have been proposed to select good subsets, but most lack an optimality criterion or are tied to a specific type of heuristic.  We propose an optimization approach that applies to any, possibly heterogeneous, set of candidate heuristics.  We equate minimizing a natural loss function with maximizing a utility function that is submodular and monotonic.  This implies that simple greedy selection under this utility function is guaranteed to be near-optimal.  We then extend the approach with a sampling scheme that retains provable optimality.  Our empirical results show significant improvements over existing strategies and bring new insight into constructing heuristics for directed domains."1461,Human Activity Recognition using a Novel Trajectory Descriptor,"Human activity recognition is a challenging machine vision task, with applications in human-robot/machine interaction, interactive entertainment, multimedia information retrieval, and surveillance. With the availability of the depth data from cheap sensors, like Microsoft\'s Kinect, real-time skeleton joints extraction became available. In this paper, we exploit the 3D skeleton sequences obtained through a Kinect sensor to develop our recognition approach. A novel fixed-length trajectory descriptor is proposed. The descriptor consists of a hierarchical histogram of the directions in each projection of the 3D trajectory. Experiments on many datasets show that the descriptor outperforms the state of the art when using off-the-shelf classification tools."1479,Predicting Knowledge in An Ontology Stream,"Recently, ontology stream reasoning has been introduced as a multidisciplinary approach, merging synergies from Artificial Intelligence, Database and World Wide-Web to reason on semantics-augmented data streams. Although analyzing knowledge evolution and answering questions on real time events has been largely addressed in ontology streams, the challenge of predicting its future (or missing) knowledge remains open and yet unexplored. We tackle predictive reasoning as a correlation and interpretation of past semantics-augmented data over multiple ontology streams. Predictions are constructed as consistent Description Logics axioms by selecting and applying significant cross-streams association rules. The experiments have shown accurate prediction with real and live stream data from Dublin City in Ireland."1484,Decidable Reasoning in a Logic of Limited Belief with Introspection and Unknown Individuals,"There are not very many existing logics of belief which have both a perspicuous semantics and are computationally attractive. An exception is the logic \SL, proposed by Liu, Lakemeyer, and Levesque, which allows for a decidable and often even tractable form of reasoning. While the language is first-order and hence quite expressive, it still has a number of shortcomings. For one, beliefs about beliefs are not addressed at all. For another, the names of individuals are  rigid, that is, their identity is assumed to be known. In this paper, we show how both shortcomings can be overcome by suitably extending the language and its semantics. Among other things, we show that determining the beliefs of a certain kind of fully introspective knowledge bases is decidable and that a finite number of unknown individuals in the knowledge base can be accommodated in a decidable manner as well."1494,A Proof-Theoretical View of Collective Rationality,"The impossibility results in judgement aggregation show a clash between fair aggregation procedures and rational collective outcomes. In this paper, we are interested in analysing the notion of rational outcome by proposing a proof-theoretical understanding of collective rationality. In particular, we use the analysis of proofs and inferences provided by linear logic in order to define a fine-grained notion of group reasoning that allows for studying collective rationality with respect to a number of logics. We analyse the well-known paradoxes in judgement aggregation and we pinpoint the reasoning steps that trigger the inconsistencies. Moreover, we extend the map of possibility and impossibility results in judgement aggregation by discussing the case of substructural reasoning. In particular, we show that there exist fragments of linear logic for which general possibility results can be obtained. "1498,Smoothing for Bracketing Induction,"Bracketing induction is the unsupervised learning of hierarchical constituents without labeling their syntactic categories such as verb phrase (VP) from natural raw sentences. Constituent Context Model (CCM) is an effective generative model for the bracketing induction, but the CCM computes probability of a constituent in a very straightforward way no matter how long this constituent is. Such method causes severe data sparse problem because long constituents are more unlikely to appear in test set. To overcome the data sparse problem, this paper proposes to define a non-parametric Bayesian prior distribution, namely the Pitman-Yor Process (PYP) prior, over constituents for constituent smoothing. The PYP prior functions as a back-off smoothing method through using a hierarchical smoothing scheme (HSS). Various kinds of HSS are proposed in this paper. We find that two kinds of HSS are effective, attaining or significantly improving the state-of-the-art performance of the bracketing induction evaluated on standard treebanks of various languages, while the easiest coming to mind HSS by using n-gram Markovization on constituents is not effective for improving the bracketing induction performance."1511,A Lossy Counting Based Approach for Learning on Streams of Graphs on a Budget ,"In many problem settings, for example on graph domains, online learning algorithms on 
streams of data need to respect strict time constraints dictated by the throughput on 
which the data arrive. 
When only a limited amount of memory (budget) is available, a learning algorithm will 
eventually need to discard some of the information used to represent the current solution, 
thus negatively affecting its classification performance. 
More importantly, the overhead due to budget management may significantly increase the 
computational burden of the learning algorithm. 
In this paper we present a novel approach inspired by the Passive Aggressive and 
the Lossy Counting algorithms. 
Our algorithm uses a fast procedure for deleting the less influential features. 
Moreover, it is able to estimate the weighted frequency of each feature and use it
for prediction. 
Experiments on real-world datasets show that the proposed approach 
achieves state-of-the-art classification performances, while being much faster 
than existing algorithms."1520,Social Collaborative Topic Regression for Celebrity Recommendation,"Recently a new application for recommendation systems has appeared on the social network websites, such as Twitter.com and t.qq.com. Unlike the items recommended in traditional recommendation systems, some items recommended in the social networks are playing the user's role in the recommendation system. For example, celebrities or famous groups recommended to the public are also the users involved in the social network. So how to recommend those celebrity users to the public becomes an interesting problem. In this paper, we proposed a unified hierarchical Bayesian model to recommend those celebrities to the massive common users. Specifically, we proposed to leverage the celebrity's connection network to improve the prediction ability of user's acceptance for celebrities. In our model, we combine topic model with social network matrix factorization and user acceptance matrix factorization. It works by regularizing item factors through item's social network and the bag of words associated with each item. We also proposed to incorporate different confidence for different dyadic context. We study a large subset of data from t.qq.com, the largest microblog website in China and show that our model achieves a higher performance and provides a more effective algorithm than the state-of-art methods. We have also found that if a common user followed a celebrity, he will be more interested in those celebrities who have connection with the one this user has followed especially when the user is a beginner. We show that our model can solve the cold start problem for both item-side and user-side existing in the area of collaborative filtering by using the celebrities' social network."1590,Exploring Knowledge Engineering Strategies in Designing &amp; Modelling Road Traffic Accident Management,"Formulating knowledge for use in AI Planning engines is currently somewhat of an ad-hoc process, where the skills of knowledge engineers and the tools they use may significantly influence the quality of the resulting planning application. There is little in the way of guidelines or standard procedures, however, for knowledge engineers to use when formulating knowledge into planning domain languages such as PDDL. This paper seeks to investigate this process using as a case study domain of road traffic accident management.
 Managing road accidents requires systematic, sound planning and coordination of resources to improve outcomes for accident victims. We have derived a set of requirements in consultation with stakeholders for the resource coordination part of managing accidents. We evaluate two separate knowledge engineering strategies for encoding the resulting planning domain from the set of requirements: (a) the traditional method of a PDDL expert and text editor, and (b) a leading planning GUI with built in UML modelling tools.
 These strategies are evaluated using product and process metrics, where the domain model (the product) was tested extensively with a range of planning engines. The results give insights into the strengths and weaknesses of the approaches, highlight lessons learned regarding knowledge encoding, and point to important lines of research for knowledge engineering for planning.
"1532,Bootstrap learning via modular concept discovery,"Suppose a learner is faced with a domain of problems about which it knows nearly nothing. It does not know the distribution of problems, the space of solutions is not smooth, and the reward signal is uninformative, providing perhaps a few bits of information but not enough to steer it effectively. How can such a learner ever get off the ground? A common intuition is that if the solutions to these problems share a common structure, and the learner can solve some simple problems by brute force, it should be able to extract useful components from these solutions and, by composing them, explore the solution space more efficiently. Here, we formalize this intuition, where the solution space is the space of typed functional programs and the gained information is stored as a stochastic grammar over programs. We propose an iterative procedure for exploring such spaces: in the first step of each iteration, the learner explores a finite subset of the domain, guided by a stochastic grammar; in the second step, the learner compresses the successful solutions from the first step to estimate a new stochastic grammar. We test this procedure on symbolic regression and boolean circuit learning and show that the learner discovers modular concepts for these domains. Whereas the learner is able to solve almost none of the posed problems in the procedure's first iteration, it rapidly becomes able to solve a large number by gaining abstract knowledge of the structure of the solution space."1535,Concept Learning for Cross-domain Text Classification: a General Probabilistic Framework,"Cross-domain learning targets at leveraging the knowledge from
source domains to train accurate models for the test data from
target domains with different but related data distributions. To
tackle the challenge of data distribution difference in terms of raw
features, previous works proposed to mine high-level concepts
(e.g., word clusters) across data domains, which shows to be more
appropriate for classification. However, all these works assume that
the same set of concepts are shared in the source and target domains
in spite that some distinct concepts may exist only in one of
the data domains. Thus, we need a general framework, which can
incorporate both shared and distinct concepts, for cross-domain
classification. To this end, we develop a probabilistic model, by
which both the shared and distinct concepts can be learned by the EM
process which optimizes the data likelihood. To validate the
effectiveness of this model we intentionally construct the
classification tasks where the distinct concepts exist in the data
domains. The systematic experiments demonstrate that our model gains
37.67% average accuracy improvement over the traditional
classification algorithm, on these much more challenging tasks."1545,Online Community Detection for Very Large Complex Networks,"Complex networks describe a wide range of systems in nature and society. To understand the complex networks, it is crucial to investigate their internal
structure. In this paper, we consider the generative mechanism of complex networks and propose an online community detection method for very large complex networks. Our method has O(m) time complexity and O(nk) space complexity, it can handle real-world networks with millons of edges in seconds and outperform other feasible methods."1550,Monte-Carlo Expectation Maximization for Decentralized POMDPs,"We address in this work two significant drawbacks of state-of-the-art solvers of decentralized POMDPs (DEC-POMDPs): the reliance on complete knowledge of the model and the limited scalability as the complexity of the domain grows.  We extend a recently proposed approach for solving DEC-POMDPs via a reduction to the maximum likelihood problem, which is then solved using EM.  We introduce a model-free version of this approach that employs Monte-Carlo EM (MCEM).  While a naive application of MCEM is inadequate in multi-agent settings, we introduce several improvements in sampling that produce high-quality results on a variety of DEC-POMDP benchmarks."1561,Low-Rank Coding with b-Matching Constraint for Semi-supervised Classification,"Graph based semi-supervised learning (SSL) methods play an important role in practical machine learning systems. In this paper, we propose a novel graph construction method based on low-rank coding and b-matching constraint. Our graph seeks compact representation for each sample, and simultaneously constructs a regular graph which benefits label propagation in SSL. Then we present a semi-supervised learning method by incorporating this graph with Gaussian harmonic function (GHF). Experimental results on a toy data set, the USPS, Extended YaleB and ORL databases demonstrate that our graph outperforms the state-of-the-art graphs, especially when the labeled samples are very limited. (To be revised soon)"1562,Data Repair of Inconsistent DL-programs,"Description Logic (DL) programs are a prominent approach for loose
coupling of a Description Logic knowledge base (alias, ontology) with
nonmonotonic logic programming, in order to support rule-based reasoning
on top of ontologies, using a well-defined query interface. They in
particular allow to combine inferences from the ontology, and a
bidirectional information flow between the rules and the ontology makes
it possible to have adaptive combinations and solve advanced reasoning
problems. However, the interaction may cause inconsistency in that no
answer set, i.e., no model, of a DL-program exists. To remedy this
problem, i.e., in order to restore consistency, we consider the repair
of DL-programs, taking the view that the data in the underlying ontology
might not be fully adequate. Therefore, repair answer sets are defined
in terms of changes to (repair of) the data part of the ontology. We
analyze the computational complexity of the notion, and present results
that allow current algorithms for computing answer sets of DL-programs
to be extended to compute repair answer sets. In the course of this, we
encounter a novel ontology repair problem as a subtask, in which the
entailment and non-entailment of a set of queries to the ontology has to
be guaranteed. While this problem is unsurprisingly intractable in
general, we identify useful classes of repairs in practice that are
tractable for the well-known Description Logic DL-Lite, and which can be
computed by the extended algorithm."1643,Multi-dimensional Single-peaked Consistency and its Approximations,"Single-peakedness is the most common domain restriction used in social choice theory, and allows the use of mechanisms with desirable properties. However, it is not clear to what extent agent preferences are single-peaked in practice, and whether recent proposals for approximate single-peakedness (in one-dimensional settings) help in this regard.  We address this issue by assessing the ability of both single-dimensional and multi-dimensional approximations to explain preference profiles drawn from several real-world elections.  We develop a simple \emph{branch-and-bound} algorithm for finding multi-dimensional single-peaked axes that best fit a given profile, and that works with several forms of approximation. Empirical results show that agent preferences in these elections are far from single-peakedness in any one-dimensional space, but are approximately single-peaked in two-dimensions.  Our algorithms are quite efficient as well, despite the NP-completeness of finding one-dimensional single-peaked approximations, and show excellent anytime performance."1570,Defender (Mis)coordination in Security Games,"We study security games with multiple defenders. To achieve maximum security, defenders must perfectly synchronize their randomized allocations of resources. However, in real-life scenarios (such as protection of the port of Boston) this is not the case. Our goal is to quantify the loss incurred by miscoordination between defenders, both theoretically and empirically. We introduce two notions that capture this loss under different assumptions: the price of miscoordination, and the price of sequential commitment. Generally speaking, our theoretical bounds indicate that the loss may be extremely high in the worst case, while our simulations establish a smaller yet significant loss in practice."1571,End-to-End Coreference Resolution for Clinical Narratives,"Coreference resolution is the problem of clustering mentions into entities and is very critical for natural language understanding. This paper studies the problem of coreference resolution in the context of the newly emerging domain of Electronic Health Records (EHRs). We used a variant of best-link decoding strategy in our coreference model. Our decoding strategy allows us to use several constraints while selecting the best antecedent. Most of our constraints are based on the context in which the mentions appear. In this paper, we also show that different pronouns behave quite differently while doing coreference resolution. So, we developed different modules for resolving different pronouns. Our method for pronominal resolution and our use of constraints gave us significant performance improvements. To the best of our knowledge, we report the best results on the corpora used by us. Finally, we also report the results for end-to-end coreference resolution. As far as we know, end-to-end results have not been reported previously on the corpora used by us.
"1572,Assessing the resilience of bilingual societies: coupling viability and active learning with kd-tree,"Assessing the resilience of social or ecological systems has become a very important scientific issue, since preserving biodiversity, natural resources and human diversity is considered as urgent stakes. Considering the problem of language coexistence, although bilingual societies do exist, many languages have disappeared in the past and many seem endangered presently. Mathematical models of language coexistence generally conclude that one language will disappear, except when the prestige of one language compared to the other can be modified. As it is done for ecosystems we define the resilience of this kind of society as its ability to recover after a perturbation. The mathematical viability theory provides concepts and methods that are suitable to study the resilience when a set of differential equations is available to describe the evolution of the system. However, computational problems arise very quickly with the number of state variables or when the model of the dynamics is difficult to run. In order to increase the dimension of the problems that can be studied in the viability framework, we consider the computation of the capture basin as a particular active learning problem, so that the calls to the model can be restrained. In this purpose we adapt a kd-tree algorithm  to approximate the boundary of the capture basin, which provides the level sets of the resilience value. This resilience value can then be used to propose a policy of action in risky situations such as migration flows."1574,Control in the Presence of Manipulators: Cooperative and Competitive Cases,"Control and manipulation are two of the most studied types of attacks on elections.  In this paper, we study the complexity of control attacks on elections in which there are manipulators.  We study both the case where the ""chair"" who is seeking to control the election is allied with the manipulators, and the case where the manipulators seek to thwart the chair.  In the latter case, we see that the order of play matters substantially regarding the complexity.  We prove upper bounds, that hold over every election system with a polynomial-time winner problem, for all standard control cases, and some of these bounds are at the second or third level of the polynomial hierarchy.  Nonetheless, for important natural systems the complexity can be much lower.  For example, we prove that for plurality, Condorcet, and approval elections, the complexity of even competitive clashes between a controller and manipulators falls far below those high bounds, even as low as polynomial time."1575,"Semiring Labelled Decision Diagrams, Revisited: Canonicity and Spatial Efficiency Issues","Existing languages of valued decision diagrams (VDDs), including ADD, AADD and those of the SLDD family, prove valuable target languages for compiling mappings which associate valuations with assignments of values to variables. Among other operations, conditioning and optimization can typically be achieved in polynomial time from such diagrams. However, their efficiency is directly related to the size of the compiled representations. The notion of succinctness is a key to compare the spatial efficiency of representation languages from the theoretical side. In practice, the existence of canonical forms may also have a major impact on the size of the compiled VDDs. While efficient normalization procedures have been pointed out for ADD and AADD, the canonicity issue for SLDD representations has not been addressed so far.
In this paper, the SLDD family is revisited. We modify the algebraic requirements imposed on the corresponding valuation structure so as to ensure tractable optimization and normalization for revisited SLDD representations. We show that AADD is captured by the revisited family. We provide a number of succinctness results relating members of the revisited family with ADD and AADD. We finaly report some results from experiments where we compiled some instances of an industrial configuration problem into each of those languages, which enables us to compare their spatial efficiency from the practical side."1577,Multi Class Learning with Individual Sparsity,"Multiclass problems are everywhere, given an input the goal is to predict one of few possible classes. Most previous work reduced learning to minimization the empirical loss over some training set, and an additional regularization term, prompting simple models, or some other prior knowledge. A common learning regularization promotes sparsity, that is, small models or small number of features, as performed in group LASSO. Yet, these assumption do
not always hold. In some problems, for each class, there is a small set of features that represents it well, yet the union of these sets is not small. We propose to use other regularizations that promotes this type of sparsity, analyze the generalization property of such formulations, and show empirically that indeed, these regularizations not only perform well, but also promote such sparsity structure."1579,Fast Linearization of Tree Kernels over Large-Scale Data,"Convolution tree kernels have been success- fully applied to many language processing tasks for achieving state-of-the-art accuracy. Unfortunately, kernels require working in the dual space, which renders large-scale learning impractical. In this paper, we study the lat- est approaches to solve such problem ranging from feature hashing to reverse kernel engi- neering and approximate cutting plane train- ing with model compression. We derive a method that relies on reverse-kernel engineer- ing together with an efficient kernel learning method. The approach gives the advantage of using tree kernels to automatically gener- ate rich structured feature spaces and working in the linear space where learning and testing is fast. We experimented with training sets up to 4 million examples from Semantic Role Labeling. The results show that (i) structural features are essential and (ii) we can speed-up training from weeks to less than 20 minutes."1581,The GoDeL Planning System: A more perfect union of Domain-Independent and Hierarchical Planning,"One drawback of Hierarchical Task Network (HTN) planning is the difficulty of providing complete domain knowledge, i.e., a complete and correct set of HTN methods for every task. To provide a principled way to overcome this difficulty, we define a formalism for Partial-Knowledge Goal Network (PGN) planning, and a planning algorithm based on this formalism.

Like HTN planning, PGN planning includes hierarchical decomposition using methods. But PGN methods specify ways to achieve goals rather than tasks, and goals may be achieved even if the domain knowledge is incomplete or nonexistent (i.e., if there isn't a complete set of methods for every goal).

Our planning algorithm, GoDeL (Goal Decomposition using Landmarks), has several desirable theoretical guarantees, including soundness and completeness irrespective of whether the domain knowledge (i.e., the set of methods given to the planner) is complete. By comparing GoDeL's performance with varying amounts of domain knowledge across three benchmark planning domains, we show experimentally that (1) GoDeL works correctly with partial planning knowledge, (2) GoDeL's performance improves as more planning knowledge is given, and (3) when given full domain knowledge, GoDel matches the performance of state-of-the-art hierarchical planners."1587,Agent Failures in All-Pay Auctions,"All-pay auctions, a common mechanism for various human and agent interactions, suffers, like many other mechanisms, from the possibility of players' failure to participate in the auction. We model such failures and show how they effect the equilibrium state, revealing various properties, such as the lack of influence of the most-likely-to-participate player on the behavior of the other players. We perform this analysis with two scenarios: the sum-pro&amp;#64257;t model, where the auctioneer obtains the sum of all submitted bids, and the max-pro&amp;#64257;t model of crowdsourcing contests, where the auctioneer can only use the best submissions and thus obtains only the winning bid.

Furthermore, we examine various methods of influencing the probability of participation: the effects of misreporting one's own probability of participating; how influencing another player's participation chances (e.g., sabotage) changes the player's strategy --- and what player should the saboteur aim for; and we look into the value of changing everyone's participation probabilities (e.g., weather effects)."1596,Active Learning with Multi-label SVM Classification,"Multi-label classification, where each instance is assigned to multiple categories, is a prevalent problem in data analysis. However, annotations of multi-label data are typically more difficult, time-consuming or expensive to obtain than collecting labels for standard single-label data. To reduce the labeling effort, multi-label active learning is a critical tool to be investigated. Though active learning has been widely studied for single-label problems, current research on multi-label active learning remains in a preliminary state. In this paper, we propose two novel multi-label active learning instance selection criteria, a max-margin prediction uncertainty criterion and a label cardinality inconsistency criterion, and integrate them into an adaptive framework of multi-label active learning. Our empirical results on multiple multi-label datasets demonstrate the efficacy of the proposed active instance selection criteria and the integrated active learning approach."1597,Optimal Airline Ticket Purchasing Using Automated User-Guided Feature Selection,"Airline ticket purchase timing is an ubiquitous strategic problem that requires both historical data and domain knowledge to solve consistently.  Even with some historical information (often a feature of modern travel reservation web sites), it is difficult to measure a cost-minimization strategy's performance in a scientific way.  To address this problem, we introduce an automated agent which is able to optimize purchase timing on behalf of customers and provide performance estimates of the agentâ€™s computed action policy based on past performance.

The proposed technique applies machine learning to data containing recent ticket price quotes from many competing airlines for the target flight route.  Our novelty lies in extending this using a systematic feature extraction technique incorporating elementary user-provided domain knowledge that greatly enhances the performance of known machine learning algorithms for this kind of large multivariate, temporal domain. Using this technique, our agents are able to achieve much closer to the optimal purchase policy than other proposed decision theoretic approaches for this domain. This approach meets or exceeds the performance of existing feature selection methods from the literature. Applications to other domains for this feature selection process are also discussed."1599,TutorialPlan: Automated Tutorial Generation from CAD Drawings,"Authoring tutorials for complex software applications is a time consuming process. It also highly depends on the tutorial designerâ€™s skill level and experience. This paper introduces an approach which automatically generates software tutorials from digital artifacts produced by the software. We model this process as an optimal planning problem using software produced artifacts, software specifications and the Keystroke-level model (KLM) for human-computer interactions. We present TutorialPlan, an automated tutorial generator, which creates step-by-step text and image instructions from CAD drawings and helps users learn AutoCAD, a complex design and drafting software. In our tutorial generator, the optimal planning problem is represented and solved using DLV, an efficient Answer Set Programming (ASP) system. DLV offers a natural representation of both the problem and the heuristics needed to solve it efficiently. Study shows that the tutorials generated by our system are comparable to those generated by experienced AutoCAD users."1600,Bayesian Optimization in High Dimensions via Random Embeddings,"Bayesian optimization techniques have been successfully applied to robotics, planning, sensor placement, recommendation, advertising, intelligent user interfaces and automatic algorithm configuration. Despite these successes, the approach is restricted to problems of moderate dimension, and several workshops on Bayesian optimization have identified its scaling to high-dimensions as one of the holy grails of the field. In this paper, we introduce a novel random embedding idea to attack this problem. The resulting Random EMbedding Bayesian Optimization (REMBO) algorithm is very simple and applies to domains with both categorical and continuous variables. Our experiments demonstrate that REMBO can effectively solve high-dimensional problems, including the optimization of a 2-dimensional test function embedded in a billion dimensions and the optimization of 47 categorical parameters of a popular mixed integer linear programming solver."1603,An Active Learning Approach to Home Heating in the Smart Grid,"A key issue for the realization of the smart grid vision is the implementation of efficient demand-side management. One possible mechanism for this is to expose dynamic energy prices to end-users. In this paper, we consider a resulting problem on the end-user's side: how to adaptively heat a home given dynamic prices. The user faces the challenge of having to react to dynamic prices in real time, trading off his comfort with the cost of heating his home to a certain temperature. We propose an active learning approach to adjust the home temperature in a semi-automatic way. Our algorithm learns the user's preferences over time and automatically adjusts the temperature in real-time as prices change. In addition, the algorithm asks the user for feedback once a day.
To find the best query time, the algorithm solves an optimal stopping problem, to minimize the user's expected squared loss of utility. Via simulations, we show that our algorithm learns the user's preferences quickly, is robust to a large degree of noise, and our query criterion outperforms other approaches from the active learning literature."1605,Bimodal Switching for Online Planning in Multiagent Settings,"We present a bimodal method for online planning in partially observable multiagent settings as formalized by finitely-nested interactive partially observable Markov decision process. The approach is significantly faster on larger environments than the previous online planning approach. An agent planning in an environment shared with another updates beliefs both over the physical state and the other agentsâ€™ models. In problems where we do not observe otherâ€™s actions explicitly but must infer it from sensing its effect on the state, observations are more informative on the other agent when the belief over the state space has reduced uncertainty. For initial beliefs with high uncertainty about the state, we model the agent as if it were acting alone and utilize fast online planning for POMDPs. Subsequently, the agent switches to online planning in multiagent settings. We maintain lower and upper bounds at each step that are computed quickly, and switch over when the difference between them reduces to less than &amp;#491;. If the online planning is performed exactly, we may bound the error of this approach. We show favorable results in a large UAV reconnaissance problem.
"1618,Probabilistic Multi-label Classification with Sparse Feature Learning,"In this paper we propose a probabilistic multi-label classi&amp;#64257;cation model based on
novel sparse feature learning. By employing an individual sparsity inducing L1 -norm and a group sparsity inducing L2,1 -norm, the proposed model has the capacity of capturing both label interdependencies and common predictive model structures. We formulate this sparse norm regularized learning problem as a non-smooth convex optimization problem, and develop a fast proximal gradient algorithm to solve it for an optimal solution. Our empirical study demonstrates the e&amp;#64259;cacy of the proposed method on a set of multi-label tasks given a limited number of labeled training instances."1621,Tractable Approximations of Consistent Query Answering for Robust Ontology-based Data Access,"A robust system for ontology-based data access should provide meaningful answers to queries even when the data conflicts with the ontology. This can be accomplished by adopting an inconsistency-tolerant semantics, with the consistent query answering (CQA) semantics being the most prominent example. Unfortunately, query answering under the CQA semantics has been shown to be computationally intractable, even when extremely simple ontology languages are considered. In this paper, we address this problem by proposing two new families of inconsistency-tolerant semantics which approximate the CQA semantics from above (complete approximations) and from below (sound approximations) and converge to the CQA semantics in the limit. We study the data complexity of conjunctive query answering under these new semantics and show a general tractability result for the whole class of first-order rewritable ontology languages, which includes, among others, several description logics of the DL-Lite family and rule-based languages of the Datalog+/- family. We also analyze the combined complexity of query answering for ontology languages of the DL-Lite family.
"1633,Analysis and Optimization of Multi-dimensional Percentile Mechanisms,"We consider the mechanism design problem for agents with single-peaked preferences over multi-dimensional domains when multiple alternatives can be chosen. Facility location and committee selection are classic embodiments of this problem. We propose a class of \emph{percentile mechanisms}, a form of generalized median mechanisms, that are strategy-proof, and derive worst-case approximation ratios for social cost and maximum load for $L_1$ and $L_2$ cost models. More importantly, we propose a sample-based framework for optimizing the choice of percentiles relative to any prior distribution over preferences, while maintaining strategy-proofness. Our empirical investigations, using social cost and maximum load as objectives, demonstrate the viability of this approach and the value of such optimized mechanisms \emph{vis-\`{a}-vis} mechanisms derived through worst-case analysis."1634,Kemeny Elections with Bounded Single-peaked or Single-crossing Width,"This paper is devoted to complexity results regarding specific measures of proximity to single-peakedness and single-crossingness, called ""single-peaked width"" and ""single-crossing width"" [Cornaz et al., 2012]. Thanks to the use of the PQ-tree data structure [Booth and Lueker, 1976], we show that boths problems are polynomial time solvable in the general case (while it was only known for single-peaked width and in the case of \emph{narcissistic preferences}). Furthermore, we establish one of the first results (to our knowledge) concerning the effect of nearly single-peaked electorates on the complexity of an NP-hard voting system, namely we show the fixed-parameter tractability of Kemeny elections with respect to the parameters ""single-peaked width"" and ""single-crossing width""."1636,Recommendation Using Textual Opinions,"Customers can share reviews of products
and services, which are used to provide rankings of their quality,. However this ranking is
not personalized to reflect the specific needs of a particular user.
We investigate how the information in reviews written by a particular
user can be used to personalize this ranking. We build profiles from users' review texts and use these
profiles to analyze review texts with the eyes of this user. We show
that we can obtain a ranking that more accurately reflects the user's own ranking, and
would thus be a more useful recommendation.
Moreover, we show that a rating prediction evaluation is not suitable, and we propose two alternatives. 
A first uses the connection between the rating attached to a review and its accompanying text. 
A second employs solely the rating differences from a single user to establish preferences.
"1646,Interactive Value Iteration for Markov Decision Processes with Unknown Rewards,"To tackle the potentially hard task of defining the reward function in an Markov Decision Process, we propose in this paper a new approach, based on Value Iteration, consisting in interweaving the elicitation and optimization phases.
  In our work, we assume that rewards whose numeric values are unknown can only be ordered and a tutor is present to help comparing pairs of reward sequences.
  We first prove that the set of possible reward functions for a given preference relation over policies is a polyhedral cone. 
  By normalizing rewards, possible reward functions can be represented as a polytope.
  The new algorithm we proposed, called Interactive Value Iteration, searches for an optimal policy while refining its knowledge about possible reward functions, represented as a polytope, by querying a tutor when necessary.
  We prove that the number of needed queries before finding an optimal policy is upperbounded by a polynomial in the size of the problem and we present experimental results that demonstrate the efficiency of our approach."1658,Fault-Tolerant Planning under Uncertainty,"A fault represents some erroneous operation of a system that could result from an action selection error or some abnormal condition.  We formally define error models that characterize the likelihood of various faults and consider the problem of fault-tolerant planning, which optimizes performance given an error model.  We show that factoring the possibility of errors significantly degrades the performance of stochastic planning algorithms such as LAO*, because the number of reachable states grows dramatically.  We introduce an approach to plan for a bounded number of faults and analyze its theoretical properties.  When combined with a continual planning paradigm, the k-fault-tolerant planning method can produce near-optimal performance, even when the number of faults exceeds the bound.  Empirical results in two challenging domains confirm the effectiveness of the approach in handling different types of runtime errors."1659,Multi-agent Team Formation - Diversity Beats Strength?,"Team formation is an important aspect when using a multi-agent system to solve a complex problem. However, when selecting agents for a team, should we focus on the diversity of the team or the strength of each individual member? Can a team of diverse (and weak) agents outperform a team of similar but strong agents? In this paper we propose a new model for diversity and strength that allows us to analyze a team of agents that cooperate to solve a complex problem by voting. We show the necessary conditions for a diverse team to overcome a uniform team, and the optimal voting rule for a team. We present synthetic experiments that demonstrate that both diversity and strength contribute to the effectiveness of a team. Finally, we show real experiments in one of the most difficult challenges for artificial intelligence: Computer Go."1669,Verification of Inconsistency-Tolerant Knowledge and Action Bases,"Description Logic Knowledge and Action Bases (KABs)
have been recently introduced as a mechanism that provides a
semantically rich representation of the information on the domain of
interest in terms of a DL KB and a set of actions to change such
information over time, possibly introducing new objects. In this
setting, decidability of verification of sophisticated temporal properties over KABs,
expressed in a variant of first-order mu-calculus, has been shown.
However, the established framework assumes a rather simple treatment of inconsistency resulting as an
effect of action execution: inconsistent states are simply
rejected. In general, this is not satisfactory, since the
inconsistency may affect just a small portion of the entire KB, and
should be treated in a more careful way. In
this paper, we address this problem by resorting to
inconsistency-tolerant semantics, which have been extensively studied
in the context of DL instance-level evolution. In particular, we show
how inconsistency handling based on the notion of repairs can be
integrated into KABs, dealing at the level of the
temporal verification language with quantification over repairs, that is
typical of inconsistency-tolerant semantics. In this setting, we establish decidability and complexity of verification.
We then show how in this approach the
temporal properties can be enriched with meta-level requests about the sources of
inconsistency, and we extend decidability and complexity of
verification accordingly."1673,Multi-dimensional Causal Discovery,"We propose a method for learning causal relations within high-dimensional data in the form of multi-variate time-series as they are typically recorded in non-experimental databases. The method allows the simultaneous inclusion of numerous dimensions within the data analysis such as samples, time and domain variables construed as tensors. In such tensor data our method exploits and integrates results from non-Gaussian models and tensor analytic algorithms in a novel way. The work proves that we can determine simple causal relations independently of how complex the dimensionality of the data is. More specifically, we rely on a statistical decomposition that flattens higher dimensional data tensors into matrices. This decomposition preserves the causal information and is therefore suitable to be included in the structure learning process of causal graphical models, where a causal relation can be generalised beyond dimension, for example, over all points in time. Related methods either focus on a set of samples for instantaneous effects or look at one sample for effects at certain points in time. We evaluate the resulting algorithm and discuss its performance both with synthetic and real-world data."1674,Bounded Programs: A New Decidable Class of Logic Programs with Function Symbols,"While function symbols are widely acknowledged as an important feature in logic programming, they make common inference tasks undecidable. To cope with this problem, recent research has focused on identifying classes of programs imposing restrictions on the use of function symbols, but guaranteeing decidability of inference tasks. This has led to several criteria, called termination criteria, providing sufficient conditions for a program to admit finitely many finite stable models.
This paper introduces the new class of ``bounded programs'' guaranteeing the existence of finitely many finite stable models for programs in the class. The class of bounded programs is decidable and strictly includes the classes determined by current termination criteria. Different results on the correctness, the expressiveness, and the complexity of the class of bounded programs are presented."1691,Flexible Execution of Partial Order Plans With Temporal Constraints,"We propose a unified approach to plan execution and schedule dispatching that converts a plan, which has been augmented with temporal constraints, into a policy for dispatching. Our approach generalizes the original plan and temporal constraints so that the executor need only consider the subset of state that is relevant to successful execution of a large multitude of valid plan fragments. We can accommodate a variety of calamitous and serendipitous changes to the state of the world by supporting the seamless re-execution or omission of plan fragments, without the need for costly replanning. Our methodology for plan generalization and online dispatching is a novel combination of plan execution and schedule dispatching techniques. We demonstrate the effectiveness of our method through a prototype implementation and a series of experiments."1696,Probabilistic Constraint Logic Programming,"Probabilistic logics combine the expressive power of logic with the ability to reason with uncertainty. Several probabilistic logic languages have been proposed in the past, each of them with their own features. In this paper, we propose a new probabilistic constraint logic programming language, which combines constraint logic programming with probabilistic reasoning. The language supports the modeling of discrete as well as continuous probability distributions by expressing constraints on random variables. We introduce the declarative semantics of this language, present an exact inference algorithm to derive bounds on the joint probability distribution, and give experimental results. The results obtained are encouraging, indicating that exact inference using this language is feasible for real-world problems."1704,Revenue Maximization via Hiding Item Attributes,"We study probabilistic single-item second-price auctions where the item is
characterized by a set of attributes. The seller knows the actual instantiation
of all the attributes, but he may choose to reveal only a subset of these
attributes to the bidders.  Our model is an abstraction of the following Ad
Auction scenario. The website (seller) knows the demographic information of its
impressions, and this information is in terms of a list of attributes (e.g.,
age, gender, country of location). The website may hide certain attributes from
its advertisers (bidders) in order to create thicker market, which may lead
to higher revenue. We study how to hide attributes in an optimal way.  We show
that it is NP-complete to solve for the optimal attribute hiding scheme.  We
then derive a polynomial-time solvable upper bound on the optimal revenue.
Finally, we propose two heuristic-based attribute hiding schemes.  Experiments
show that revenue achieved by these schemes is close to the upper bound."1720,Causal Belief Decomposition for Planning with Sensing: Completeness and Practical Approximation,"Belief tracking is a key problem in planning with sensing. While the problem is intractable, it has recently been shown that for both deterministic and non-deterministic systems expressed in compact form, it can be done in time and space that are exponential in the problem width. The width measures the maximum number of state variables that are all relevant to a given precondition or goal. In this work, we extend this result both theoretically and practically. First, we introduce an alternative decomposition scheme and algorithm with the same time complexity and slightly different completeness guarantees, but whose space complexity is often much smaller: exponential in the causal width of the problem that measures the number of state variables that are all causally relevant to a given precondition, goal, or observable. Second, we introduce a fast, meaningful, and powerful approximation that trades completeness by speed, and is both time and space exponential in the problem\'s causal width. It is then shown empirically that the algorithm combined with simple heuristics yields state-of-the-art real-time performance in domains with high widths but low causal widths such as Minesweeper, Battleship, and Wumpus."1723,An Admissible Heuristic for SAS Planning Obtained from the State Equation,"Domain-independent optimal planning has seen important breakthroughs in recent years in the development of tractable and informative admissible heuristics for planning, suitable for planners based on forward state-space search. These heuristics allow planners to optimally solve an important number of benchmark problems, including problems that are quite involved and difficult for the layman (even when the optimality requirement is dropped). In this paper we present a new admissible heuristic that is obtained from the state equation associated to the Petri-net representation of the planning problem. The new heuristic, that does not fall into one of the four standard classes of heuristics, can be computed in polynomial time and is competitive with the current state of the art for optimal planning, as empirically demonstrated over a large number of benchmark problems, mainly because it often shows an improved quality-cost ratio. The new heuristic is defined for planning tasks that are expressed in SAS+ and that may have arbitrary non-negative action costs."1734,Online Expectation Maximization for Reinforcement Learning in POMDPs,"We present an online expectation-maximization (EM) algorithm for learning the regionalized pol- icy representation (RPR), a parametric policy for model-free reinforcement learning in POMDPs. In the E-step, the algorithm performs a partial evaluation of the policy based on the most recent episodes, updating a noisy objective function; in the M-step, the algorithm improves the policy by maximizing the updated objective function. A thorough analysis is given for the convergence of the algorithm. The algorithm is demonstrated on several bench- mark POMDP problems."1737,On Computing Minimal Correction Subsets,"A set of constraints that cannot be simultaneously satisfied is over-constrained. Minimal relaxations and minimal explanations for over-constrained problems find many practical uses. For Boolean formulas, minimal relaxations of over-constrained problems are referred to as Minimal Correction Subsets (MCSes). MCSes find many applications, including the enumeration of MUSes. Existing approaches for computing MCSes either use a Maximum Satisfiability (Max-SAT) solver or iterative calls to a Boolean Satisfiability (SAT) solver. This paper shows that existing algorithms for MCS computation can be inefficient, and so inadequate, in certain practical settings. To address this problem, this paper develops a number of novel techniques for improving the performance of existing MCS computation algorithms. More importantly, the paper proposes a novel algorithm for computing MCSes. Both the techniques and the algorithm are evaluated empirically on representative problem instances, and are shown to yield the most efficient and robust solutions for MCS computation."1738,Tractable Constrained Optimization Problems on Ordered Domains,"In this paper, we identify a tractable class of constrained optimization problems (COPs) defined on variables with ordered domains. A COP consists of both a satisfaction component (""hard"" constraints), as well as an optimization component (""soft"" constraints). In the tractable class of COPs identified here, the hard constraints exhibit a ""left-staircase"" form, while the soft constraints specify for each variable a preference or a cost function defined on its domain. We reduce a given instance of this class to a max-flow problem staged on a graphical representation of the hard constraints. Our techniques yield the first known polynomial-time algorithm for solving left-staircase constraints with preferences or costs. This positive result is in the context of well known negative results associated with many closely related COPs."1749,Supervised Hypothesis Discovery Using Syllogistic Patterns in the Biomedical Literature,"The ever-growing literature in biomedicine makes it virtually
impossible for individuals to grasp all the information relevant to
their interests.  Since even experts' knowledge is limited,
important associations among key biomedical concepts may remain
unnoticed in the flood of information.  Discovering those implicit,
hidden knowledge is called hypothesis discovery.  This
paper reports our approach to hypothesis discovery, which takes
advantage of a syllogistic chain of relations extracted from
published knowledge (i.e., scientific literature).  We consider such
chains of relations as implicit patterns or rules which can be used
to generate potential hypotheses.  The generated hypotheses are then
compared with newer knowledge for examining their correctness and,
if confirmed, they are served as positive examples for learning a
hypothesis discovery model.  We implement this framework of
supervised hypothesis discovery and experiment on
real-world knowledge from the biomedical literature.  The results
demonstrate the validity of the proposed framework in generating and
identifying true hypotheses.
"1760,Optimally Solving Dec-POMDPs as Continuous-State MDPs,"Optimally solving decentralized partially observable Markov decision processes (Dec-POMDPs) is a hard combinatorial problem. Current algorithms search through the space of full histories for each agent. Because of the doubly exponential growth of the full history space, these methods quickly become intractable. However, in real world problems, computing policies over the full history space is often unnecessary. True histories experienced by the controller often lie near a structured, low-dimensional manifold embedded into the full history space. We show that by transforming a Dec-POMDP into a continuous-state MDP, we are able to find and exploit these low-dimensional representations. Using this novel transformation, we can now employ powerful techniques for solving POMDPs and continuous-state MDPs. By combining a general search algorithm and dimension reduction based on feature selection methods, we are able to solve problems of an order of magnitude larger than those commonly used in the literature."1775,Control Complexity of Schulze Voting,"Schulze voting is a recently introduced voting system enjoying unusual
popularity and a high degree of real-world use, with users including
the Wikimedia foundation, several branches of the Pirate Party, and
MTV.  It is a Condorcet voting system that determines the winners of
an election using information about paths in a graph representation of
the election.  We fully characterize the worst-case behavior of
Schulze voting under control.  We find that it falls short of the best
known voting systems in terms of control resistance, demonstrating
vulnerabilities of concern to some prospective users of the system.
"1776,Rolling Dispersion for Robot Teams,"Dispersing a team of robots into an unknown and dangerous environment, such as a collapsed building, can provide information about structural damage and
locations of survivors and help rescuers plan their actions. We propose a rolling dispersion algorithm, which makes use of a small number of robots and achieves full exploration. The robots disperse as much as possible while maintaining communication, and then advance as a group, leaving behind beacons to mark explored areas and provide a path back to the entrance.  The novelty of this algorithm comes from manner in which the robots continue their exploration as a group after reaching the maximum dispersion possible while staying in contact with each other.  We use simulation to show that the algorithm works in multiple environments and for varying numbers of robots."1784,A Brain-Computer Interface to a Plan-based Narrative,"Interactive Narrative is an emerging form of digital entertainment heavily based on AI techniques to support narrative generation and user interaction. In recent years, there has been significant progress, with the adoption of Planning for narrative generation. However, there is still a need for unified models that incorporate narrative generation, user response and user interaction. The ability of Interactive Narratives to elicit affective responses has been considered both as a measure of believability and as an important requirement for the technologies underpinning them.

In this paper, we revisit Interactive Narratives by granting an explicit status to the userâ€™s disposition towards story characters. We have adapted narrative generation as well as user interaction so as to revolve around the mental support that the user can offer to a story character.
This is implemented through a new form of Brain-Computer Interface that naturally incorporates empathy within a filmic conception of story generation supported by planning techniques.

Our theoretical framework of interaction is based on previous findings that frontal cortex EEG asymmetry can be incorporated into a model of empathy. The cognitive elements of empathic response have been shown to manifest themselves in identifiable areas of the brain which makes this measure an ideal candidate for a BCI based around neurofeedback.  We present a fully-implemented Interactive Narrative based on this approach that generates narratives in the genre of medical drama. The narrative presents the challenges facing a junior doctor and spontaneously evolves into negative situations. At certain stages (determined dynamically) the user is given the opportunity to support the character ``mentally'' through the BCI interface. User support will shift the evolution of the narrative towards a more favourable ending.

Experimental results have demonstrated the effectiveness of our approach, as subjects were able to successfully modulate their empathetic response to improve the narrative outcome for the feature character, being aware of their impact on the story. In addition, we have validated our hypothesis through fMRI data analysis of the actual areas of brain activation during expression of empathetic support, compared against passive viewing of the same narrative."1789,Game-Theoretic Problem Selection for Tests,"Conventionally, the problems on a test are assumed to be kept secret
from test takers until the test.  However, for tests that are taken on
a large scale, particularly asynchronously, this is very hard to
achieve.  For example, example TOEFL iBT and driver's license test
problems are easily found online.  This also appears likely to become
an issue for Massive Open Online Courses (MOOCs).

In this paper, we take the loss of confidentiality as a fact.  Even
so, not all hope is lost as the test taker can memorize only a limited
set of problems, and the tester can randomize which problems appear on
the test.  We model this as a Stackelberg game, where the tester
commits to a mixed strategy and the follower responds.  We provide an
exponential-size linear program formulation, prove several NP-hardness
results, and give efficient algorithms for special cases.
"1790,A Strongly-Local Contextual Logic,"A novel contextual system is presented that combines features of both multi-context systems and logics of context. Broadly, contextual logics provide a formal notion of context, being knowledge that is true only under a specific set of assumptions. Multi-context systems use discrete logistic systems as individual contexts that are related by meta-level rules, whereas logics of context partition a single knowledge base into contexts, related using object-level rules. The contextual system presented is strongly-local in that inference and knowledge for individual contexts is discrete, but which are nevertheless represented within the one logistic system and are related at the object-level, thus gaining advantages of both. A contextual deduction system and semantics are given, with formal results including soundness and completeness. A number of properties are also examined."1793,Adaptive error-correcting output codes,"In traditional framework of error-correcting output codes (ECOC), the base classifiers are always trained independently. In this paper, we propose a new framework for adaptive ECOC learning, where the base classifiers are trained in a common subspace of the data. In this new framework, we take the base classifier training as related tasks, and all the tasks will benefit from each other for the multi-class learning. Extensive experiments on datasets from UCI machine learning repository and other real world applications demonstrate the effectiveness of our method.   "1795,Fusion of Word and Letter Based Metrics for Automatic MT Evaluation,"With the progress in machine translation, it becomes more subtle to develop the evalua-tion metric capturing the systemsâ€™ differences in comparison to the human translations. In contrast to the current efforts in leveraging more linguistic information to depict transla-tion quality, this paper takes the thread of combining those language independent fea-tures for a robust solution to MT evaluation metric. To compete with finer granularity of modeling brought by linguistic features, the proposed method augments the word level metrics by a letter based calculation. An em-pirical study is then conducted over WMT da-ta for better training of the metrics by ranking SVM. The results reveal that the integration of current language independent metrics can generate well enough performance for a varie-ty of languages. And a time-split data valida-tion is promising as a better training setting, though the greedy strategy also works well."1797,A Probabilistic Approach to Latent Cluster Analysis,"Facing a large number of clustering solution, cluster ensemble is an effective approach to aggregating them into a better one. In this paper, we propose a novel cluster ensembe method from probabilistc perspective. It assumes that each clustering solution is generated from a latent cluster model, under the control of two probabilistic parameters. Thus, the cluster ensemble problem is reformulated into an optimization problem of maximum likelihood. An EM-style algorithm is designed to solve this problem. It can determine the number of clusters automatically. Experimenal results have shown that the proposed algorithm outperforms the state-of-the-art methods including EAC-AL, CSPA, HGPA, and MCLA. Furthermore, it has been shown that our algorithm is robust to noise, and is stable in the predicted numbers of clusters."1799,Handling Open Knowledge for Service Robots,"Users may ask a service robot to accomplish various tasks so that the designer of the robot cannot program each of the tasks beforehand. Robot autonomous planning has been proposed as a means to overcome this difficulty, but in turn leads to the notorious bottleneck problem of knowledge acquisition. As more and more open-source knowledge resources become available on the web, it is worthwhile trying to make use of open knowledge, i.e., the knowledge from these open-source knowledge resources for robot planning. This paper presents our up-to-date progress along this research line. To provide a basis for searching the missing knowledge, we formulize the knowledge gaps between a user task and the robotâ€™s local knowledge and provide the complexity results of the main reasoning problems for filling the knowledge gaps. A second contribution is to introduce techniques for recursive planning with the open knowledge provided by non-experts in semi-structured natural language. Techniques for translating the semi-structured knowledge from an open-source knowledge base, OMICS, are also presented. Experiments on robot task planning based on the proposed techniques are reported with some lessons on handling open knowledge for service robots."1815,A Classification of First-Order Progressable Action Theories in Situation Calculus,"Projection and progression in situation calculus are two fundamental reasoning tasks in the context of action theories that model the initial state and the evolution of a domain. Projection refers to answering queries about the future evolutions of the domain, while progression refers to updating the logical representation of the initial state so that it reflects the changes due to an action that has been performed. In the general case projection is not decidable and progression may require second-order logic. In this paper we focus on a recent result about the decidability of projection and use it to drive results for the problem of progression. In particular we contribute with the following: i) a major result showing that a large class of practical action theories can be progressed using first-order logic; ii) a comprehensive classification of the known classes that can be progressed in  first-order, giving intuitive examples from robotic applications which show the subtle representational differences between them that play a crucial role."1837,Planning with MIP for Supply Restoration in Power Distribution Systems,"The next generation of power systems face significant challenges, both in coping with increased loading of an aging infrastructure and incorporating renewable energy sources.  Meeting these challenges requires a fundamental change in the operation of power systems by replacing human-in-the-loop operations with autonomous systems.  This is especially acute in distribution systems, where renewable integration often occurs.  This paper investigates the automation of power supply restoration (PSR), that is, the process of optimally reconfiguring a faulty distribution grid to resupply customers.  The key contributions of the paper are (1) a flexible mixed-integer programming framework for solving PSR, (2) a model decomposition to obtain high-quality solutions within the required time constraints, and (3) an experimental validation of the potential benefits of incorporating optimisation technology into PSR operations.
"1839,Utilizing Workers' Self-reported Confidence to Integrate Multiple Crowdsourced Labels,"We propose a method for using confidence scores to integrate multiple labels provided by crowdsourcing workers. Although confidence scores can be useful information for estimating the quality of the provided labels, how to effectively incorporate them has not been studied. Moreover, some workers are overconfident about the quality of their labels while others are underconfident, and some which are well-calibrated to their actual abilities, i.e., the reliability of the confidence scores varies among workers, which leads us to assume the probability distributions for the reported confidence scores are different among workers. We extend the Dawid-Skene model to create probabilistic models in which the values of unobserved true labels are inferred from the observed provided labels and reported confidence scores by using the expectation-maximization algorithm. Results of experiments using actual crowdsourced data for image labeling and binary question answering tasks showed that incorporating workers' confidence scores can improve the accuracy of integrated crowdsourced labels when the number of workers available to a task is limited. "1843,Context-Dependent Conceptualization,"Conceptualization, a problem of mapping a word or a multi-word expression to a concept, is an important task in understanding of text. Most of prior research in conceptualization uses an expertly curated knowledge base of concept-to-instance relationships. Such approach to conceptualization has the limitation that the context-sensitivity of those relationships are not captured by the knowledge base. To overcome this limitation, we propose a framework in which we harness the power of a probabilistic topic model which inherently captures the semantic relations between words. By combining latent Dirichlet allocation, a widely used topic model with Probase, a large-scale probabilistic knowledge base, we develop a corpus-based framework for context-dependent conceptualization. Through this simple but powerful framework, we improve conceptualization and thus achieve improvements in various semantic understanding tasks including frame element prediction, word similarity in context, ad-query similarity, and query similarity."1844,"Analogico-Deductive Generation of G\""{o}del's First Incompleteness Theorem from the Liar Paradox","G\""{o}del's proof of his famous first incompleteness theorem
(\textbf{G1}) has quite understandably long been a tantalizing target
for those wanting to engineer impressively intelligent computational
systems.  After all, in establishing \textbf{G1}, G\""{o}del did
something that by any metric must be classified as stunningly
intelligent.  Unfortunately, to this point, automated proofs
of \textbf{G1}, while impressive, have succeeded in no small part
because the machines in question have ahead of time been spoon-fed by
humans with formulas that are only a trivial distance
from \textbf{G1}.  Could an intelligent system
prove \textbf{G1} \emph{from first principles}?  The answer to this
question hinges, of course, on what is meant by `first principles' ---
but this phrase is painfully informal, and hence hard to clarify.  On
our interpretation of this phrase, no purely deductive computational
process that reaches \textbf{G1} can be sufficiently close to the kind
of creativity and intelligence that G\""{o}del manifested.  Following
an argument for this interpretation that appeals to G\""{o}del's
Consistency Theorem, we observe that it has long been understood that
there is some sort of analogical relationship between the Liar Paradox
(\textbf{LP}) and \textbf{G1}, and that G\""{o}del himself appreciated
and exploited the relationship.  Yet the exact nature of the
relationship has hitherto not been uncovered, by which we mean that
the following question has not been answered: Given a description
of \textbf{LP}, and the suspicion that it may somehow be
used by a suitably programmed computing machine to find a proof of the
incompleteness of Peano Arithmetic, can such a machine, provided this
description as input, produce as output a complete and verifiably
correct proof of \textbf{G1}?  In this paper, we summarize engineering
that entails an affirmative answer to this question.  Our approach
uses what we call \textit{analogico-deductive
reasoning} (ADR), which combines analogical and deductive reasoning to produce a full deductive proof of \textbf{G1}
from \textbf{LP}.  Our engineering uses a form of ADR based on a
modified implementation of Stephen Owen's Flexible Heuristic Matcher
(FHM), and a connection between the Liar Sentence in \textbf{LP}
and G\""{o}del's Fixed Point Lemma, from which \textbf{G1} follows
quickly.  We conclude by explaining that our approach should enable a
machine to obtain Tarski's Theorem, and by briefly pointing toward the
next goal in our overall project: a machine able to prove \textbf{G1}
via the so-called ``Master Argument,'' which uses Tarski's Theorem,
and which G\""{o}del regarded to be the most perspicuous route to
showing incompleteness."1846,On Condensing a Sequence of Updates in Answer-Set Programming,"Update semantics for Answer-Set Programming assign models to sequences of answer-set programs which result from the iterative process of updating programs by programs. Each program in the sequence represents an update of the preceding ones.

One of the enduring problems in this context is state condensing, or the problem of determining a single logic program that faithfully represents the sequence of programs. Such logic program should a) be written in the same alphabet, b) have the same stable models and c) be equivalent to the sequence of programs when subject to further updates. 

It has been known for more than a decade that update semantics easily lead to non-minimal stable models, so an update sequence cannot be represented by a single non-disjunctive program. On the other hand, more expressive classes of programs were never considered, mainly because it was not clear how they could be updated further.

In this paper we solve the state condensing problem for two foundational rule update semantics, using nested logic programs. Furthermore, we also show that a disjunctive program with default negation in the head can be used for the same purpose and discuss the extension of these results to other update semantics."1850,Double-Wheel Graphs Are Graceful,"We present the first polynomial time construction procedure for generating double-wheel graceful graphs. A graph is graceful if its vertices can be labeled with distinct integer values from {0,..., e}, where e is the number of edges, such that each edge has a unique value corresponding to the absolute difference of its endpoints. Graceful graphs have a range of practical application domains, including in radio astronomy, X-ray crystallography, cryptography, and experimental design. Various families of graphs have been proven to be graceful, while others have only been conjectured to be. In particular, it has been conjectured that so-called double-wheel graphs are graceful. A double-wheel graph consists of two cycles of  N nodes connected to a common hub. We prove this conjecture by providing the first construction for double-wheel graceful graphs, for any N &gt; 3, using a framework that combines streamlined constraint reasoning with insights from human computation. We also use this framework to provide a polynomial time construction for diagonally ordered magic squares."1859,An Exact Algorithm for Computing the Same-Decision Probability,"  When using graphical models for decision making, often the presence 
  of unobserved variables may hinder our ability to reach the correct
  decision. For a decision maker, a fundamental question is whether or not
  one is ready to make a decision (stopping criteria), and if not, what
  observations should be made in order to better prepare for a decision
  (selection criteria).  A recently introduced notion, called the
  Same-Decision Probability (SDP), has been shown to be useful
  as both a stopping criteria and selection criteria.
  This query has been proven to be highly intractable,
  as it has been   shown to be a PP^PP-complete problem. 
  We propose a novel algorithm for computing the SDP, and demonstrate
  its effectiveness  on several  real and synthetic networks.
  Furthermore, we present a new complexity result for computing the SDP."1863,Adaptive Norm Minimization for Semi-Supervised Elastic Embedding,"The semi-supervised learning usually only
predict labels for unlabeled data appearing
in training data, and cannot effectively predict
labels for testing data never appearing
in training set. To handle this out-of-sample
problem, many inductive methods make a
constraint such that the predicted label matrix
should be exactly equal to a linear model.
In practice, this constraint is too rigid to capture
the manifold structure of data. Motivated
by this deficiency, we relax the rigid
linear embedding constraint and propose to
use an elastic embedding constraint on the
predicted label matrix such that the manifold
structure can be better explored. To
solve our new objective and also a more general
optimization problem, we introduce a
novel adaptive norm with efficient optimization
algorithm. Our new adaptive norm minimization
method takes the advantages of
both L1 norm and L2 norm, and is robust to
the data outlier under Laplacian distribution
and can efficiently learn the normal data under
Gaussian distribution. Extensive experiments
have been performed on both singlelabel
and multi-label classification tasks and
our approach outperforms other state-of-theart
methods."1907,Robust Optimization for Hybrid MDPs with State-dependent Noise,"Recent advances in solutions to hybrid MDPs with discrete and
continuous state and action spaces have significantly extended the
class of MDPs for which exact solutions can be derived, albeit at the
expense of a restricted transition noise model.  In this paper, we
work around limitations of previous solutions by adopting a robust
optimization approach in which Nature is allowed to adversarially
determine transition noise within automatically-derived confidence
intervals.  This allows one to derive an optimal policy with an
arbitrary (user-specified) level of success probability and
significantly extends the class of transition noise models for which
hybrid MDPs can be solved.  This work also significantly extends
results for the related ``chance-constrained'' approach in stochastic
hybrid control to accommodate state-dependent noise.  We demonstrate
our approach working on a variety of hybrid MDPs taken from AI
planning, operations research, and control theory, noting that this is
the first time optimal robust solutions have been automatically
derived for such problems."1910,"  	Hierarchical Object Discovery and Dense Modelling From Motion Cues in RGB-D Video","Object segmentation and description is key to
many perceptual and manipulation tasks. In this paper, we propose
a novel method for object discovery and dense modelling
in RGB-D image sequences using motion cues. Our approach
simultaneously segments motion within key views, and discovers
objects and hierarchical relations between object parts. The
poses of the key views are optimized in a graph of spatial
relations to recover the rigid body motion trajectories of the
camera with respect to the object segments. In experiments, we
demonstrate that our approach finds moving segments, aligns
partial views on the objects, and retrieves hierarchical relations
between the objects."1918,A Unified Framework for Reputation Estimation in Online Rating Systems,"Online rating systems are now ubiquitous due to the success of recommender systems. In such systems, users are allowed to rate the items (movies, songs, commodities) in a predefined range. The ratings collected can be used to infer usersâ€™ preferences as well as itemsâ€™ intrinsic features, which can then be matched to do personalised recommendation. Due to the large quantity of users in such system, reasonable recommendation can improve user experience which may turn into profit. Collaborative filtering as one of the most successful method to solve the recommendation problem, has drawn a lot of research attention over the years. Most previous work focus on improving the prediction accuracy. Little attention is paid to the problem of spammers or low-reputed users in such systems. Spammers contaminate the rating system by assigning unreasonable scores to items, which may affect the accuracy of a recommender system. There is evidence supporting that spammer users do exist in online rating systems. In this paper, we propose a unified framework for computing the reputation score of a user, given only usersâ€™ ratings on a set of items. We show that previously proposed reputation estimation methods can be captured as special cases of our framework. New methods for computing reputation scores are also proposed.
"1925,Crowdsourcing Backdoor Identification for Combinatorial Optimization,"We will show how human computation insights can be key to identifying
so-called backdoor variables in combinatorial optimization problems.
Backdoor variables can be used to obtain dramatic speed-ups in
combinatorial search. Our approach leverages the complementary
strength of human input, based on a visual identification of problem
structure,  crowdsourcing, and the power of combinatorial solvers to
exploit complex constraints. We describe our work in the context of
the domain of materials discovery. The motivation for considering the
materials discovery domain comes from the fact that new materials can
provide solutions for key challenges in sustainability, e.g., in
energy, new catalysts for more efficient fuel cell technology."1933,Crowdsourcing-supported Query Structure Interpretation,"Recently, incorporating results from structured data sources into Web search has attracted much attention from academic and industrial communities. To understand userâ€™s intent, query structure interpretation is proposed to analyze the query structure and map query keywords to the attributes of data sources in a target domain. Existing methods are limited to assume the queries should be classified to the target domain. However, the closed-world assumption is impractical to the real Web queries that may belong to various domains. To address this problem, we propose a crowdsourcing-supported method in this paper. We first adopt an iterative probabilistic inference framework which takes a small amount of mappings between query keywords to domain attributes as seeds. In order to improve the performance, we utilize crowdsourcing to reduce the errors, and judiciously select the queries to crowdsourcing. We evaluate our method on the real queries logs, and the experimental results show our method outperforms the state-of-the-art methods."1938,StarVars -- Effective Reasoning about Relative Direction Information,"Relative direction information is very commonly used. Observers typically describe their environment by specifying the relative directions in which they see other objects or other people from their point of view. Or they receive navigation instructions with respect to their point of view, for example, turn left at the next intersection. 
However, it is surprisingly hard to integrate relative direction information obtained from different observers, and to reconstruct a model of the environment or the locations of the observers based on this information. Despite intensive research, there is currently no algorithm that can effectively integrate this information: this problem is known to be NP-hard, but not known to be in NP, even if the only information we use is left and right. 

In this paper we present a novel qualitative representation, StarVars, that can solve these problems. It is an extension of the STAR calculus (Renz and Mitra, 2004) by a VARiable interpretation of the orientation of observers. We show that reasoning in StarVars is in NP and present the first algorithm that allows us to effectively integrate relative direction information from different observers. 

"1965,Representation and Reasoning about General Solid Rectangles,"Entities in two-dimensional space are often approximated using rectangles that are parallel to the two axes that define the space, so-called minimum-bounding rectangles (MBRs). MBRs are popular in Computer Vision and other areas as they are easy to obtain and easy to represent. In the area of Qualitative Spatial Reasoning, many different spatial representations are based on MBRs. 

Surprisingly, there is no such representation for general rectangles, i.e., rectangles that do not have to be parallel to the axes but can have any angle. Nor for general solid rectangles (GSRs) that cannot penetrate each other. GSRs are often used in computer graphics and computer games, such as Angry Birds, where they form the building blocks of more complicated structures. 
In order to represent and reason about these structures, we need a spatial representation that allows us to use GSRs as the basic spatial entities. 

In this paper we develop and analyze a qualitative spatial representation for GSRs. We apply our representation and the corresponding reasoning methods to solve a very interesting  problem: Assuming we want to use Computer Vision to detect GSRs in computer games, but vision can only detect MBRs. How can we infer the actual GSRs from the detected MBRs? We evaluate our algorithms and test their usefulness in extracting spatial structures for the Angry Birds game. "